{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_openpose_small.ipynb",
      "provenance": [],
      "mount_file_id": "1gyPMcI24FrulxYggnKAcgSHu5mytB7jq",
      "authorship_tag": "ABX9TyMXH4SalNTq11FySgDSeXSC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KokiNiimura/study/blob/master/Training_openpose_small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gBcd8IrUELC",
        "outputId": "93bbf325-41f5-4c4e-ea52-8e1cbe6a217d"
      },
      "source": [
        "%cd /content/drive/My Drive/study/PyTorch_Advanced/04"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/study/PyTorch_Advanced/04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8kq3Ce_UMC-"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvZfPlKfVh1M"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grxh_nhWVp6-"
      },
      "source": [
        "from utils.dataloader import make_datapath_list, DataTransform, COCOkeypointsDataset\n",
        "\n",
        "train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list = \\\n",
        "    make_datapath_list(rootpath='./data/')\n",
        "\n",
        "train_dataset = COCOkeypointsDataset(\n",
        "    val_img_list, val_mask_list, val_meta_list, phase=\"train\", transform=DataTransform())\n",
        "\n",
        "# val_dataset = CocokeypointsDataset(\n",
        "#     val_img_list, val_mask_list, val_meta_list, phase=\"val\", transform=DataTransform())\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size, shuffle=True)\n",
        "\n",
        "# val_dataloader = data.DataLoader(\n",
        "#     val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": None}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt_c8l3lWJ2t"
      },
      "source": [
        "from utils.openpose_net import OpenPoseNet\n",
        "net = OpenPoseNet()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkcFpaJtYFuy"
      },
      "source": [
        "class OpenPoseLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OpenPoseLoss, self).__init__()\n",
        "    \n",
        "    def forward(self, saved_for_loss, heatmap_target, heat_mask, paf_target, paf_mask):\n",
        "        total_loss = 0\n",
        "        for j in range(6):\n",
        "            # PAFs\n",
        "            pred1 = saved_for_loss[2 * j] * paf_mask\n",
        "            gt1 = paf_target.float() * paf_mask\n",
        "\n",
        "            # heatmaps\n",
        "            pred2 = saved_for_loss[2 * j + 1] * heat_mask\n",
        "            gt2 = heatmap_target.float()*heat_mask\n",
        "\n",
        "            total_loss += F.mse_loss(pred1, gt1, reduction='mean') + \\\n",
        "                F.mse_loss(pred2, gt2, reduction='mean')\n",
        "    \n",
        "        return total_loss\n",
        "\n",
        "criterion = OpenPoseLoss()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjHnGnE7bW67"
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=1e-2, \n",
        "                      momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "def lambda_epoch(epoch):\n",
        "    max_epoch = 10\n",
        "    return math.pow((1-epoch/max_epoch), 0.9)\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibdLUrGf9QWu",
        "outputId": "e25043c8-1833-4fc4-cec9-a28f26de33d4"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsFFtEQS6ia8"
      },
      "source": [
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YzbUItfJ3Bt"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN1x8TFDcZqL"
      },
      "source": [
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device:\", device)\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    iteration = 1\n",
        "    logs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_val_loss = 0.0\n",
        "\n",
        "        print(\"---------------\")\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print(\"---------------\")\n",
        "\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                net.train()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                print(\"(train)\")\n",
        "\n",
        "            else:\n",
        "                continue\n",
        "                # net.eval()\n",
        "                # print(\"---------------\")\n",
        "                # print(\"(val)\")\n",
        "\n",
        "            for images, heatmap_target, heat_mask, paf_target, paf_mask in dataloaders_dict[phase]:\n",
        "                if images.size()[0] == 1:\n",
        "                    continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                heatmap_target = heatmap_target.to(device)\n",
        "                heat_mask = heat_mask.to(device)\n",
        "                paf_target = paf_target.to(device)\n",
        "                paf_mask = paf_mask.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    _, saved_for_loss = net(images)\n",
        "                    \n",
        "                    loss = criterion(saved_for_loss, heatmap_target, \n",
        "                                    heat_mask, paf_target, paf_mask)\n",
        "                    \n",
        "                    del saved_for_loss\n",
        "\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item()/batch_size, duration))\n",
        "                            t_iter_start = time.time()\n",
        "                            \n",
        "                        epoch_train_loss += loss.item()\n",
        "                        iteration += 1\n",
        "\n",
        "                    # else:\n",
        "                    #     epoch_val_loss += loss.item()\n",
        "        \n",
        "        t_epoch_finish = time.time()\n",
        "        print('--------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss: {:.4f} || Epoch_VAL_Loss: {:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss/num_train_imgs, 0))\n",
        "        print('timer: {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        writer.add_scalars('data/scalars', {'train': epoch_train_loss/num_train_imgs}, epoch+1)\n",
        "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss/num_train_imgs, \n",
        "                'val_loss': 0}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(\"log_output_openpose.csv\")\n",
        "\n",
        "        torch.save(net.state_dict(), 'weights/openpose_' + str(epoch+1) + '.pth')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIPRt5PZi_Gh",
        "outputId": "eedf5d9a-4af8-4e45-fec3-2c7d17323b91"
      },
      "source": [
        "num_epochs = 10\n",
        "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda:0\n",
            "---------------\n",
            "Epoch 1/10\n",
            "---------------\n",
            "(train)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 10 || Loss: 0.0092 || 10iter: 47.1449 sec.\n",
            "iteration 20 || Loss: 0.0082 || 10iter: 38.9631 sec.\n",
            "iteration 30 || Loss: 0.0070 || 10iter: 39.9942 sec.\n",
            "iteration 40 || Loss: 0.0064 || 10iter: 40.0579 sec.\n",
            "iteration 50 || Loss: 0.0053 || 10iter: 43.5700 sec.\n",
            "iteration 60 || Loss: 0.0044 || 10iter: 39.2145 sec.\n",
            "iteration 70 || Loss: 0.0036 || 10iter: 41.5225 sec.\n",
            "iteration 80 || Loss: 0.0038 || 10iter: 39.9206 sec.\n",
            "iteration 90 || Loss: 0.0033 || 10iter: 39.3675 sec.\n",
            "iteration 100 || Loss: 0.0029 || 10iter: 41.4963 sec.\n",
            "iteration 110 || Loss: 0.0024 || 10iter: 41.0915 sec.\n",
            "iteration 120 || Loss: 0.0023 || 10iter: 40.6406 sec.\n",
            "iteration 130 || Loss: 0.0022 || 10iter: 39.0994 sec.\n",
            "iteration 140 || Loss: 0.0022 || 10iter: 41.3139 sec.\n",
            "iteration 150 || Loss: 0.0018 || 10iter: 39.6242 sec.\n",
            "--------------\n",
            "epoch 1 || Epoch_TRAIN_Loss: 0.0045 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 630.9393 sec.\n",
            "---------------\n",
            "Epoch 2/10\n",
            "---------------\n",
            "(train)\n",
            "iteration 160 || Loss: 0.0019 || 10iter: 31.7031 sec.\n",
            "iteration 170 || Loss: 0.0017 || 10iter: 39.5609 sec.\n",
            "iteration 180 || Loss: 0.0019 || 10iter: 42.6561 sec.\n",
            "iteration 190 || Loss: 0.0017 || 10iter: 40.4781 sec.\n",
            "iteration 200 || Loss: 0.0013 || 10iter: 37.9408 sec.\n",
            "iteration 210 || Loss: 0.0019 || 10iter: 40.7386 sec.\n",
            "iteration 220 || Loss: 0.0018 || 10iter: 37.5671 sec.\n",
            "iteration 230 || Loss: 0.0017 || 10iter: 40.9511 sec.\n",
            "iteration 240 || Loss: 0.0018 || 10iter: 39.6049 sec.\n",
            "iteration 250 || Loss: 0.0015 || 10iter: 37.8035 sec.\n",
            "iteration 260 || Loss: 0.0017 || 10iter: 39.6145 sec.\n",
            "iteration 270 || Loss: 0.0015 || 10iter: 40.5529 sec.\n",
            "iteration 280 || Loss: 0.0015 || 10iter: 39.3341 sec.\n",
            "iteration 290 || Loss: 0.0014 || 10iter: 41.0955 sec.\n",
            "iteration 300 || Loss: 0.0013 || 10iter: 38.3341 sec.\n",
            "--------------\n",
            "epoch 2 || Epoch_TRAIN_Loss: 0.0016 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 615.6165 sec.\n",
            "---------------\n",
            "Epoch 3/10\n",
            "---------------\n",
            "(train)\n",
            "iteration 310 || Loss: 0.0014 || 10iter: 15.2033 sec.\n",
            "iteration 320 || Loss: 0.0013 || 10iter: 40.9769 sec.\n",
            "iteration 330 || Loss: 0.0013 || 10iter: 39.8913 sec.\n",
            "iteration 340 || Loss: 0.0015 || 10iter: 40.4144 sec.\n",
            "iteration 350 || Loss: 0.0017 || 10iter: 39.6761 sec.\n",
            "iteration 360 || Loss: 0.0013 || 10iter: 40.6545 sec.\n",
            "iteration 370 || Loss: 0.0012 || 10iter: 40.0240 sec.\n",
            "iteration 380 || Loss: 0.0013 || 10iter: 39.5645 sec.\n",
            "iteration 390 || Loss: 0.0014 || 10iter: 41.6662 sec.\n",
            "iteration 400 || Loss: 0.0014 || 10iter: 40.2398 sec.\n",
            "iteration 410 || Loss: 0.0014 || 10iter: 39.1455 sec.\n",
            "iteration 420 || Loss: 0.0016 || 10iter: 40.9839 sec.\n",
            "iteration 430 || Loss: 0.0012 || 10iter: 40.1288 sec.\n",
            "iteration 440 || Loss: 0.0015 || 10iter: 39.8832 sec.\n",
            "iteration 450 || Loss: 0.0016 || 10iter: 42.7422 sec.\n",
            "--------------\n",
            "epoch 3 || Epoch_TRAIN_Loss: 0.0014 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 619.0719 sec.\n",
            "---------------\n",
            "Epoch 4/10\n",
            "---------------\n",
            "(train)\n",
            "iteration 460 || Loss: 0.0015 || 10iter: 3.1637 sec.\n",
            "iteration 470 || Loss: 0.0013 || 10iter: 40.9888 sec.\n",
            "iteration 480 || Loss: 0.0016 || 10iter: 39.1830 sec.\n",
            "iteration 490 || Loss: 0.0013 || 10iter: 38.5824 sec.\n",
            "iteration 500 || Loss: 0.0016 || 10iter: 39.9655 sec.\n",
            "iteration 510 || Loss: 0.0012 || 10iter: 40.5447 sec.\n",
            "iteration 520 || Loss: 0.0015 || 10iter: 41.7010 sec.\n",
            "iteration 530 || Loss: 0.0017 || 10iter: 41.4779 sec.\n",
            "iteration 540 || Loss: 0.0015 || 10iter: 40.3006 sec.\n",
            "iteration 550 || Loss: 0.0014 || 10iter: 42.4254 sec.\n",
            "iteration 560 || Loss: 0.0013 || 10iter: 40.7243 sec.\n",
            "iteration 570 || Loss: 0.0015 || 10iter: 37.5674 sec.\n",
            "iteration 580 || Loss: 0.0015 || 10iter: 40.8102 sec.\n",
            "iteration 590 || Loss: 0.0015 || 10iter: 40.7381 sec.\n",
            "iteration 600 || Loss: 0.0012 || 10iter: 38.9387 sec.\n",
            "iteration 610 || Loss: 0.0015 || 10iter: 38.6285 sec.\n",
            "--------------\n",
            "epoch 4 || Epoch_TRAIN_Loss: 0.0014 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 618.2393 sec.\n",
            "---------------\n",
            "Epoch 5/10\n",
            "---------------\n",
            "(train)\n",
            "iteration 620 || Loss: 0.0015 || 10iter: 32.8523 sec.\n",
            "iteration 630 || Loss: 0.0014 || 10iter: 39.5220 sec.\n",
            "iteration 640 || Loss: 0.0014 || 10iter: 38.9019 sec.\n",
            "iteration 650 || Loss: 0.0014 || 10iter: 41.0793 sec.\n",
            "iteration 660 || Loss: 0.0015 || 10iter: 38.7033 sec.\n",
            "iteration 670 || Loss: 0.0011 || 10iter: 41.8278 sec.\n",
            "iteration 680 || Loss: 0.0014 || 10iter: 42.1498 sec.\n",
            "iteration 690 || Loss: 0.0015 || 10iter: 43.4727 sec.\n",
            "iteration 700 || Loss: 0.0015 || 10iter: 38.2966 sec.\n",
            "iteration 710 || Loss: 0.0014 || 10iter: 39.9038 sec.\n",
            "iteration 720 || Loss: 0.0014 || 10iter: 41.6278 sec.\n",
            "iteration 730 || Loss: 0.0014 || 10iter: 38.8744 sec.\n",
            "iteration 740 || Loss: 0.0018 || 10iter: 40.2532 sec.\n",
            "iteration 750 || Loss: 0.0013 || 10iter: 41.1352 sec.\n",
            "iteration 760 || Loss: 0.0014 || 10iter: 39.0198 sec.\n",
            "--------------\n",
            "epoch 5 || Epoch_TRAIN_Loss: 0.0014 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 623.6145 sec.\n",
            "---------------\n",
            "Epoch 6/10\n",
            "---------------\n",
            "(train)\n",
            "iteration 770 || Loss: 0.0014 || 10iter: 19.5539 sec.\n",
            "iteration 780 || Loss: 0.0017 || 10iter: 43.7840 sec.\n",
            "iteration 790 || Loss: 0.0016 || 10iter: 43.1142 sec.\n",
            "iteration 800 || Loss: 0.0015 || 10iter: 38.2965 sec.\n",
            "iteration 810 || Loss: 0.0016 || 10iter: 41.5451 sec.\n",
            "iteration 820 || Loss: 0.0012 || 10iter: 40.0851 sec.\n",
            "iteration 830 || Loss: 0.0016 || 10iter: 40.1536 sec.\n",
            "iteration 840 || Loss: 0.0013 || 10iter: 38.3377 sec.\n",
            "iteration 850 || Loss: 0.0016 || 10iter: 39.9531 sec.\n",
            "iteration 860 || Loss: 0.0010 || 10iter: 41.5110 sec.\n",
            "iteration 870 || Loss: 0.0016 || 10iter: 41.3274 sec.\n",
            "iteration 880 || Loss: 0.0014 || 10iter: 40.6146 sec.\n",
            "iteration 890 || Loss: 0.0014 || 10iter: 41.8207 sec.\n",
            "iteration 900 || Loss: 0.0015 || 10iter: 37.7920 sec.\n",
            "iteration 910 || Loss: 0.0015 || 10iter: 38.3278 sec.\n",
            "--------------\n",
            "epoch 6 || Epoch_TRAIN_Loss: 0.0014 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 624.2919 sec.\n",
            "---------------\n",
            "Epoch 7/10\n",
            "---------------\n",
            "(train)\n",
            "iteration 920 || Loss: 0.0013 || 10iter: 7.5740 sec.\n",
            "iteration 930 || Loss: 0.0012 || 10iter: 40.6574 sec.\n",
            "iteration 940 || Loss: 0.0014 || 10iter: 38.5375 sec.\n",
            "iteration 950 || Loss: 0.0014 || 10iter: 41.3667 sec.\n",
            "iteration 960 || Loss: 0.0014 || 10iter: 39.9550 sec.\n",
            "iteration 970 || Loss: 0.0016 || 10iter: 40.7605 sec.\n",
            "iteration 980 || Loss: 0.0013 || 10iter: 45.0045 sec.\n",
            "iteration 990 || Loss: 0.0015 || 10iter: 39.4201 sec.\n",
            "iteration 1000 || Loss: 0.0015 || 10iter: 39.4464 sec.\n",
            "iteration 1010 || Loss: 0.0012 || 10iter: 41.4702 sec.\n",
            "iteration 1020 || Loss: 0.0012 || 10iter: 41.5418 sec.\n",
            "iteration 1030 || Loss: 0.0017 || 10iter: 38.6226 sec.\n",
            "iteration 1040 || Loss: 0.0014 || 10iter: 38.8588 sec.\n",
            "iteration 1050 || Loss: 0.0011 || 10iter: 40.5900 sec.\n",
            "iteration 1060 || Loss: 0.0015 || 10iter: 40.3042 sec.\n",
            "iteration 1070 || Loss: 0.0018 || 10iter: 40.1024 sec.\n",
            "--------------\n",
            "epoch 7 || Epoch_TRAIN_Loss: 0.0014 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 622.5968 sec.\n",
            "---------------\n",
            "Epoch 8/10\n",
            "---------------\n",
            "(train)\n",
            "iteration 1080 || Loss: 0.0014 || 10iter: 37.3384 sec.\n",
            "iteration 1090 || Loss: 0.0015 || 10iter: 39.6424 sec.\n",
            "iteration 1100 || Loss: 0.0013 || 10iter: 42.6533 sec.\n",
            "iteration 1110 || Loss: 0.0011 || 10iter: 40.3267 sec.\n",
            "iteration 1120 || Loss: 0.0012 || 10iter: 41.2111 sec.\n",
            "iteration 1130 || Loss: 0.0012 || 10iter: 38.9431 sec.\n",
            "iteration 1140 || Loss: 0.0014 || 10iter: 40.9492 sec.\n",
            "iteration 1150 || Loss: 0.0015 || 10iter: 38.6528 sec.\n",
            "iteration 1160 || Loss: 0.0015 || 10iter: 39.3858 sec.\n",
            "iteration 1170 || Loss: 0.0012 || 10iter: 41.0058 sec.\n",
            "iteration 1180 || Loss: 0.0017 || 10iter: 39.3218 sec.\n",
            "iteration 1190 || Loss: 0.0014 || 10iter: 39.7005 sec.\n",
            "iteration 1200 || Loss: 0.0012 || 10iter: 42.2310 sec.\n",
            "iteration 1210 || Loss: 0.0016 || 10iter: 38.4274 sec.\n",
            "iteration 1220 || Loss: 0.0016 || 10iter: 40.5552 sec.\n",
            "--------------\n",
            "epoch 8 || Epoch_TRAIN_Loss: 0.0014 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 620.6580 sec.\n",
            "---------------\n",
            "Epoch 9/10\n",
            "---------------\n",
            "(train)\n",
            "iteration 1230 || Loss: 0.0016 || 10iter: 24.3155 sec.\n",
            "iteration 1240 || Loss: 0.0016 || 10iter: 43.2022 sec.\n",
            "iteration 1250 || Loss: 0.0014 || 10iter: 39.1281 sec.\n",
            "iteration 1260 || Loss: 0.0013 || 10iter: 38.8166 sec.\n",
            "iteration 1270 || Loss: 0.0014 || 10iter: 39.5378 sec.\n",
            "iteration 1280 || Loss: 0.0015 || 10iter: 39.6874 sec.\n",
            "iteration 1290 || Loss: 0.0013 || 10iter: 38.8484 sec.\n",
            "iteration 1300 || Loss: 0.0014 || 10iter: 40.0385 sec.\n",
            "iteration 1310 || Loss: 0.0019 || 10iter: 41.9064 sec.\n",
            "iteration 1320 || Loss: 0.0015 || 10iter: 40.9870 sec.\n",
            "iteration 1330 || Loss: 0.0016 || 10iter: 42.0696 sec.\n",
            "iteration 1340 || Loss: 0.0016 || 10iter: 39.2356 sec.\n",
            "iteration 1350 || Loss: 0.0013 || 10iter: 39.9421 sec.\n",
            "iteration 1360 || Loss: 0.0016 || 10iter: 40.0551 sec.\n",
            "iteration 1370 || Loss: 0.0014 || 10iter: 41.8041 sec.\n",
            "--------------\n",
            "epoch 9 || Epoch_TRAIN_Loss: 0.0014 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 622.0899 sec.\n",
            "---------------\n",
            "Epoch 10/10\n",
            "---------------\n",
            "(train)\n",
            "iteration 1380 || Loss: 0.0015 || 10iter: 12.1366 sec.\n",
            "iteration 1390 || Loss: 0.0012 || 10iter: 41.0180 sec.\n",
            "iteration 1400 || Loss: 0.0017 || 10iter: 39.3511 sec.\n",
            "iteration 1410 || Loss: 0.0014 || 10iter: 44.1662 sec.\n",
            "iteration 1420 || Loss: 0.0011 || 10iter: 41.7286 sec.\n",
            "iteration 1430 || Loss: 0.0011 || 10iter: 41.1985 sec.\n",
            "iteration 1440 || Loss: 0.0012 || 10iter: 38.1231 sec.\n",
            "iteration 1450 || Loss: 0.0017 || 10iter: 39.0231 sec.\n",
            "iteration 1460 || Loss: 0.0015 || 10iter: 40.8013 sec.\n",
            "iteration 1470 || Loss: 0.0015 || 10iter: 39.8159 sec.\n",
            "iteration 1480 || Loss: 0.0011 || 10iter: 41.9758 sec.\n",
            "iteration 1490 || Loss: 0.0014 || 10iter: 38.6004 sec.\n",
            "iteration 1500 || Loss: 0.0014 || 10iter: 39.9805 sec.\n",
            "iteration 1510 || Loss: 0.0013 || 10iter: 38.1184 sec.\n",
            "iteration 1520 || Loss: 0.0017 || 10iter: 37.5307 sec.\n",
            "iteration 1530 || Loss: 0.0015 || 10iter: 36.8784 sec.\n",
            "--------------\n",
            "epoch 10 || Epoch_TRAIN_Loss: 0.0014 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 617.3394 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjz1Xzo_gX6x"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}