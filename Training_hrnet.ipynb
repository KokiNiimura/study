{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_hrnet.ipynb",
      "provenance": [],
      "mount_file_id": "1CZKhKNf5v3sI_1mGIydhGhM4oJE_Y-HR",
      "authorship_tag": "ABX9TyOnnBd5srhicpJ3AgBNpVx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KokiNiimura/study/blob/master/Training_hrnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Dtu9s5Lkxp",
        "outputId": "61fca657-9ea9-48d3-b3bb-df03f5415c9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/My Drive/study/PyTorch_Advanced/03"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/study/PyTorch_Advanced/03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXKa0gG4MioW"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "import functools\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch._utils"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFCgKVvvdssn"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_nZJeoZd0rq"
      },
      "source": [
        "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
        "\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath=rootpath)\n",
        "\n",
        "color_mean = (0.485, 0.456, 0.406)\n",
        "color_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", \n",
        "                           transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", \n",
        "                           transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqhUuSBXzWUu"
      },
      "source": [
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "\n",
        "BN_MOMENTUM = 0.01\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out + residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn3 = BatchNorm2d(planes * self.expansion,\n",
        "                               momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out + residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class HighResolutionModule(nn.Module):\n",
        "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
        "                 num_channels, fuse_method, multi_scale_output=True):\n",
        "        super(HighResolutionModule, self).__init__()\n",
        "        self._check_branches(\n",
        "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
        "\n",
        "        self.num_inchannels = num_inchannels\n",
        "        self.fuse_method = fuse_method\n",
        "        self.num_branches = num_branches\n",
        "\n",
        "        self.multi_scale_output = multi_scale_output\n",
        "\n",
        "        self.branches = self._make_branches(\n",
        "            num_branches, blocks, num_blocks, num_channels)\n",
        "        self.fuse_layers = self._make_fuse_layers()\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def _check_branches(self, num_branches, blocks, num_blocks,\n",
        "                        num_inchannels, num_channels):\n",
        "        if num_branches != len(num_blocks):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n",
        "                num_branches, len(num_blocks))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        if num_branches != len(num_channels):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n",
        "                num_branches, len(num_channels))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        if num_branches != len(num_inchannels):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n",
        "                num_branches, len(num_inchannels))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
        "                         stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or \\\n",
        "           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.num_inchannels[branch_index],\n",
        "                          num_channels[branch_index] * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(num_channels[branch_index] * block.expansion,\n",
        "                            momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.num_inchannels[branch_index],\n",
        "                            num_channels[branch_index], stride, downsample))\n",
        "        self.num_inchannels[branch_index] = \\\n",
        "            num_channels[branch_index] * block.expansion\n",
        "        for i in range(1, num_blocks[branch_index]):\n",
        "            layers.append(block(self.num_inchannels[branch_index],\n",
        "                                num_channels[branch_index]))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
        "        branches = []\n",
        "\n",
        "        for i in range(num_branches):\n",
        "            branches.append(\n",
        "                self._make_one_branch(i, block, num_blocks, num_channels))\n",
        "\n",
        "        return nn.ModuleList(branches)\n",
        "\n",
        "    def _make_fuse_layers(self):\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "\n",
        "        num_branches = self.num_branches\n",
        "        num_inchannels = self.num_inchannels\n",
        "        fuse_layers = []\n",
        "        for i in range(num_branches if self.multi_scale_output else 1):\n",
        "            fuse_layer = []\n",
        "            for j in range(num_branches):\n",
        "                if j > i:\n",
        "                    fuse_layer.append(nn.Sequential(\n",
        "                        nn.Conv2d(num_inchannels[j],\n",
        "                                  num_inchannels[i],\n",
        "                                  1,\n",
        "                                  1,\n",
        "                                  0,\n",
        "                                  bias=False),\n",
        "                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n",
        "                elif j == i:\n",
        "                    fuse_layer.append(None)\n",
        "                else:\n",
        "                    conv3x3s = []\n",
        "                    for k in range(i-j):\n",
        "                        if k == i - j - 1:\n",
        "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
        "                            conv3x3s.append(nn.Sequential(\n",
        "                                nn.Conv2d(num_inchannels[j],\n",
        "                                          num_outchannels_conv3x3,\n",
        "                                          3, 2, 1, bias=False),\n",
        "                                BatchNorm2d(num_outchannels_conv3x3, \n",
        "                                            momentum=BN_MOMENTUM)))\n",
        "                        else:\n",
        "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
        "                            conv3x3s.append(nn.Sequential(\n",
        "                                nn.Conv2d(num_inchannels[j],\n",
        "                                          num_outchannels_conv3x3,\n",
        "                                          3, 2, 1, bias=False),\n",
        "                                BatchNorm2d(num_outchannels_conv3x3,\n",
        "                                            momentum=BN_MOMENTUM),\n",
        "                                nn.ReLU(inplace=False)))\n",
        "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
        "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
        "\n",
        "        return nn.ModuleList(fuse_layers)\n",
        "\n",
        "    def get_num_inchannels(self):\n",
        "        return self.num_inchannels\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.num_branches == 1:\n",
        "            return [self.branches[0](x[0])]\n",
        "\n",
        "        for i in range(self.num_branches):\n",
        "            x[i] = self.branches[i](x[i])\n",
        "\n",
        "        x_fuse = []\n",
        "        for i in range(len(self.fuse_layers)):\n",
        "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
        "            for j in range(1, self.num_branches):\n",
        "                if i == j:\n",
        "                    y = y + x[j]\n",
        "                elif j > i:\n",
        "                    width_output = x[i].shape[-1]\n",
        "                    height_output = x[i].shape[-2]\n",
        "                    y = y + F.interpolate(\n",
        "                        self.fuse_layers[i][j](x[j]),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear')\n",
        "                else:\n",
        "                    y = y + self.fuse_layers[i][j](x[j])\n",
        "            x_fuse.append(self.relu(y))\n",
        "\n",
        "        return x_fuse\n",
        "\n",
        "\n",
        "blocks_dict = {\n",
        "    'BASIC': BasicBlock,\n",
        "    'BOTTLENECK': Bottleneck\n",
        "}\n",
        "\n",
        "\n",
        "class HighResolutionNet(nn.Module):\n",
        "\n",
        "    def __init__(self, config, **kwargs):\n",
        "        #extra = config.MODEL.EXTRA\n",
        "        extra = config['MODEL']['EXTRA']\n",
        "        super(HighResolutionNet, self).__init__()\n",
        "\n",
        "        # stem net\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "        self.stage1_cfg = extra['STAGE1']\n",
        "        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n",
        "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
        "        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n",
        "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
        "        stage1_out_channel = block.expansion*num_channels\n",
        "\n",
        "        self.stage2_cfg = extra['STAGE2']\n",
        "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition1 = self._make_transition_layer(\n",
        "            [stage1_out_channel], num_channels)\n",
        "        self.stage2, pre_stage_channels = self._make_stage(\n",
        "            self.stage2_cfg, num_channels)\n",
        "\n",
        "        self.stage3_cfg = extra['STAGE3']\n",
        "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition2 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage3, pre_stage_channels = self._make_stage(\n",
        "            self.stage3_cfg, num_channels)\n",
        "\n",
        "        self.stage4_cfg = extra['STAGE4']\n",
        "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition3 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage4, pre_stage_channels = self._make_stage(\n",
        "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
        "        \n",
        "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
        "\n",
        "        self.last_layer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=last_inp_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0),\n",
        "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=config['NUM_CLASSES'],\n",
        "                kernel_size=extra['FINAL_CONV_KERNEL'],\n",
        "                stride=1,\n",
        "                padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0)\n",
        "        )\n",
        "\n",
        "    def _make_transition_layer(\n",
        "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
        "        num_branches_cur = len(num_channels_cur_layer)\n",
        "        num_branches_pre = len(num_channels_pre_layer)\n",
        "\n",
        "        transition_layers = []\n",
        "        for i in range(num_branches_cur):\n",
        "            if i < num_branches_pre:\n",
        "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
        "                    transition_layers.append(nn.Sequential(\n",
        "                        nn.Conv2d(num_channels_pre_layer[i],\n",
        "                                  num_channels_cur_layer[i],\n",
        "                                  3,\n",
        "                                  1,\n",
        "                                  1,\n",
        "                                  bias=False),\n",
        "                        BatchNorm2d(\n",
        "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
        "                        nn.ReLU(inplace=False)))\n",
        "                else:\n",
        "                    transition_layers.append(None)\n",
        "            else:\n",
        "                conv3x3s = []\n",
        "                for j in range(i+1-num_branches_pre):\n",
        "                    inchannels = num_channels_pre_layer[-1]\n",
        "                    outchannels = num_channels_cur_layer[i] \\\n",
        "                        if j == i-num_branches_pre else inchannels\n",
        "                    conv3x3s.append(nn.Sequential(\n",
        "                        nn.Conv2d(\n",
        "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
        "                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
        "                        nn.ReLU(inplace=False)))\n",
        "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
        "\n",
        "        return nn.ModuleList(transition_layers)\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_stage(self, layer_config, num_inchannels,\n",
        "                    multi_scale_output=True):\n",
        "        num_modules = layer_config['NUM_MODULES']\n",
        "        num_branches = layer_config['NUM_BRANCHES']\n",
        "        num_blocks = layer_config['NUM_BLOCKS']\n",
        "        num_channels = layer_config['NUM_CHANNELS']\n",
        "        block = blocks_dict[layer_config['BLOCK']]\n",
        "        fuse_method = layer_config['FUSE_METHOD']\n",
        "\n",
        "        modules = []\n",
        "        for i in range(num_modules):\n",
        "            # multi_scale_output is only used last module\n",
        "            if not multi_scale_output and i == num_modules - 1:\n",
        "                reset_multi_scale_output = False\n",
        "            else:\n",
        "                reset_multi_scale_output = True\n",
        "            modules.append(\n",
        "                HighResolutionModule(num_branches,\n",
        "                                      block,\n",
        "                                      num_blocks,\n",
        "                                      num_inchannels,\n",
        "                                      num_channels,\n",
        "                                      fuse_method,\n",
        "                                      reset_multi_scale_output)\n",
        "            )\n",
        "            num_inchannels = modules[-1].get_num_inchannels()\n",
        "\n",
        "        return nn.Sequential(*modules), num_inchannels\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        #print(x.shape)\n",
        "        x = self.layer1(x)\n",
        "        #print(x.shape)\n",
        "        x_list = []\n",
        "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
        "            if self.transition1[i] is not None:\n",
        "                x_list.append(self.transition1[i](x))\n",
        "            else:\n",
        "                x_list.append(x)\n",
        "        y_list = self.stage2(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
        "            if self.transition2[i] is not None:\n",
        "                if i < self.stage2_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition2[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition2[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        y_list = self.stage3(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
        "            if self.transition3[i] is not None:\n",
        "                if i < self.stage3_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition3[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition3[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        x = self.stage4(x_list)\n",
        "        #print(x.shape)\n",
        "        # Upsampling\n",
        "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
        "        #x0_h, x0_w = x0_h* 4, x0_w * 2 \n",
        "        #print(x0_h, x0_w)\n",
        "        x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')\n",
        "        x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')\n",
        "        x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')\n",
        "\n",
        "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
        "\n",
        "        x = self.last_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def init_weights(self, pretrained='',):\n",
        "        logger.info('=> init weights from normal distribution')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.001)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        if pretrained:\n",
        "            pretrained_dict = torch.load(pretrained)\n",
        "            logger.info('=> loading pretrained model {}'.format(pretrained))\n",
        "            model_dict = self.state_dict()\n",
        "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
        "                               if k in model_dict.keys()}\n",
        "            for k, _ in pretrained_dict.items():\n",
        "                logger.info(\n",
        "                    '=> loading {} pretrained model {}'.format(k, pretrained))\n",
        "            model_dict.update(pretrained_dict)\n",
        "            self.load_state_dict(model_dict)\n",
        "\n",
        "def get_seg_model(cfg, **kwargs):\n",
        "    model = HighResolutionNet(cfg, **kwargs)\n",
        "    model.init_weights(cfg['PRETRAINED'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1EGoRi10oMx"
      },
      "source": [
        "# dict for model configuration\n",
        "config = {}\n",
        "\n",
        "config['NUM_CLASSES'] = 21\n",
        "config['PRETRAINED'] = None\n",
        "\n",
        "config['MODEL'] = {'EXTRA': {'FINAL_CONV_KERNEL': 1,\n",
        "      'STAGE1': {'BLOCK': 'BOTTLENECK', \n",
        "                 'FUSE_METHOD': 'SUM',\n",
        "                 'NUM_BLOCKS': [1],\n",
        "                 'NUM_CHANNELS': [32],\n",
        "                 'NUM_MODULES': 1,\n",
        "                 'NUM_RANCHES': 1\n",
        "                },\n",
        "      'STAGE2': {'BLOCK': 'BASIC',\n",
        "                 'FUSE_METHOD': 'SUM',\n",
        "                 'NUM_BLOCKS': [2, 2],\n",
        "                 'NUM_BRANCHES': 2,\n",
        "                 'NUM_CHANNELS': [16, 32],\n",
        "                 'NUM_MODULES': 1\n",
        "                },\n",
        "      'STAGE3':{'BLOCK': 'BASIC',\n",
        "                'FUSE_METHOD': 'SUM',\n",
        "                'NUM_BLOCKS': [2, 2, 2],\n",
        "                'NUM_BRANCHES': 3,\n",
        "                'NUM_CHANNELS': [16, 32, 64],\n",
        "                'NUM_MODULES': 1\n",
        "               },\n",
        "       'STAGE4': {'BLOCK': 'BASIC',\n",
        "                 'FUSE_METHOD': 'SUM',\n",
        "                 'NUM_BLOCKS': [2, 2, 2, 2],\n",
        "                 'NUM_BRANCHES': 4,\n",
        "                 'NUM_CHANNELS': [16, 32, 64, 128],\n",
        "                 'NUM_MODULES': 1\n",
        "                 }}}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlMNdp2A012h"
      },
      "source": [
        "net = get_seg_model(config)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgDZepO_p2sW"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EXrJsEYt1ND"
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "def lambda_epoch(epoch):\n",
        "    max_epoch = 30\n",
        "    return math.pow((1-epoch/max_epoch), 0.9)\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GguQkl7qFT6"
      },
      "source": [
        "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device: {}\".format(device))\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
        "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    iteration = 1\n",
        "    logs = []\n",
        "\n",
        "    batch_multiplier = 6\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_val_loss = 0.0\n",
        "\n",
        "        print('--------------')\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('--------------')\n",
        "        \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                print('(train)')\n",
        "\n",
        "            else:\n",
        "                if ((epoch+1) % 5 == 0):\n",
        "                    net.eval()\n",
        "                    print('--------------')\n",
        "                    print('(val)')\n",
        "                else:\n",
        "                    continue\n",
        "        \n",
        "            count = 0\n",
        "            for images, anno_class_images in dataloaders_dict[phase]:\n",
        "                if images.size()[0] == 1:\n",
        "                    continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                anno_class_images = anno_class_images.to(device)\n",
        "\n",
        "                if (phase == 'train') and (count == 0):\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    count = batch_multiplier\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(images)\n",
        "                    loss = criterion(outputs, anno_class_images.long()) / batch_multiplier\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        count -= 1\n",
        "\n",
        "                        if (iteration % 20 == 0):\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('iteration {} || Loss: {:.4f} || 20iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                        epoch_train_loss += loss.item() * batch_multiplier\n",
        "                        iteration += 1\n",
        "\n",
        "                    else:\n",
        "                        epoch_val_loss += loss.item() * batch_multiplier\n",
        "\n",
        "        t_epoch_finish = time.time()\n",
        "        print('--------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss: {:.4f} || Epoch_VAL_Loss: {:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
        "        print('timer: {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss/num_train_imgs, \n",
        "                        'val_loss': epoch_val_loss/num_val_imgs}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(\"log_output_hr.csv\")\n",
        "\n",
        "        torch.save(net.state_dict(), 'weights/hrnet_' + str(epoch+1) + '.pth')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRaChXgJrefI",
        "outputId": "012bf489-29a0-4b2f-e051-9d0c0d606cdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_epochs = 30\n",
        "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda:0\n",
            "--------------\n",
            "Epoch 1/30\n",
            "--------------\n",
            "(train)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 20 || Loss: 0.7397 || 20iter: 100.8303 sec.\n",
            "iteration 40 || Loss: 0.6833 || 20iter: 92.7852 sec.\n",
            "iteration 60 || Loss: 0.5661 || 20iter: 95.0938 sec.\n",
            "iteration 80 || Loss: 0.3939 || 20iter: 96.1119 sec.\n",
            "iteration 100 || Loss: 0.3493 || 20iter: 98.8140 sec.\n",
            "iteration 120 || Loss: 0.1737 || 20iter: 97.8158 sec.\n",
            "iteration 140 || Loss: 0.3832 || 20iter: 93.7607 sec.\n",
            "iteration 160 || Loss: 0.4527 || 20iter: 94.8175 sec.\n",
            "iteration 180 || Loss: 0.2911 || 20iter: 93.8128 sec.\n",
            "iteration 200 || Loss: 0.4323 || 20iter: 96.7164 sec.\n",
            "iteration 220 || Loss: 0.1739 || 20iter: 94.6848 sec.\n",
            "iteration 240 || Loss: 0.1911 || 20iter: 96.5148 sec.\n",
            "iteration 260 || Loss: 0.2811 || 20iter: 126.3000 sec.\n",
            "iteration 280 || Loss: 0.2782 || 20iter: 95.3992 sec.\n",
            "iteration 300 || Loss: 0.4484 || 20iter: 97.5888 sec.\n",
            "iteration 320 || Loss: 0.2156 || 20iter: 99.8383 sec.\n",
            "iteration 340 || Loss: 0.3941 || 20iter: 94.9109 sec.\n",
            "iteration 360 || Loss: 0.4807 || 20iter: 96.2731 sec.\n",
            "--------------\n",
            "epoch 1 || Epoch_TRAIN_Loss: 0.3863 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 1800.2109 sec.\n",
            "--------------\n",
            "Epoch 2/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 380 || Loss: 0.2863 || 20iter: 10.2858 sec.\n",
            "iteration 400 || Loss: 0.1951 || 20iter: 14.9515 sec.\n",
            "iteration 420 || Loss: 0.1979 || 20iter: 14.8626 sec.\n",
            "iteration 440 || Loss: 0.2826 || 20iter: 14.9333 sec.\n",
            "iteration 460 || Loss: 0.1456 || 20iter: 14.9009 sec.\n",
            "iteration 480 || Loss: 0.1761 || 20iter: 14.8808 sec.\n",
            "iteration 500 || Loss: 0.2157 || 20iter: 14.9532 sec.\n",
            "iteration 520 || Loss: 0.1309 || 20iter: 14.8768 sec.\n",
            "iteration 540 || Loss: 0.5360 || 20iter: 14.8591 sec.\n",
            "iteration 560 || Loss: 0.1254 || 20iter: 14.8541 sec.\n",
            "iteration 580 || Loss: 0.2094 || 20iter: 14.7644 sec.\n",
            "iteration 600 || Loss: 0.1846 || 20iter: 14.8084 sec.\n",
            "iteration 620 || Loss: 0.2590 || 20iter: 14.7968 sec.\n",
            "iteration 640 || Loss: 0.2554 || 20iter: 14.8457 sec.\n",
            "iteration 660 || Loss: 0.4896 || 20iter: 14.8378 sec.\n",
            "iteration 680 || Loss: 0.2561 || 20iter: 14.8350 sec.\n",
            "iteration 700 || Loss: 0.1761 || 20iter: 14.7594 sec.\n",
            "iteration 720 || Loss: 0.2689 || 20iter: 14.7811 sec.\n",
            "--------------\n",
            "epoch 2 || Epoch_TRAIN_Loss: 0.2732 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 281.3506 sec.\n",
            "--------------\n",
            "Epoch 3/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 740 || Loss: 0.1778 || 20iter: 5.6243 sec.\n",
            "iteration 760 || Loss: 0.1961 || 20iter: 14.7638 sec.\n",
            "iteration 780 || Loss: 0.1692 || 20iter: 14.7919 sec.\n",
            "iteration 800 || Loss: 0.2863 || 20iter: 14.7813 sec.\n",
            "iteration 820 || Loss: 0.2177 || 20iter: 14.7301 sec.\n",
            "iteration 840 || Loss: 0.1274 || 20iter: 14.7683 sec.\n",
            "iteration 860 || Loss: 0.5761 || 20iter: 14.7749 sec.\n",
            "iteration 880 || Loss: 0.1602 || 20iter: 14.7556 sec.\n",
            "iteration 900 || Loss: 0.3916 || 20iter: 14.7303 sec.\n",
            "iteration 920 || Loss: 0.2166 || 20iter: 14.8145 sec.\n",
            "iteration 940 || Loss: 0.2098 || 20iter: 14.7333 sec.\n",
            "iteration 960 || Loss: 0.1709 || 20iter: 14.8094 sec.\n",
            "iteration 980 || Loss: 0.4274 || 20iter: 14.8133 sec.\n",
            "iteration 1000 || Loss: 0.1761 || 20iter: 14.7809 sec.\n",
            "iteration 1020 || Loss: 0.2039 || 20iter: 14.7792 sec.\n",
            "iteration 1040 || Loss: 0.4588 || 20iter: 14.8574 sec.\n",
            "iteration 1060 || Loss: 0.2326 || 20iter: 14.8629 sec.\n",
            "iteration 1080 || Loss: 0.1361 || 20iter: 14.8024 sec.\n",
            "--------------\n",
            "epoch 3 || Epoch_TRAIN_Loss: 0.2739 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 280.0727 sec.\n",
            "--------------\n",
            "Epoch 4/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 1100 || Loss: 0.3800 || 20iter: 1.0372 sec.\n",
            "iteration 1120 || Loss: 0.2201 || 20iter: 14.7439 sec.\n",
            "iteration 1140 || Loss: 0.1276 || 20iter: 14.7249 sec.\n",
            "iteration 1160 || Loss: 0.2262 || 20iter: 14.7828 sec.\n",
            "iteration 1180 || Loss: 0.3085 || 20iter: 14.7492 sec.\n",
            "iteration 1200 || Loss: 0.3958 || 20iter: 14.8004 sec.\n",
            "iteration 1220 || Loss: 0.3779 || 20iter: 14.7906 sec.\n",
            "iteration 1240 || Loss: 0.0884 || 20iter: 14.7146 sec.\n",
            "iteration 1260 || Loss: 0.1568 || 20iter: 14.8116 sec.\n",
            "iteration 1280 || Loss: 0.1619 || 20iter: 14.7310 sec.\n",
            "iteration 1300 || Loss: 0.3361 || 20iter: 14.7552 sec.\n",
            "iteration 1320 || Loss: 0.2868 || 20iter: 14.7510 sec.\n",
            "iteration 1340 || Loss: 0.1693 || 20iter: 14.7069 sec.\n",
            "iteration 1360 || Loss: 0.3547 || 20iter: 14.7703 sec.\n",
            "iteration 1380 || Loss: 0.1714 || 20iter: 14.7564 sec.\n",
            "iteration 1400 || Loss: 0.2534 || 20iter: 14.7377 sec.\n",
            "iteration 1420 || Loss: 0.1589 || 20iter: 14.7566 sec.\n",
            "iteration 1440 || Loss: 0.0759 || 20iter: 14.6988 sec.\n",
            "iteration 1460 || Loss: 0.3252 || 20iter: 14.7578 sec.\n",
            "--------------\n",
            "epoch 4 || Epoch_TRAIN_Loss: 0.2690 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 279.5132 sec.\n",
            "--------------\n",
            "Epoch 5/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 1480 || Loss: 0.3055 || 20iter: 11.7798 sec.\n",
            "iteration 1500 || Loss: 0.3257 || 20iter: 14.8077 sec.\n",
            "iteration 1520 || Loss: 0.4181 || 20iter: 14.8050 sec.\n",
            "iteration 1540 || Loss: 0.1451 || 20iter: 14.8017 sec.\n",
            "iteration 1560 || Loss: 0.1466 || 20iter: 14.8000 sec.\n",
            "iteration 1580 || Loss: 0.4188 || 20iter: 14.7934 sec.\n",
            "iteration 1600 || Loss: 0.2221 || 20iter: 14.8208 sec.\n",
            "iteration 1620 || Loss: 0.1944 || 20iter: 14.8345 sec.\n",
            "iteration 1640 || Loss: 0.4167 || 20iter: 14.8207 sec.\n",
            "iteration 1660 || Loss: 0.2979 || 20iter: 14.7760 sec.\n",
            "iteration 1680 || Loss: 0.2625 || 20iter: 14.7705 sec.\n",
            "iteration 1700 || Loss: 0.1819 || 20iter: 14.8299 sec.\n",
            "iteration 1720 || Loss: 0.3630 || 20iter: 14.7722 sec.\n",
            "iteration 1740 || Loss: 0.1962 || 20iter: 14.7945 sec.\n",
            "iteration 1760 || Loss: 0.2549 || 20iter: 14.8046 sec.\n",
            "iteration 1780 || Loss: 0.1489 || 20iter: 14.7456 sec.\n",
            "iteration 1800 || Loss: 0.0878 || 20iter: 14.8222 sec.\n",
            "iteration 1820 || Loss: 0.2459 || 20iter: 14.7710 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 5 || Epoch_TRAIN_Loss: 0.2667 || Epoch_VAL_Loss: 0.3080\n",
            "timer: 1817.7719 sec.\n",
            "--------------\n",
            "Epoch 6/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 1840 || Loss: 0.1221 || 20iter: 7.2282 sec.\n",
            "iteration 1860 || Loss: 0.2093 || 20iter: 14.8400 sec.\n",
            "iteration 1880 || Loss: 0.2594 || 20iter: 14.8936 sec.\n",
            "iteration 1900 || Loss: 0.3109 || 20iter: 14.9498 sec.\n",
            "iteration 1920 || Loss: 0.2355 || 20iter: 14.8563 sec.\n",
            "iteration 1940 || Loss: 0.3286 || 20iter: 14.8676 sec.\n",
            "iteration 1960 || Loss: 0.4474 || 20iter: 14.9041 sec.\n",
            "iteration 1980 || Loss: 0.3248 || 20iter: 14.8603 sec.\n",
            "iteration 2000 || Loss: 0.2399 || 20iter: 14.8976 sec.\n",
            "iteration 2020 || Loss: 0.2221 || 20iter: 14.8759 sec.\n",
            "iteration 2040 || Loss: 0.3490 || 20iter: 14.8650 sec.\n",
            "iteration 2060 || Loss: 0.3310 || 20iter: 14.8632 sec.\n",
            "iteration 2080 || Loss: 0.1348 || 20iter: 14.8452 sec.\n",
            "iteration 2100 || Loss: 0.1783 || 20iter: 14.8517 sec.\n",
            "iteration 2120 || Loss: 0.3922 || 20iter: 14.8971 sec.\n",
            "iteration 2140 || Loss: 0.2096 || 20iter: 14.9496 sec.\n",
            "iteration 2160 || Loss: 0.2476 || 20iter: 14.9884 sec.\n",
            "iteration 2180 || Loss: 0.3301 || 20iter: 14.9826 sec.\n",
            "--------------\n",
            "epoch 6 || Epoch_TRAIN_Loss: 0.2671 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.1604 sec.\n",
            "--------------\n",
            "Epoch 7/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 2200 || Loss: 0.2492 || 20iter: 2.6058 sec.\n",
            "iteration 2220 || Loss: 0.1155 || 20iter: 14.9008 sec.\n",
            "iteration 2240 || Loss: 0.2051 || 20iter: 14.8328 sec.\n",
            "iteration 2260 || Loss: 0.3995 || 20iter: 14.8109 sec.\n",
            "iteration 2280 || Loss: 0.2569 || 20iter: 14.8443 sec.\n",
            "iteration 2300 || Loss: 0.1385 || 20iter: 14.9419 sec.\n",
            "iteration 2320 || Loss: 0.4555 || 20iter: 14.8742 sec.\n",
            "iteration 2340 || Loss: 0.4331 || 20iter: 14.7990 sec.\n",
            "iteration 2360 || Loss: 0.1477 || 20iter: 14.8570 sec.\n",
            "iteration 2380 || Loss: 0.2858 || 20iter: 14.8201 sec.\n",
            "iteration 2400 || Loss: 0.4568 || 20iter: 14.8852 sec.\n",
            "iteration 2420 || Loss: 0.4973 || 20iter: 14.9088 sec.\n",
            "iteration 2440 || Loss: 0.2141 || 20iter: 14.8073 sec.\n",
            "iteration 2460 || Loss: 0.2066 || 20iter: 14.8523 sec.\n",
            "iteration 2480 || Loss: 0.2589 || 20iter: 14.8308 sec.\n",
            "iteration 2500 || Loss: 0.3285 || 20iter: 14.8262 sec.\n",
            "iteration 2520 || Loss: 0.1892 || 20iter: 14.8128 sec.\n",
            "iteration 2540 || Loss: 0.3517 || 20iter: 14.8596 sec.\n",
            "iteration 2560 || Loss: 0.3859 || 20iter: 14.8162 sec.\n",
            "--------------\n",
            "epoch 7 || Epoch_TRAIN_Loss: 0.2684 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 281.3069 sec.\n",
            "--------------\n",
            "Epoch 8/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 2580 || Loss: 0.2977 || 20iter: 13.3025 sec.\n",
            "iteration 2600 || Loss: 0.3117 || 20iter: 14.7610 sec.\n",
            "iteration 2620 || Loss: 0.2086 || 20iter: 14.7832 sec.\n",
            "iteration 2640 || Loss: 0.3470 || 20iter: 14.7893 sec.\n",
            "iteration 2660 || Loss: 0.4915 || 20iter: 14.7688 sec.\n",
            "iteration 2680 || Loss: 0.1896 || 20iter: 14.8007 sec.\n",
            "iteration 2700 || Loss: 0.5042 || 20iter: 14.8302 sec.\n",
            "iteration 2720 || Loss: 0.2317 || 20iter: 14.9104 sec.\n",
            "iteration 2740 || Loss: 0.3082 || 20iter: 14.8213 sec.\n",
            "iteration 2760 || Loss: 0.4253 || 20iter: 14.7839 sec.\n",
            "iteration 2780 || Loss: 0.1921 || 20iter: 14.7823 sec.\n",
            "iteration 2800 || Loss: 0.3816 || 20iter: 14.7536 sec.\n",
            "iteration 2820 || Loss: 0.2524 || 20iter: 14.7787 sec.\n",
            "iteration 2840 || Loss: 0.1767 || 20iter: 14.8782 sec.\n",
            "iteration 2860 || Loss: 0.2499 || 20iter: 14.8433 sec.\n",
            "iteration 2880 || Loss: 0.2564 || 20iter: 14.8870 sec.\n",
            "iteration 2900 || Loss: 0.2481 || 20iter: 14.8632 sec.\n",
            "iteration 2920 || Loss: 0.2023 || 20iter: 14.8119 sec.\n",
            "--------------\n",
            "epoch 8 || Epoch_TRAIN_Loss: 0.2651 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 280.6402 sec.\n",
            "--------------\n",
            "Epoch 9/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 2940 || Loss: 0.1757 || 20iter: 8.7271 sec.\n",
            "iteration 2960 || Loss: 0.2359 || 20iter: 14.7531 sec.\n",
            "iteration 2980 || Loss: 0.1053 || 20iter: 14.8018 sec.\n",
            "iteration 3000 || Loss: 0.2650 || 20iter: 14.7546 sec.\n",
            "iteration 3020 || Loss: 0.2243 || 20iter: 14.7953 sec.\n",
            "iteration 3040 || Loss: 0.1861 || 20iter: 14.7701 sec.\n",
            "iteration 3060 || Loss: 0.4893 || 20iter: 14.7637 sec.\n",
            "iteration 3080 || Loss: 0.2767 || 20iter: 14.7279 sec.\n",
            "iteration 3100 || Loss: 0.2679 || 20iter: 14.7480 sec.\n",
            "iteration 3120 || Loss: 0.3276 || 20iter: 14.8154 sec.\n",
            "iteration 3140 || Loss: 0.1540 || 20iter: 14.7404 sec.\n",
            "iteration 3160 || Loss: 0.4756 || 20iter: 14.8159 sec.\n",
            "iteration 3180 || Loss: 0.3652 || 20iter: 14.7070 sec.\n",
            "iteration 3200 || Loss: 0.4343 || 20iter: 14.7762 sec.\n",
            "iteration 3220 || Loss: 0.2603 || 20iter: 14.7245 sec.\n",
            "iteration 3240 || Loss: 0.2141 || 20iter: 14.6769 sec.\n",
            "iteration 3260 || Loss: 0.2557 || 20iter: 14.7862 sec.\n",
            "iteration 3280 || Loss: 0.1370 || 20iter: 14.7262 sec.\n",
            "--------------\n",
            "epoch 9 || Epoch_TRAIN_Loss: 0.2616 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 279.6784 sec.\n",
            "--------------\n",
            "Epoch 10/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 3300 || Loss: 0.5631 || 20iter: 4.0306 sec.\n",
            "iteration 3320 || Loss: 0.1570 || 20iter: 14.8021 sec.\n",
            "iteration 3340 || Loss: 0.2287 || 20iter: 14.6824 sec.\n",
            "iteration 3360 || Loss: 0.2052 || 20iter: 14.7495 sec.\n",
            "iteration 3380 || Loss: 0.2454 || 20iter: 14.7713 sec.\n",
            "iteration 3400 || Loss: 0.2905 || 20iter: 14.6827 sec.\n",
            "iteration 3420 || Loss: 0.4899 || 20iter: 14.7317 sec.\n",
            "iteration 3440 || Loss: 0.2369 || 20iter: 14.7066 sec.\n",
            "iteration 3460 || Loss: 0.2741 || 20iter: 14.7420 sec.\n",
            "iteration 3480 || Loss: 0.3007 || 20iter: 14.7151 sec.\n",
            "iteration 3500 || Loss: 0.3214 || 20iter: 14.6902 sec.\n",
            "iteration 3520 || Loss: 0.2493 || 20iter: 14.7433 sec.\n",
            "iteration 3540 || Loss: 0.3804 || 20iter: 14.7019 sec.\n",
            "iteration 3560 || Loss: 0.2224 || 20iter: 14.7117 sec.\n",
            "iteration 3580 || Loss: 0.2570 || 20iter: 14.7775 sec.\n",
            "iteration 3600 || Loss: 0.1103 || 20iter: 14.7343 sec.\n",
            "iteration 3620 || Loss: 0.2169 || 20iter: 14.6981 sec.\n",
            "iteration 3640 || Loss: 0.2182 || 20iter: 14.6962 sec.\n",
            "iteration 3660 || Loss: 0.3420 || 20iter: 14.7625 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 10 || Epoch_TRAIN_Loss: 0.2572 || Epoch_VAL_Loss: 0.2997\n",
            "timer: 354.0718 sec.\n",
            "--------------\n",
            "Epoch 11/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 3680 || Loss: 0.2201 || 20iter: 14.7422 sec.\n",
            "iteration 3700 || Loss: 0.1446 || 20iter: 14.7288 sec.\n",
            "iteration 3720 || Loss: 0.3782 || 20iter: 14.6721 sec.\n",
            "iteration 3740 || Loss: 0.1992 || 20iter: 14.6656 sec.\n",
            "iteration 3760 || Loss: 0.3680 || 20iter: 14.7255 sec.\n",
            "iteration 3780 || Loss: 0.4857 || 20iter: 14.6971 sec.\n",
            "iteration 3800 || Loss: 0.3732 || 20iter: 14.6981 sec.\n",
            "iteration 3820 || Loss: 0.2843 || 20iter: 14.6843 sec.\n",
            "iteration 3840 || Loss: 0.2489 || 20iter: 14.7527 sec.\n",
            "iteration 3860 || Loss: 0.2941 || 20iter: 14.7434 sec.\n",
            "iteration 3880 || Loss: 0.4038 || 20iter: 14.7782 sec.\n",
            "iteration 3900 || Loss: 0.3184 || 20iter: 14.7799 sec.\n",
            "iteration 3920 || Loss: 0.3608 || 20iter: 14.7791 sec.\n",
            "iteration 3940 || Loss: 0.2522 || 20iter: 14.7003 sec.\n",
            "iteration 3960 || Loss: 0.3275 || 20iter: 14.7337 sec.\n",
            "iteration 3980 || Loss: 0.1739 || 20iter: 14.6749 sec.\n",
            "iteration 4000 || Loss: 0.1087 || 20iter: 14.6982 sec.\n",
            "iteration 4020 || Loss: 0.3650 || 20iter: 14.6894 sec.\n",
            "--------------\n",
            "epoch 11 || Epoch_TRAIN_Loss: 0.2589 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 278.8955 sec.\n",
            "--------------\n",
            "Epoch 12/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 4040 || Loss: 0.1360 || 20iter: 10.1491 sec.\n",
            "iteration 4060 || Loss: 0.2103 || 20iter: 14.7348 sec.\n",
            "iteration 4080 || Loss: 0.1310 || 20iter: 14.6682 sec.\n",
            "iteration 4100 || Loss: 0.2449 || 20iter: 14.6959 sec.\n",
            "iteration 4120 || Loss: 0.2654 || 20iter: 14.6608 sec.\n",
            "iteration 4140 || Loss: 0.2145 || 20iter: 14.6870 sec.\n",
            "iteration 4160 || Loss: 0.1666 || 20iter: 14.7383 sec.\n",
            "iteration 4180 || Loss: 0.2393 || 20iter: 14.6900 sec.\n",
            "iteration 4200 || Loss: 0.1324 || 20iter: 14.7446 sec.\n",
            "iteration 4220 || Loss: 0.3105 || 20iter: 14.7537 sec.\n",
            "iteration 4240 || Loss: 0.4459 || 20iter: 14.7542 sec.\n",
            "iteration 4260 || Loss: 0.1791 || 20iter: 14.7283 sec.\n",
            "iteration 4280 || Loss: 0.1951 || 20iter: 14.6966 sec.\n",
            "iteration 4300 || Loss: 0.2682 || 20iter: 14.6977 sec.\n",
            "iteration 4320 || Loss: 0.1628 || 20iter: 14.7518 sec.\n",
            "iteration 4340 || Loss: 0.2322 || 20iter: 14.7095 sec.\n",
            "iteration 4360 || Loss: 0.2278 || 20iter: 14.7142 sec.\n",
            "iteration 4380 || Loss: 0.3376 || 20iter: 14.7424 sec.\n",
            "--------------\n",
            "epoch 12 || Epoch_TRAIN_Loss: 0.2535 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 278.8240 sec.\n",
            "--------------\n",
            "Epoch 13/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 4400 || Loss: 0.0936 || 20iter: 5.5582 sec.\n",
            "iteration 4420 || Loss: 0.4702 || 20iter: 14.7613 sec.\n",
            "iteration 4440 || Loss: 0.2000 || 20iter: 14.7046 sec.\n",
            "iteration 4460 || Loss: 0.3548 || 20iter: 14.7528 sec.\n",
            "iteration 4480 || Loss: 0.2789 || 20iter: 14.6589 sec.\n",
            "iteration 4500 || Loss: 0.1011 || 20iter: 14.6840 sec.\n",
            "iteration 4520 || Loss: 0.2590 || 20iter: 14.7739 sec.\n",
            "iteration 4540 || Loss: 0.1434 || 20iter: 14.8685 sec.\n",
            "iteration 4560 || Loss: 0.2492 || 20iter: 14.8433 sec.\n",
            "iteration 4580 || Loss: 0.1836 || 20iter: 14.9120 sec.\n",
            "iteration 4600 || Loss: 0.2165 || 20iter: 14.8811 sec.\n",
            "iteration 4620 || Loss: 0.1927 || 20iter: 14.7916 sec.\n",
            "iteration 4640 || Loss: 0.2898 || 20iter: 14.8002 sec.\n",
            "iteration 4660 || Loss: 0.2855 || 20iter: 14.7243 sec.\n",
            "iteration 4680 || Loss: 0.1590 || 20iter: 14.8021 sec.\n",
            "iteration 4700 || Loss: 0.2430 || 20iter: 14.7341 sec.\n",
            "iteration 4720 || Loss: 0.3723 || 20iter: 14.7650 sec.\n",
            "iteration 4740 || Loss: 0.2514 || 20iter: 14.7849 sec.\n",
            "--------------\n",
            "epoch 13 || Epoch_TRAIN_Loss: 0.2587 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 279.9432 sec.\n",
            "--------------\n",
            "Epoch 14/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 4760 || Loss: 0.1934 || 20iter: 1.0468 sec.\n",
            "iteration 4780 || Loss: 0.2563 || 20iter: 14.7598 sec.\n",
            "iteration 4800 || Loss: 0.2745 || 20iter: 14.7048 sec.\n",
            "iteration 4820 || Loss: 0.1724 || 20iter: 14.7779 sec.\n",
            "iteration 4840 || Loss: 0.2875 || 20iter: 14.7749 sec.\n",
            "iteration 4860 || Loss: 0.2890 || 20iter: 14.7947 sec.\n",
            "iteration 4880 || Loss: 0.1520 || 20iter: 14.7772 sec.\n",
            "iteration 4900 || Loss: 0.2776 || 20iter: 14.7337 sec.\n",
            "iteration 4920 || Loss: 0.3203 || 20iter: 14.8088 sec.\n",
            "iteration 4940 || Loss: 0.2741 || 20iter: 14.7193 sec.\n",
            "iteration 4960 || Loss: 0.0982 || 20iter: 14.8027 sec.\n",
            "iteration 4980 || Loss: 0.1811 || 20iter: 14.6933 sec.\n",
            "iteration 5000 || Loss: 0.1978 || 20iter: 14.7248 sec.\n",
            "iteration 5020 || Loss: 0.3806 || 20iter: 14.7320 sec.\n",
            "iteration 5040 || Loss: 0.1668 || 20iter: 14.7364 sec.\n",
            "iteration 5060 || Loss: 0.1981 || 20iter: 14.8353 sec.\n",
            "iteration 5080 || Loss: 0.2215 || 20iter: 14.7656 sec.\n",
            "iteration 5100 || Loss: 0.3010 || 20iter: 14.7147 sec.\n",
            "iteration 5120 || Loss: 0.2462 || 20iter: 14.7616 sec.\n",
            "--------------\n",
            "epoch 14 || Epoch_TRAIN_Loss: 0.2559 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 279.6067 sec.\n",
            "--------------\n",
            "Epoch 15/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 5140 || Loss: 0.3893 || 20iter: 11.6710 sec.\n",
            "iteration 5160 || Loss: 0.3135 || 20iter: 14.7310 sec.\n",
            "iteration 5180 || Loss: 0.1481 || 20iter: 14.7547 sec.\n",
            "iteration 5200 || Loss: 0.1751 || 20iter: 14.7335 sec.\n",
            "iteration 5220 || Loss: 0.1907 || 20iter: 14.7219 sec.\n",
            "iteration 5240 || Loss: 0.1472 || 20iter: 14.7817 sec.\n",
            "iteration 5260 || Loss: 0.2780 || 20iter: 14.7408 sec.\n",
            "iteration 5280 || Loss: 0.3525 || 20iter: 14.7010 sec.\n",
            "iteration 5300 || Loss: 0.2620 || 20iter: 14.6853 sec.\n",
            "iteration 5320 || Loss: 0.2059 || 20iter: 14.7305 sec.\n",
            "iteration 5340 || Loss: 0.2483 || 20iter: 14.7249 sec.\n",
            "iteration 5360 || Loss: 0.3259 || 20iter: 14.7591 sec.\n",
            "iteration 5380 || Loss: 0.1736 || 20iter: 14.7640 sec.\n",
            "iteration 5400 || Loss: 0.3067 || 20iter: 14.7509 sec.\n",
            "iteration 5420 || Loss: 0.2805 || 20iter: 14.7370 sec.\n",
            "iteration 5440 || Loss: 0.2942 || 20iter: 14.8027 sec.\n",
            "iteration 5460 || Loss: 0.2024 || 20iter: 14.8187 sec.\n",
            "iteration 5480 || Loss: 0.1701 || 20iter: 14.8177 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 15 || Epoch_TRAIN_Loss: 0.2537 || Epoch_VAL_Loss: 0.2938\n",
            "timer: 355.2403 sec.\n",
            "--------------\n",
            "Epoch 16/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 5500 || Loss: 0.1060 || 20iter: 7.1430 sec.\n",
            "iteration 5520 || Loss: 0.5702 || 20iter: 14.7589 sec.\n",
            "iteration 5540 || Loss: 0.2288 || 20iter: 14.7156 sec.\n",
            "iteration 5560 || Loss: 0.1812 || 20iter: 14.6563 sec.\n",
            "iteration 5580 || Loss: 0.2716 || 20iter: 14.7322 sec.\n",
            "iteration 5600 || Loss: 0.2357 || 20iter: 14.7434 sec.\n",
            "iteration 5620 || Loss: 0.4196 || 20iter: 14.7306 sec.\n",
            "iteration 5640 || Loss: 0.2086 || 20iter: 14.6984 sec.\n",
            "iteration 5660 || Loss: 0.1431 || 20iter: 14.7204 sec.\n",
            "iteration 5680 || Loss: 0.2482 || 20iter: 14.6780 sec.\n",
            "iteration 5700 || Loss: 0.4000 || 20iter: 14.7783 sec.\n",
            "iteration 5720 || Loss: 0.2271 || 20iter: 14.6869 sec.\n",
            "iteration 5740 || Loss: 0.2373 || 20iter: 14.6910 sec.\n",
            "iteration 5760 || Loss: 0.1496 || 20iter: 14.6912 sec.\n",
            "iteration 5780 || Loss: 0.3150 || 20iter: 14.7724 sec.\n",
            "iteration 5800 || Loss: 0.3154 || 20iter: 14.7122 sec.\n",
            "iteration 5820 || Loss: 0.1072 || 20iter: 14.6961 sec.\n",
            "iteration 5840 || Loss: 0.3639 || 20iter: 14.7026 sec.\n",
            "--------------\n",
            "epoch 16 || Epoch_TRAIN_Loss: 0.2548 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 278.8626 sec.\n",
            "--------------\n",
            "Epoch 17/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 5860 || Loss: 0.2498 || 20iter: 2.5372 sec.\n",
            "iteration 5880 || Loss: 0.1495 || 20iter: 14.6965 sec.\n",
            "iteration 5900 || Loss: 0.2493 || 20iter: 14.7504 sec.\n",
            "iteration 5920 || Loss: 0.3632 || 20iter: 14.7354 sec.\n",
            "iteration 5940 || Loss: 0.4780 || 20iter: 14.6510 sec.\n",
            "iteration 5960 || Loss: 0.3459 || 20iter: 14.7136 sec.\n",
            "iteration 5980 || Loss: 0.3379 || 20iter: 14.6880 sec.\n",
            "iteration 6000 || Loss: 0.2760 || 20iter: 14.7122 sec.\n",
            "iteration 6020 || Loss: 0.1193 || 20iter: 14.7662 sec.\n",
            "iteration 6040 || Loss: 0.2392 || 20iter: 14.7603 sec.\n",
            "iteration 6060 || Loss: 0.1301 || 20iter: 14.6264 sec.\n",
            "iteration 6080 || Loss: 0.2282 || 20iter: 14.7159 sec.\n",
            "iteration 6100 || Loss: 0.3092 || 20iter: 14.6802 sec.\n",
            "iteration 6120 || Loss: 0.2731 || 20iter: 14.7010 sec.\n",
            "iteration 6140 || Loss: 0.2557 || 20iter: 14.7241 sec.\n",
            "iteration 6160 || Loss: 0.2995 || 20iter: 14.6892 sec.\n",
            "iteration 6180 || Loss: 0.1736 || 20iter: 14.7904 sec.\n",
            "iteration 6200 || Loss: 0.1858 || 20iter: 14.6685 sec.\n",
            "iteration 6220 || Loss: 0.2003 || 20iter: 14.7462 sec.\n",
            "--------------\n",
            "epoch 17 || Epoch_TRAIN_Loss: 0.2546 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 278.7698 sec.\n",
            "--------------\n",
            "Epoch 18/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 6240 || Loss: 0.2022 || 20iter: 13.2284 sec.\n",
            "iteration 6260 || Loss: 0.1736 || 20iter: 14.7265 sec.\n",
            "iteration 6280 || Loss: 0.3403 || 20iter: 14.7023 sec.\n",
            "iteration 6300 || Loss: 0.1789 || 20iter: 14.7087 sec.\n",
            "iteration 6320 || Loss: 0.2217 || 20iter: 14.6870 sec.\n",
            "iteration 6340 || Loss: 0.1264 || 20iter: 14.7088 sec.\n",
            "iteration 6360 || Loss: 0.3457 || 20iter: 14.6390 sec.\n",
            "iteration 6380 || Loss: 0.1992 || 20iter: 14.6840 sec.\n",
            "iteration 6400 || Loss: 0.2123 || 20iter: 14.7138 sec.\n",
            "iteration 6420 || Loss: 0.2015 || 20iter: 14.7233 sec.\n",
            "iteration 6440 || Loss: 0.3408 || 20iter: 14.7450 sec.\n",
            "iteration 6460 || Loss: 0.4032 || 20iter: 14.7290 sec.\n",
            "iteration 6480 || Loss: 0.0851 || 20iter: 14.6964 sec.\n",
            "iteration 6500 || Loss: 0.1973 || 20iter: 14.6890 sec.\n",
            "iteration 6520 || Loss: 0.3618 || 20iter: 14.7243 sec.\n",
            "iteration 6540 || Loss: 0.2136 || 20iter: 14.7872 sec.\n",
            "iteration 6560 || Loss: 0.1571 || 20iter: 14.7461 sec.\n",
            "iteration 6580 || Loss: 0.1481 || 20iter: 14.7931 sec.\n",
            "--------------\n",
            "epoch 18 || Epoch_TRAIN_Loss: 0.2529 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 278.9273 sec.\n",
            "--------------\n",
            "Epoch 19/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 6600 || Loss: 0.2970 || 20iter: 8.6573 sec.\n",
            "iteration 6620 || Loss: 0.3059 || 20iter: 14.7253 sec.\n",
            "iteration 6640 || Loss: 0.1429 || 20iter: 14.6758 sec.\n",
            "iteration 6660 || Loss: 0.1795 || 20iter: 14.7321 sec.\n",
            "iteration 6680 || Loss: 0.1662 || 20iter: 14.6600 sec.\n",
            "iteration 6700 || Loss: 0.2214 || 20iter: 14.5929 sec.\n",
            "iteration 6720 || Loss: 0.1458 || 20iter: 14.6635 sec.\n",
            "iteration 6740 || Loss: 0.3265 || 20iter: 14.7084 sec.\n",
            "iteration 6760 || Loss: 0.2297 || 20iter: 14.7668 sec.\n",
            "iteration 6780 || Loss: 0.2052 || 20iter: 14.6552 sec.\n",
            "iteration 6800 || Loss: 0.2674 || 20iter: 14.7117 sec.\n",
            "iteration 6820 || Loss: 0.1747 || 20iter: 14.6696 sec.\n",
            "iteration 6840 || Loss: 0.1736 || 20iter: 14.6853 sec.\n",
            "iteration 6860 || Loss: 0.1497 || 20iter: 14.6623 sec.\n",
            "iteration 6880 || Loss: 0.4633 || 20iter: 14.6983 sec.\n",
            "iteration 6900 || Loss: 0.3698 || 20iter: 14.7509 sec.\n",
            "iteration 6920 || Loss: 0.1816 || 20iter: 14.7187 sec.\n",
            "iteration 6940 || Loss: 0.1213 || 20iter: 14.6548 sec.\n",
            "--------------\n",
            "epoch 19 || Epoch_TRAIN_Loss: 0.2566 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 278.4095 sec.\n",
            "--------------\n",
            "Epoch 20/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 6960 || Loss: 0.2432 || 20iter: 4.0465 sec.\n",
            "iteration 6980 || Loss: 0.2283 || 20iter: 14.7403 sec.\n",
            "iteration 7000 || Loss: 0.2303 || 20iter: 14.7783 sec.\n",
            "iteration 7020 || Loss: 0.1084 || 20iter: 14.6525 sec.\n",
            "iteration 7040 || Loss: 0.2040 || 20iter: 14.6818 sec.\n",
            "iteration 7060 || Loss: 0.2801 || 20iter: 14.6552 sec.\n",
            "iteration 7080 || Loss: 0.2228 || 20iter: 14.7390 sec.\n",
            "iteration 7100 || Loss: 0.4276 || 20iter: 14.6849 sec.\n",
            "iteration 7120 || Loss: 0.2569 || 20iter: 14.6771 sec.\n",
            "iteration 7140 || Loss: 0.4923 || 20iter: 14.6887 sec.\n",
            "iteration 7160 || Loss: 0.1824 || 20iter: 14.7015 sec.\n",
            "iteration 7180 || Loss: 0.2461 || 20iter: 14.7294 sec.\n",
            "iteration 7200 || Loss: 0.3124 || 20iter: 14.6566 sec.\n",
            "iteration 7220 || Loss: 0.3351 || 20iter: 14.6768 sec.\n",
            "iteration 7240 || Loss: 0.2840 || 20iter: 14.6418 sec.\n",
            "iteration 7260 || Loss: 0.2783 || 20iter: 14.6416 sec.\n",
            "iteration 7280 || Loss: 0.1972 || 20iter: 14.7277 sec.\n",
            "iteration 7300 || Loss: 0.2196 || 20iter: 14.7389 sec.\n",
            "iteration 7320 || Loss: 0.3182 || 20iter: 14.7715 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 20 || Epoch_TRAIN_Loss: 0.2577 || Epoch_VAL_Loss: 0.2987\n",
            "timer: 354.1646 sec.\n",
            "--------------\n",
            "Epoch 21/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 7340 || Loss: 0.4445 || 20iter: 14.7127 sec.\n",
            "iteration 7360 || Loss: 0.2600 || 20iter: 14.7569 sec.\n",
            "iteration 7380 || Loss: 0.3653 || 20iter: 14.6730 sec.\n",
            "iteration 7400 || Loss: 0.1624 || 20iter: 14.7069 sec.\n",
            "iteration 7420 || Loss: 0.1390 || 20iter: 14.6904 sec.\n",
            "iteration 7440 || Loss: 0.3874 || 20iter: 14.6756 sec.\n",
            "iteration 7460 || Loss: 0.4666 || 20iter: 14.6830 sec.\n",
            "iteration 7480 || Loss: 0.1719 || 20iter: 14.6665 sec.\n",
            "iteration 7500 || Loss: 0.2816 || 20iter: 14.6926 sec.\n",
            "iteration 7520 || Loss: 0.1649 || 20iter: 14.6954 sec.\n",
            "iteration 7540 || Loss: 0.2132 || 20iter: 14.6680 sec.\n",
            "iteration 7560 || Loss: 0.4483 || 20iter: 14.7273 sec.\n",
            "iteration 7580 || Loss: 0.2683 || 20iter: 14.6826 sec.\n",
            "iteration 7600 || Loss: 0.2344 || 20iter: 14.6730 sec.\n",
            "iteration 7620 || Loss: 0.4103 || 20iter: 14.7014 sec.\n",
            "iteration 7640 || Loss: 0.2152 || 20iter: 14.6336 sec.\n",
            "iteration 7660 || Loss: 0.1617 || 20iter: 14.6385 sec.\n",
            "iteration 7680 || Loss: 0.1962 || 20iter: 14.6660 sec.\n",
            "--------------\n",
            "epoch 21 || Epoch_TRAIN_Loss: 0.2560 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 278.2617 sec.\n",
            "--------------\n",
            "Epoch 22/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 7700 || Loss: 0.2572 || 20iter: 10.1116 sec.\n",
            "iteration 7720 || Loss: 0.1448 || 20iter: 14.7375 sec.\n",
            "iteration 7740 || Loss: 0.1888 || 20iter: 14.7143 sec.\n",
            "iteration 7760 || Loss: 0.3227 || 20iter: 14.7132 sec.\n",
            "iteration 7780 || Loss: 0.1514 || 20iter: 14.6514 sec.\n",
            "iteration 7800 || Loss: 0.4142 || 20iter: 14.6760 sec.\n",
            "iteration 7820 || Loss: 0.3011 || 20iter: 14.6719 sec.\n",
            "iteration 7840 || Loss: 0.1728 || 20iter: 14.6829 sec.\n",
            "iteration 7860 || Loss: 0.3094 || 20iter: 14.6409 sec.\n",
            "iteration 7880 || Loss: 0.3870 || 20iter: 14.7025 sec.\n",
            "iteration 7900 || Loss: 0.4035 || 20iter: 14.6924 sec.\n",
            "iteration 7920 || Loss: 0.4271 || 20iter: 14.6741 sec.\n",
            "iteration 7940 || Loss: 0.2620 || 20iter: 14.7156 sec.\n",
            "iteration 7960 || Loss: 0.2032 || 20iter: 14.6841 sec.\n",
            "iteration 7980 || Loss: 0.3812 || 20iter: 14.6848 sec.\n",
            "iteration 8000 || Loss: 0.2133 || 20iter: 14.7054 sec.\n",
            "iteration 8020 || Loss: 0.3265 || 20iter: 14.6665 sec.\n",
            "iteration 8040 || Loss: 0.1850 || 20iter: 14.6784 sec.\n",
            "--------------\n",
            "epoch 22 || Epoch_TRAIN_Loss: 0.2520 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 278.3083 sec.\n",
            "--------------\n",
            "Epoch 23/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 8060 || Loss: 0.2731 || 20iter: 5.5662 sec.\n",
            "iteration 8080 || Loss: 0.2711 || 20iter: 14.6944 sec.\n",
            "iteration 8100 || Loss: 0.1868 || 20iter: 14.6780 sec.\n",
            "iteration 8120 || Loss: 0.1540 || 20iter: 14.8173 sec.\n",
            "iteration 8140 || Loss: 0.3904 || 20iter: 14.7339 sec.\n",
            "iteration 8160 || Loss: 0.2477 || 20iter: 14.7607 sec.\n",
            "iteration 8180 || Loss: 0.3616 || 20iter: 14.7700 sec.\n",
            "iteration 8200 || Loss: 0.0678 || 20iter: 14.7907 sec.\n",
            "iteration 8220 || Loss: 0.2633 || 20iter: 14.8755 sec.\n",
            "iteration 8240 || Loss: 0.3923 || 20iter: 15.0249 sec.\n",
            "iteration 8260 || Loss: 0.2445 || 20iter: 14.8674 sec.\n",
            "iteration 8280 || Loss: 0.3124 || 20iter: 14.7025 sec.\n",
            "iteration 8300 || Loss: 0.2624 || 20iter: 14.7799 sec.\n",
            "iteration 8320 || Loss: 0.2575 || 20iter: 14.7142 sec.\n",
            "iteration 8340 || Loss: 0.5120 || 20iter: 14.7001 sec.\n",
            "iteration 8360 || Loss: 0.1062 || 20iter: 14.7548 sec.\n",
            "iteration 8380 || Loss: 0.2357 || 20iter: 14.7392 sec.\n",
            "iteration 8400 || Loss: 0.3724 || 20iter: 14.7935 sec.\n",
            "--------------\n",
            "epoch 23 || Epoch_TRAIN_Loss: 0.2571 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 279.8764 sec.\n",
            "--------------\n",
            "Epoch 24/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 8420 || Loss: 0.2028 || 20iter: 1.0145 sec.\n",
            "iteration 8440 || Loss: 0.2376 || 20iter: 14.7570 sec.\n",
            "iteration 8460 || Loss: 0.2600 || 20iter: 14.7047 sec.\n",
            "iteration 8480 || Loss: 0.1892 || 20iter: 14.8082 sec.\n",
            "iteration 8500 || Loss: 0.3552 || 20iter: 14.7800 sec.\n",
            "iteration 8520 || Loss: 0.3315 || 20iter: 14.8232 sec.\n",
            "iteration 8540 || Loss: 0.4503 || 20iter: 14.8368 sec.\n",
            "iteration 8560 || Loss: 0.2129 || 20iter: 14.7511 sec.\n",
            "iteration 8580 || Loss: 0.3368 || 20iter: 14.6935 sec.\n",
            "iteration 8600 || Loss: 0.2987 || 20iter: 14.8079 sec.\n",
            "iteration 8620 || Loss: 0.2842 || 20iter: 14.7717 sec.\n",
            "iteration 8640 || Loss: 0.1376 || 20iter: 14.7289 sec.\n",
            "iteration 8660 || Loss: 0.1849 || 20iter: 14.7679 sec.\n",
            "iteration 8680 || Loss: 0.1365 || 20iter: 14.7585 sec.\n",
            "iteration 8700 || Loss: 0.2263 || 20iter: 14.6992 sec.\n",
            "iteration 8720 || Loss: 0.2511 || 20iter: 14.7121 sec.\n",
            "iteration 8740 || Loss: 0.2549 || 20iter: 14.7291 sec.\n",
            "iteration 8760 || Loss: 0.1981 || 20iter: 14.7079 sec.\n",
            "iteration 8780 || Loss: 0.3248 || 20iter: 14.7996 sec.\n",
            "--------------\n",
            "epoch 24 || Epoch_TRAIN_Loss: 0.2566 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 279.6052 sec.\n",
            "--------------\n",
            "Epoch 25/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 8800 || Loss: 0.3561 || 20iter: 11.6985 sec.\n",
            "iteration 8820 || Loss: 0.1711 || 20iter: 14.7417 sec.\n",
            "iteration 8840 || Loss: 0.2478 || 20iter: 14.7516 sec.\n",
            "iteration 8860 || Loss: 0.1294 || 20iter: 14.7753 sec.\n",
            "iteration 8880 || Loss: 0.2798 || 20iter: 14.8256 sec.\n",
            "iteration 8900 || Loss: 0.2179 || 20iter: 14.7624 sec.\n",
            "iteration 8920 || Loss: 0.2829 || 20iter: 14.7385 sec.\n",
            "iteration 8940 || Loss: 0.2572 || 20iter: 14.7928 sec.\n",
            "iteration 8960 || Loss: 0.3712 || 20iter: 14.7367 sec.\n",
            "iteration 8980 || Loss: 0.2723 || 20iter: 14.6663 sec.\n",
            "iteration 9000 || Loss: 0.1054 || 20iter: 14.7579 sec.\n",
            "iteration 9020 || Loss: 0.2820 || 20iter: 14.7427 sec.\n",
            "iteration 9040 || Loss: 0.4562 || 20iter: 14.7107 sec.\n",
            "iteration 9060 || Loss: 0.2018 || 20iter: 14.7186 sec.\n",
            "iteration 9080 || Loss: 0.1451 || 20iter: 14.7699 sec.\n",
            "iteration 9100 || Loss: 0.1857 || 20iter: 14.7787 sec.\n",
            "iteration 9120 || Loss: 0.3442 || 20iter: 14.6983 sec.\n",
            "iteration 9140 || Loss: 0.3705 || 20iter: 14.7824 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 25 || Epoch_TRAIN_Loss: 0.2534 || Epoch_VAL_Loss: 0.2868\n",
            "timer: 354.8570 sec.\n",
            "--------------\n",
            "Epoch 26/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 9160 || Loss: 0.3247 || 20iter: 7.1257 sec.\n",
            "iteration 9180 || Loss: 0.3478 || 20iter: 14.7873 sec.\n",
            "iteration 9200 || Loss: 0.2744 || 20iter: 14.7237 sec.\n",
            "iteration 9220 || Loss: 0.1701 || 20iter: 14.7726 sec.\n",
            "iteration 9240 || Loss: 0.2149 || 20iter: 14.8347 sec.\n",
            "iteration 9260 || Loss: 0.3290 || 20iter: 14.6941 sec.\n",
            "iteration 9280 || Loss: 0.2920 || 20iter: 14.6912 sec.\n",
            "iteration 9300 || Loss: 0.1954 || 20iter: 14.7341 sec.\n",
            "iteration 9320 || Loss: 0.2320 || 20iter: 14.8044 sec.\n",
            "iteration 9340 || Loss: 0.5088 || 20iter: 14.7801 sec.\n",
            "iteration 9360 || Loss: 0.1538 || 20iter: 14.7411 sec.\n",
            "iteration 9380 || Loss: 0.1518 || 20iter: 14.7478 sec.\n",
            "iteration 9400 || Loss: 0.2297 || 20iter: 14.7545 sec.\n",
            "iteration 9420 || Loss: 0.1744 || 20iter: 14.8288 sec.\n",
            "iteration 9440 || Loss: 0.2319 || 20iter: 14.8078 sec.\n",
            "iteration 9460 || Loss: 0.1270 || 20iter: 14.7651 sec.\n",
            "iteration 9480 || Loss: 0.1078 || 20iter: 14.7333 sec.\n",
            "iteration 9500 || Loss: 0.4462 || 20iter: 14.7468 sec.\n",
            "--------------\n",
            "epoch 26 || Epoch_TRAIN_Loss: 0.2541 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 279.6493 sec.\n",
            "--------------\n",
            "Epoch 27/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 9520 || Loss: 0.1681 || 20iter: 2.5616 sec.\n",
            "iteration 9540 || Loss: 0.2137 || 20iter: 14.7040 sec.\n",
            "iteration 9560 || Loss: 0.1831 || 20iter: 14.7251 sec.\n",
            "iteration 9580 || Loss: 0.1770 || 20iter: 14.7442 sec.\n",
            "iteration 9600 || Loss: 0.1023 || 20iter: 14.6885 sec.\n",
            "iteration 9620 || Loss: 0.3108 || 20iter: 14.7778 sec.\n",
            "iteration 9640 || Loss: 0.1482 || 20iter: 14.7313 sec.\n",
            "iteration 9660 || Loss: 0.2517 || 20iter: 14.7782 sec.\n",
            "iteration 9680 || Loss: 0.3725 || 20iter: 14.7535 sec.\n",
            "iteration 9700 || Loss: 0.1937 || 20iter: 14.7238 sec.\n",
            "iteration 9720 || Loss: 0.3043 || 20iter: 14.7253 sec.\n",
            "iteration 9740 || Loss: 0.2314 || 20iter: 14.7628 sec.\n",
            "iteration 9760 || Loss: 0.1550 || 20iter: 14.7582 sec.\n",
            "iteration 9780 || Loss: 0.3311 || 20iter: 14.7255 sec.\n",
            "iteration 9800 || Loss: 0.3509 || 20iter: 14.8148 sec.\n",
            "iteration 9820 || Loss: 0.2134 || 20iter: 14.7721 sec.\n",
            "iteration 9840 || Loss: 0.2329 || 20iter: 14.7958 sec.\n",
            "iteration 9860 || Loss: 0.1553 || 20iter: 14.7636 sec.\n",
            "iteration 9880 || Loss: 0.0833 || 20iter: 14.7744 sec.\n",
            "--------------\n",
            "epoch 27 || Epoch_TRAIN_Loss: 0.2466 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 279.5017 sec.\n",
            "--------------\n",
            "Epoch 28/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 9900 || Loss: 0.2004 || 20iter: 13.2903 sec.\n",
            "iteration 9920 || Loss: 0.2649 || 20iter: 14.8199 sec.\n",
            "iteration 9940 || Loss: 0.3248 || 20iter: 14.7507 sec.\n",
            "iteration 9960 || Loss: 0.2801 || 20iter: 14.7787 sec.\n",
            "iteration 9980 || Loss: 0.4036 || 20iter: 14.7748 sec.\n",
            "iteration 10000 || Loss: 0.3373 || 20iter: 14.7838 sec.\n",
            "iteration 10020 || Loss: 0.2017 || 20iter: 14.7266 sec.\n",
            "iteration 10040 || Loss: 0.4197 || 20iter: 14.7679 sec.\n",
            "iteration 10060 || Loss: 0.1379 || 20iter: 14.9107 sec.\n",
            "iteration 10080 || Loss: 0.2670 || 20iter: 14.8265 sec.\n",
            "iteration 10100 || Loss: 0.2166 || 20iter: 14.7957 sec.\n",
            "iteration 10120 || Loss: 0.2821 || 20iter: 14.7861 sec.\n",
            "iteration 10140 || Loss: 0.2770 || 20iter: 14.7499 sec.\n",
            "iteration 10160 || Loss: 0.1488 || 20iter: 14.8723 sec.\n",
            "iteration 10180 || Loss: 0.2741 || 20iter: 14.7556 sec.\n",
            "iteration 10200 || Loss: 0.2127 || 20iter: 14.7242 sec.\n",
            "iteration 10220 || Loss: 0.1881 || 20iter: 14.7460 sec.\n",
            "iteration 10240 || Loss: 0.1660 || 20iter: 14.7440 sec.\n",
            "--------------\n",
            "epoch 28 || Epoch_TRAIN_Loss: 0.2522 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 280.0523 sec.\n",
            "--------------\n",
            "Epoch 29/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 10260 || Loss: 0.2639 || 20iter: 8.6915 sec.\n",
            "iteration 10280 || Loss: 0.2491 || 20iter: 14.7460 sec.\n",
            "iteration 10300 || Loss: 0.4077 || 20iter: 14.7234 sec.\n",
            "iteration 10320 || Loss: 0.2380 || 20iter: 14.7181 sec.\n",
            "iteration 10340 || Loss: 0.3562 || 20iter: 14.7534 sec.\n",
            "iteration 10360 || Loss: 0.4619 || 20iter: 14.7678 sec.\n",
            "iteration 10380 || Loss: 0.5098 || 20iter: 14.7278 sec.\n",
            "iteration 10400 || Loss: 0.1887 || 20iter: 14.7309 sec.\n",
            "iteration 10420 || Loss: 0.1892 || 20iter: 14.7705 sec.\n",
            "iteration 10440 || Loss: 0.2579 || 20iter: 14.7363 sec.\n",
            "iteration 10460 || Loss: 0.1429 || 20iter: 14.8101 sec.\n",
            "iteration 10480 || Loss: 0.2736 || 20iter: 14.7080 sec.\n",
            "iteration 10500 || Loss: 0.2748 || 20iter: 14.7431 sec.\n",
            "iteration 10520 || Loss: 0.1354 || 20iter: 14.6651 sec.\n",
            "iteration 10540 || Loss: 0.1791 || 20iter: 14.7249 sec.\n",
            "iteration 10560 || Loss: 0.0885 || 20iter: 14.6888 sec.\n",
            "iteration 10580 || Loss: 0.1223 || 20iter: 14.7236 sec.\n",
            "iteration 10600 || Loss: 0.4443 || 20iter: 14.7233 sec.\n",
            "--------------\n",
            "epoch 29 || Epoch_TRAIN_Loss: 0.2546 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 279.1840 sec.\n",
            "--------------\n",
            "Epoch 30/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 10620 || Loss: 0.1280 || 20iter: 4.0404 sec.\n",
            "iteration 10640 || Loss: 0.1276 || 20iter: 14.8115 sec.\n",
            "iteration 10660 || Loss: 0.3241 || 20iter: 14.7700 sec.\n",
            "iteration 10680 || Loss: 0.3406 || 20iter: 14.8638 sec.\n",
            "iteration 10700 || Loss: 0.2520 || 20iter: 14.8332 sec.\n",
            "iteration 10720 || Loss: 0.2129 || 20iter: 14.7727 sec.\n",
            "iteration 10740 || Loss: 0.2064 || 20iter: 14.7410 sec.\n",
            "iteration 10760 || Loss: 0.1097 || 20iter: 14.7572 sec.\n",
            "iteration 10780 || Loss: 0.2293 || 20iter: 14.8291 sec.\n",
            "iteration 10800 || Loss: 0.4376 || 20iter: 14.8281 sec.\n",
            "iteration 10820 || Loss: 0.1944 || 20iter: 14.8522 sec.\n",
            "iteration 10840 || Loss: 0.2528 || 20iter: 14.7797 sec.\n",
            "iteration 10860 || Loss: 0.3050 || 20iter: 14.8537 sec.\n",
            "iteration 10880 || Loss: 0.1039 || 20iter: 14.9227 sec.\n",
            "iteration 10900 || Loss: 0.1703 || 20iter: 14.9665 sec.\n",
            "iteration 10920 || Loss: 0.4236 || 20iter: 14.8803 sec.\n",
            "iteration 10940 || Loss: 0.2302 || 20iter: 14.8790 sec.\n",
            "iteration 10960 || Loss: 0.2536 || 20iter: 14.8469 sec.\n",
            "iteration 10980 || Loss: 0.4681 || 20iter: 14.8305 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 30 || Epoch_TRAIN_Loss: 0.2503 || Epoch_VAL_Loss: 0.2911\n",
            "timer: 357.6071 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZJWV8b_-i4q"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqPTK67w6J5J"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}