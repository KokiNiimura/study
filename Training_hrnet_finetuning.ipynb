{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_hrnet_finetuning.ipynb",
      "provenance": [],
      "mount_file_id": "12Qs5SUEGgHcNt1nEj9seZG34baqSmcOX",
      "authorship_tag": "ABX9TyNxezvBMtd4miNjIEalREp1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KokiNiimura/study/blob/master/Training_hrnet_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Dtu9s5Lkxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cafa9b9-9ad3-411f-bae5-975a40f6d111"
      },
      "source": [
        "%cd /content/drive/My Drive/study/PyTorch_Advanced/03"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/study/PyTorch_Advanced/03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXKa0gG4MioW"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "import functools\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch._utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFCgKVvvdssn"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_nZJeoZd0rq"
      },
      "source": [
        "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
        "\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath=rootpath)\n",
        "\n",
        "color_mean = (0.485, 0.456, 0.406)\n",
        "color_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", \n",
        "                           transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", \n",
        "                           transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqhUuSBXzWUu"
      },
      "source": [
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "\n",
        "BN_MOMENTUM = 0.01\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out + residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn3 = BatchNorm2d(planes * self.expansion,\n",
        "                               momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out + residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class HighResolutionModule(nn.Module):\n",
        "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
        "                 num_channels, fuse_method, multi_scale_output=True):\n",
        "        super(HighResolutionModule, self).__init__()\n",
        "        self._check_branches(\n",
        "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
        "\n",
        "        self.num_inchannels = num_inchannels\n",
        "        self.fuse_method = fuse_method\n",
        "        self.num_branches = num_branches\n",
        "\n",
        "        self.multi_scale_output = multi_scale_output\n",
        "\n",
        "        self.branches = self._make_branches(\n",
        "            num_branches, blocks, num_blocks, num_channels)\n",
        "        self.fuse_layers = self._make_fuse_layers()\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def _check_branches(self, num_branches, blocks, num_blocks,\n",
        "                        num_inchannels, num_channels):\n",
        "        if num_branches != len(num_blocks):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n",
        "                num_branches, len(num_blocks))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        if num_branches != len(num_channels):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n",
        "                num_branches, len(num_channels))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        if num_branches != len(num_inchannels):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n",
        "                num_branches, len(num_inchannels))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
        "                         stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or \\\n",
        "           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.num_inchannels[branch_index],\n",
        "                          num_channels[branch_index] * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(num_channels[branch_index] * block.expansion,\n",
        "                            momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.num_inchannels[branch_index],\n",
        "                            num_channels[branch_index], stride, downsample))\n",
        "        self.num_inchannels[branch_index] = \\\n",
        "            num_channels[branch_index] * block.expansion\n",
        "        for i in range(1, num_blocks[branch_index]):\n",
        "            layers.append(block(self.num_inchannels[branch_index],\n",
        "                                num_channels[branch_index]))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
        "        branches = []\n",
        "\n",
        "        for i in range(num_branches):\n",
        "            branches.append(\n",
        "                self._make_one_branch(i, block, num_blocks, num_channels))\n",
        "\n",
        "        return nn.ModuleList(branches)\n",
        "\n",
        "    def _make_fuse_layers(self):\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "\n",
        "        num_branches = self.num_branches\n",
        "        num_inchannels = self.num_inchannels\n",
        "        fuse_layers = []\n",
        "        for i in range(num_branches if self.multi_scale_output else 1):\n",
        "            fuse_layer = []\n",
        "            for j in range(num_branches):\n",
        "                if j > i:\n",
        "                    fuse_layer.append(nn.Sequential(\n",
        "                        nn.Conv2d(num_inchannels[j],\n",
        "                                  num_inchannels[i],\n",
        "                                  1,\n",
        "                                  1,\n",
        "                                  0,\n",
        "                                  bias=False),\n",
        "                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n",
        "                elif j == i:\n",
        "                    fuse_layer.append(None)\n",
        "                else:\n",
        "                    conv3x3s = []\n",
        "                    for k in range(i-j):\n",
        "                        if k == i - j - 1:\n",
        "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
        "                            conv3x3s.append(nn.Sequential(\n",
        "                                nn.Conv2d(num_inchannels[j],\n",
        "                                          num_outchannels_conv3x3,\n",
        "                                          3, 2, 1, bias=False),\n",
        "                                BatchNorm2d(num_outchannels_conv3x3, \n",
        "                                            momentum=BN_MOMENTUM)))\n",
        "                        else:\n",
        "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
        "                            conv3x3s.append(nn.Sequential(\n",
        "                                nn.Conv2d(num_inchannels[j],\n",
        "                                          num_outchannels_conv3x3,\n",
        "                                          3, 2, 1, bias=False),\n",
        "                                BatchNorm2d(num_outchannels_conv3x3,\n",
        "                                            momentum=BN_MOMENTUM),\n",
        "                                nn.ReLU(inplace=False)))\n",
        "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
        "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
        "\n",
        "        return nn.ModuleList(fuse_layers)\n",
        "\n",
        "    def get_num_inchannels(self):\n",
        "        return self.num_inchannels\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.num_branches == 1:\n",
        "            return [self.branches[0](x[0])]\n",
        "\n",
        "        for i in range(self.num_branches):\n",
        "            x[i] = self.branches[i](x[i])\n",
        "\n",
        "        x_fuse = []\n",
        "        for i in range(len(self.fuse_layers)):\n",
        "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
        "            for j in range(1, self.num_branches):\n",
        "                if i == j:\n",
        "                    y = y + x[j]\n",
        "                elif j > i:\n",
        "                    width_output = x[i].shape[-1]\n",
        "                    height_output = x[i].shape[-2]\n",
        "                    y = y + F.interpolate(\n",
        "                        self.fuse_layers[i][j](x[j]),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear')\n",
        "                else:\n",
        "                    y = y + self.fuse_layers[i][j](x[j])\n",
        "            x_fuse.append(self.relu(y))\n",
        "\n",
        "        return x_fuse\n",
        "\n",
        "\n",
        "blocks_dict = {\n",
        "    'BASIC': BasicBlock,\n",
        "    'BOTTLENECK': Bottleneck\n",
        "}\n",
        "\n",
        "\n",
        "class HighResolutionNet(nn.Module):\n",
        "\n",
        "    def __init__(self, config, **kwargs):\n",
        "        #extra = config.MODEL.EXTRA\n",
        "        extra = config['MODEL']['EXTRA']\n",
        "        super(HighResolutionNet, self).__init__()\n",
        "\n",
        "        # stem net\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "        self.stage1_cfg = extra['STAGE1']\n",
        "        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n",
        "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
        "        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n",
        "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
        "        stage1_out_channel = block.expansion*num_channels\n",
        "\n",
        "        self.stage2_cfg = extra['STAGE2']\n",
        "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition1 = self._make_transition_layer(\n",
        "            [stage1_out_channel], num_channels)\n",
        "        self.stage2, pre_stage_channels = self._make_stage(\n",
        "            self.stage2_cfg, num_channels)\n",
        "\n",
        "        self.stage3_cfg = extra['STAGE3']\n",
        "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition2 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage3, pre_stage_channels = self._make_stage(\n",
        "            self.stage3_cfg, num_channels)\n",
        "\n",
        "        self.stage4_cfg = extra['STAGE4']\n",
        "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition3 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage4, pre_stage_channels = self._make_stage(\n",
        "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
        "        \n",
        "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
        "\n",
        "        self.last_layer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=last_inp_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0),\n",
        "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=config['NUM_CLASSES'],\n",
        "                kernel_size=extra['FINAL_CONV_KERNEL'],\n",
        "                stride=1,\n",
        "                padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0)\n",
        "        )\n",
        "\n",
        "    def _make_transition_layer(\n",
        "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
        "        num_branches_cur = len(num_channels_cur_layer)\n",
        "        num_branches_pre = len(num_channels_pre_layer)\n",
        "\n",
        "        transition_layers = []\n",
        "        for i in range(num_branches_cur):\n",
        "            if i < num_branches_pre:\n",
        "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
        "                    transition_layers.append(nn.Sequential(\n",
        "                        nn.Conv2d(num_channels_pre_layer[i],\n",
        "                                  num_channels_cur_layer[i],\n",
        "                                  3,\n",
        "                                  1,\n",
        "                                  1,\n",
        "                                  bias=False),\n",
        "                        BatchNorm2d(\n",
        "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
        "                        nn.ReLU(inplace=False)))\n",
        "                else:\n",
        "                    transition_layers.append(None)\n",
        "            else:\n",
        "                conv3x3s = []\n",
        "                for j in range(i+1-num_branches_pre):\n",
        "                    inchannels = num_channels_pre_layer[-1]\n",
        "                    outchannels = num_channels_cur_layer[i] \\\n",
        "                        if j == i-num_branches_pre else inchannels\n",
        "                    conv3x3s.append(nn.Sequential(\n",
        "                        nn.Conv2d(\n",
        "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
        "                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
        "                        nn.ReLU(inplace=False)))\n",
        "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
        "\n",
        "        return nn.ModuleList(transition_layers)\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_stage(self, layer_config, num_inchannels,\n",
        "                    multi_scale_output=True):\n",
        "        num_modules = layer_config['NUM_MODULES']\n",
        "        num_branches = layer_config['NUM_BRANCHES']\n",
        "        num_blocks = layer_config['NUM_BLOCKS']\n",
        "        num_channels = layer_config['NUM_CHANNELS']\n",
        "        block = blocks_dict[layer_config['BLOCK']]\n",
        "        fuse_method = layer_config['FUSE_METHOD']\n",
        "\n",
        "        modules = []\n",
        "        for i in range(num_modules):\n",
        "            # multi_scale_output is only used last module\n",
        "            if not multi_scale_output and i == num_modules - 1:\n",
        "                reset_multi_scale_output = False\n",
        "            else:\n",
        "                reset_multi_scale_output = True\n",
        "            modules.append(\n",
        "                HighResolutionModule(num_branches,\n",
        "                                      block,\n",
        "                                      num_blocks,\n",
        "                                      num_inchannels,\n",
        "                                      num_channels,\n",
        "                                      fuse_method,\n",
        "                                      reset_multi_scale_output)\n",
        "            )\n",
        "            num_inchannels = modules[-1].get_num_inchannels()\n",
        "\n",
        "        return nn.Sequential(*modules), num_inchannels\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        #print(x.shape)\n",
        "        x = self.layer1(x)\n",
        "        #print(x.shape)\n",
        "        x_list = []\n",
        "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
        "            if self.transition1[i] is not None:\n",
        "                x_list.append(self.transition1[i](x))\n",
        "            else:\n",
        "                x_list.append(x)\n",
        "        y_list = self.stage2(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
        "            if self.transition2[i] is not None:\n",
        "                if i < self.stage2_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition2[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition2[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        y_list = self.stage3(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
        "            if self.transition3[i] is not None:\n",
        "                if i < self.stage3_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition3[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition3[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        x = self.stage4(x_list)\n",
        "        #print(x.shape)\n",
        "        # Upsampling\n",
        "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
        "        #x0_h, x0_w = x0_h* 4, x0_w * 2 \n",
        "        #print(x0_h, x0_w)\n",
        "        x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')\n",
        "        x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')\n",
        "        x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')\n",
        "\n",
        "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
        "\n",
        "        x = self.last_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def init_weights(self, pretrained='',):\n",
        "        logger.info('=> init weights from normal distribution')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.001)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        if pretrained:\n",
        "            pretrained_dict = torch.load(pretrained)\n",
        "            logger.info('=> loading pretrained model {}'.format(pretrained))\n",
        "            model_dict = self.state_dict()\n",
        "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
        "                               if k in model_dict.keys()}\n",
        "            for k, _ in pretrained_dict.items():\n",
        "                logger.info(\n",
        "                    '=> loading {} pretrained model {}'.format(k, pretrained))\n",
        "            model_dict.update(pretrained_dict)\n",
        "            self.load_state_dict(model_dict)\n",
        "\n",
        "def get_seg_model(cfg, **kwargs):\n",
        "    model = HighResolutionNet(cfg, **kwargs)\n",
        "    model.init_weights(cfg['PRETRAINED'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1EGoRi10oMx"
      },
      "source": [
        "# dict for model configuration\n",
        "config = {}\n",
        "\n",
        "config['NUM_CLASSES'] = 21\n",
        "config['PRETRAINED'] = \"./hrnet_w18_small_model_v1.pth\"\n",
        "\n",
        "config['MODEL'] = {'EXTRA': {'FINAL_CONV_KERNEL': 1,\n",
        "      'STAGE1': {'BLOCK': 'BOTTLENECK', \n",
        "                 'FUSE_METHOD': 'SUM',\n",
        "                 'NUM_BLOCKS': [1],\n",
        "                 'NUM_CHANNELS': [32],\n",
        "                 'NUM_MODULES': 1,\n",
        "                 'NUM_RANCHES': 1\n",
        "                },\n",
        "      'STAGE2': {'BLOCK': 'BASIC',\n",
        "                 'FUSE_METHOD': 'SUM',\n",
        "                 'NUM_BLOCKS': [2, 2],\n",
        "                 'NUM_BRANCHES': 2,\n",
        "                 'NUM_CHANNELS': [16, 32],\n",
        "                 'NUM_MODULES': 1\n",
        "                },\n",
        "      'STAGE3':{'BLOCK': 'BASIC',\n",
        "                'FUSE_METHOD': 'SUM',\n",
        "                'NUM_BLOCKS': [2, 2, 2],\n",
        "                'NUM_BRANCHES': 3,\n",
        "                'NUM_CHANNELS': [16, 32, 64],\n",
        "                'NUM_MODULES': 1\n",
        "               },\n",
        "       'STAGE4': {'BLOCK': 'BASIC',\n",
        "                 'FUSE_METHOD': 'SUM',\n",
        "                 'NUM_BLOCKS': [2, 2, 2, 2],\n",
        "                 'NUM_BRANCHES': 4,\n",
        "                 'NUM_CHANNELS': [16, 32, 64, 128],\n",
        "                 'NUM_MODULES': 1\n",
        "                 }}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlMNdp2A012h"
      },
      "source": [
        "net = get_seg_model(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgDZepO_p2sW"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EXrJsEYt1ND"
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "def lambda_epoch(epoch):\n",
        "    max_epoch = 30\n",
        "    return math.pow((1-epoch/max_epoch), 0.9)\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GguQkl7qFT6"
      },
      "source": [
        "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device: {}\".format(device))\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
        "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    iteration = 1\n",
        "    logs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_val_loss = 0.0\n",
        "\n",
        "        print('--------------')\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('--------------')\n",
        "        \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                print('(train)')\n",
        "\n",
        "            else:\n",
        "                if ((epoch+1) % 5 == 0):\n",
        "                    net.eval()\n",
        "                    print('--------------')\n",
        "                    print('(val)')\n",
        "                else:\n",
        "                    continue\n",
        "        \n",
        "            count = 0\n",
        "            for images, anno_class_images in dataloaders_dict[phase]:\n",
        "                if images.size()[0] == 1:\n",
        "                    continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                anno_class_images = anno_class_images.to(device)\n",
        "\n",
        "                if (phase == 'train'):\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(images)\n",
        "                    loss = criterion(outputs, anno_class_images.long())\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "\n",
        "                        if (iteration % 20 == 0):\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('iteration {} || Loss: {:.4f} || 20iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item()/batch_size, duration))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                        epoch_train_loss += loss.item()\n",
        "                        iteration += 1\n",
        "\n",
        "                    else:\n",
        "                        epoch_val_loss += loss.item()\n",
        "\n",
        "        t_epoch_finish = time.time()\n",
        "        print('--------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss: {:.4f} || Epoch_VAL_Loss: {:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
        "        print('timer: {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss/num_train_imgs, \n",
        "                        'val_loss': epoch_val_loss/num_val_imgs}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(\"log_output_hr_3.csv\")\n",
        "\n",
        "        torch.save(net.state_dict(), 'weights/hrnet_3_' + str(epoch+1) + '.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRaChXgJrefI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31336400-3fb9-4286-cddb-780f832aaea6"
      },
      "source": [
        "num_epochs = 30\n",
        "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda:0\n",
            "--------------\n",
            "Epoch 1/30\n",
            "--------------\n",
            "(train)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 20 || Loss: 0.5276 || 20iter: 21.8789 sec.\n",
            "iteration 40 || Loss: 0.3685 || 20iter: 22.2980 sec.\n",
            "iteration 60 || Loss: 0.2986 || 20iter: 22.7547 sec.\n",
            "iteration 80 || Loss: 0.1504 || 20iter: 22.1943 sec.\n",
            "iteration 100 || Loss: 0.2569 || 20iter: 22.0652 sec.\n",
            "iteration 120 || Loss: 0.1025 || 20iter: 22.3312 sec.\n",
            "iteration 140 || Loss: 0.3575 || 20iter: 57.5173 sec.\n",
            "iteration 160 || Loss: 0.4694 || 20iter: 88.0588 sec.\n",
            "iteration 180 || Loss: 0.2675 || 20iter: 80.8331 sec.\n",
            "iteration 200 || Loss: 0.4086 || 20iter: 87.8432 sec.\n",
            "iteration 220 || Loss: 0.2087 || 20iter: 82.2719 sec.\n",
            "iteration 240 || Loss: 0.1779 || 20iter: 82.2785 sec.\n",
            "iteration 260 || Loss: 0.2661 || 20iter: 84.7728 sec.\n",
            "iteration 280 || Loss: 0.2625 || 20iter: 79.5939 sec.\n",
            "iteration 300 || Loss: 0.3593 || 20iter: 85.4984 sec.\n",
            "iteration 320 || Loss: 0.1902 || 20iter: 98.4943 sec.\n",
            "iteration 340 || Loss: 0.3671 || 20iter: 81.5157 sec.\n",
            "iteration 360 || Loss: 0.4532 || 20iter: 86.1657 sec.\n",
            "--------------\n",
            "epoch 1 || Epoch_TRAIN_Loss: 0.3150 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 1168.1088 sec.\n",
            "--------------\n",
            "Epoch 2/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 380 || Loss: 0.2269 || 20iter: 14.7750 sec.\n",
            "iteration 400 || Loss: 0.1544 || 20iter: 21.8080 sec.\n",
            "iteration 420 || Loss: 0.2177 || 20iter: 22.3450 sec.\n",
            "iteration 440 || Loss: 0.2363 || 20iter: 22.5468 sec.\n",
            "iteration 460 || Loss: 0.1253 || 20iter: 22.1197 sec.\n",
            "iteration 480 || Loss: 0.1264 || 20iter: 22.1566 sec.\n",
            "iteration 500 || Loss: 0.1718 || 20iter: 22.4209 sec.\n",
            "iteration 520 || Loss: 0.1096 || 20iter: 22.2900 sec.\n",
            "iteration 540 || Loss: 0.5001 || 20iter: 22.2546 sec.\n",
            "iteration 560 || Loss: 0.1079 || 20iter: 22.2754 sec.\n",
            "iteration 580 || Loss: 0.1948 || 20iter: 22.2486 sec.\n",
            "iteration 600 || Loss: 0.1733 || 20iter: 22.2332 sec.\n",
            "iteration 620 || Loss: 0.2081 || 20iter: 22.2713 sec.\n",
            "iteration 640 || Loss: 0.2282 || 20iter: 22.2918 sec.\n",
            "iteration 660 || Loss: 0.4807 || 20iter: 22.2732 sec.\n",
            "iteration 680 || Loss: 0.2355 || 20iter: 22.2875 sec.\n",
            "iteration 700 || Loss: 0.1485 || 20iter: 22.2193 sec.\n",
            "iteration 720 || Loss: 0.2225 || 20iter: 22.2294 sec.\n",
            "--------------\n",
            "epoch 2 || Epoch_TRAIN_Loss: 0.2439 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 422.6771 sec.\n",
            "--------------\n",
            "Epoch 3/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 740 || Loss: 0.1615 || 20iter: 8.3987 sec.\n",
            "iteration 760 || Loss: 0.1384 || 20iter: 22.3269 sec.\n",
            "iteration 780 || Loss: 0.1441 || 20iter: 22.3468 sec.\n",
            "iteration 800 || Loss: 0.2340 || 20iter: 22.2905 sec.\n",
            "iteration 820 || Loss: 0.1741 || 20iter: 22.2232 sec.\n",
            "iteration 840 || Loss: 0.1024 || 20iter: 22.2132 sec.\n",
            "iteration 860 || Loss: 0.6587 || 20iter: 22.2600 sec.\n",
            "iteration 880 || Loss: 0.1340 || 20iter: 22.2489 sec.\n",
            "iteration 900 || Loss: 0.3314 || 20iter: 22.2710 sec.\n",
            "iteration 920 || Loss: 0.1651 || 20iter: 22.2844 sec.\n",
            "iteration 940 || Loss: 0.1809 || 20iter: 22.2060 sec.\n",
            "iteration 960 || Loss: 0.1741 || 20iter: 22.3193 sec.\n",
            "iteration 980 || Loss: 0.4050 || 20iter: 22.3266 sec.\n",
            "iteration 1000 || Loss: 0.1315 || 20iter: 22.2465 sec.\n",
            "iteration 1020 || Loss: 0.1762 || 20iter: 22.2948 sec.\n",
            "iteration 1040 || Loss: 0.3997 || 20iter: 22.3464 sec.\n",
            "iteration 1060 || Loss: 0.1578 || 20iter: 22.2959 sec.\n",
            "iteration 1080 || Loss: 0.1164 || 20iter: 22.2528 sec.\n",
            "--------------\n",
            "epoch 3 || Epoch_TRAIN_Loss: 0.2372 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.6936 sec.\n",
            "--------------\n",
            "Epoch 4/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 1100 || Loss: 0.3268 || 20iter: 1.4780 sec.\n",
            "iteration 1120 || Loss: 0.1674 || 20iter: 22.2287 sec.\n",
            "iteration 1140 || Loss: 0.1042 || 20iter: 22.1581 sec.\n",
            "iteration 1160 || Loss: 0.1953 || 20iter: 22.2830 sec.\n",
            "iteration 1180 || Loss: 0.2063 || 20iter: 22.2840 sec.\n",
            "iteration 1200 || Loss: 0.3842 || 20iter: 22.1791 sec.\n",
            "iteration 1220 || Loss: 0.3454 || 20iter: 22.2663 sec.\n",
            "iteration 1240 || Loss: 0.0563 || 20iter: 22.2955 sec.\n",
            "iteration 1260 || Loss: 0.1249 || 20iter: 22.2660 sec.\n",
            "iteration 1280 || Loss: 0.1479 || 20iter: 22.1588 sec.\n",
            "iteration 1300 || Loss: 0.3076 || 20iter: 22.2928 sec.\n",
            "iteration 1320 || Loss: 0.2555 || 20iter: 22.3420 sec.\n",
            "iteration 1340 || Loss: 0.1349 || 20iter: 22.2457 sec.\n",
            "iteration 1360 || Loss: 0.2692 || 20iter: 22.2914 sec.\n",
            "iteration 1380 || Loss: 0.1506 || 20iter: 22.3142 sec.\n",
            "iteration 1400 || Loss: 0.1938 || 20iter: 22.3075 sec.\n",
            "iteration 1420 || Loss: 0.1445 || 20iter: 22.3671 sec.\n",
            "iteration 1440 || Loss: 0.0571 || 20iter: 22.2568 sec.\n",
            "iteration 1460 || Loss: 0.2880 || 20iter: 22.2938 sec.\n",
            "--------------\n",
            "epoch 4 || Epoch_TRAIN_Loss: 0.2255 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.5224 sec.\n",
            "--------------\n",
            "Epoch 5/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 1480 || Loss: 0.2456 || 20iter: 17.6055 sec.\n",
            "iteration 1500 || Loss: 0.2661 || 20iter: 22.3041 sec.\n",
            "iteration 1520 || Loss: 0.4310 || 20iter: 22.3332 sec.\n",
            "iteration 1540 || Loss: 0.1289 || 20iter: 22.3156 sec.\n",
            "iteration 1560 || Loss: 0.1261 || 20iter: 22.2479 sec.\n",
            "iteration 1580 || Loss: 0.3388 || 20iter: 22.2263 sec.\n",
            "iteration 1600 || Loss: 0.1673 || 20iter: 22.2642 sec.\n",
            "iteration 1620 || Loss: 0.1639 || 20iter: 22.3013 sec.\n",
            "iteration 1640 || Loss: 0.3454 || 20iter: 22.2784 sec.\n",
            "iteration 1660 || Loss: 0.2288 || 20iter: 22.2977 sec.\n",
            "iteration 1680 || Loss: 0.2214 || 20iter: 22.2128 sec.\n",
            "iteration 1700 || Loss: 0.1238 || 20iter: 22.2406 sec.\n",
            "iteration 1720 || Loss: 0.4399 || 20iter: 22.2420 sec.\n",
            "iteration 1740 || Loss: 0.1435 || 20iter: 22.2557 sec.\n",
            "iteration 1760 || Loss: 0.1743 || 20iter: 22.2520 sec.\n",
            "iteration 1780 || Loss: 0.1181 || 20iter: 22.1783 sec.\n",
            "iteration 1800 || Loss: 0.0686 || 20iter: 22.2825 sec.\n",
            "iteration 1820 || Loss: 0.2032 || 20iter: 22.2842 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 5 || Epoch_TRAIN_Loss: 0.2186 || Epoch_VAL_Loss: 0.2329\n",
            "timer: 1646.2382 sec.\n",
            "--------------\n",
            "Epoch 6/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 1840 || Loss: 0.0986 || 20iter: 10.3034 sec.\n",
            "iteration 1860 || Loss: 0.1824 || 20iter: 21.4799 sec.\n",
            "iteration 1880 || Loss: 0.1946 || 20iter: 21.6945 sec.\n",
            "iteration 1900 || Loss: 0.2726 || 20iter: 21.8332 sec.\n",
            "iteration 1920 || Loss: 0.1808 || 20iter: 22.0625 sec.\n",
            "iteration 1940 || Loss: 0.2963 || 20iter: 22.4469 sec.\n",
            "iteration 1960 || Loss: 0.3781 || 20iter: 22.2957 sec.\n",
            "iteration 1980 || Loss: 0.2910 || 20iter: 22.0776 sec.\n",
            "iteration 2000 || Loss: 0.1650 || 20iter: 22.2525 sec.\n",
            "iteration 2020 || Loss: 0.1641 || 20iter: 22.3394 sec.\n",
            "iteration 2040 || Loss: 0.2530 || 20iter: 22.2675 sec.\n",
            "iteration 2060 || Loss: 0.2634 || 20iter: 22.2213 sec.\n",
            "iteration 2080 || Loss: 0.0866 || 20iter: 22.2314 sec.\n",
            "iteration 2100 || Loss: 0.1915 || 20iter: 22.3307 sec.\n",
            "iteration 2120 || Loss: 0.3011 || 20iter: 22.4132 sec.\n",
            "iteration 2140 || Loss: 0.1425 || 20iter: 22.2570 sec.\n",
            "iteration 2160 || Loss: 0.1774 || 20iter: 22.2791 sec.\n",
            "iteration 2180 || Loss: 0.2352 || 20iter: 22.2571 sec.\n",
            "--------------\n",
            "epoch 6 || Epoch_TRAIN_Loss: 0.2148 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 421.2087 sec.\n",
            "--------------\n",
            "Epoch 7/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 2200 || Loss: 0.1818 || 20iter: 3.7754 sec.\n",
            "iteration 2220 || Loss: 0.0986 || 20iter: 22.2674 sec.\n",
            "iteration 2240 || Loss: 0.1169 || 20iter: 22.2625 sec.\n",
            "iteration 2260 || Loss: 0.3157 || 20iter: 22.2907 sec.\n",
            "iteration 2280 || Loss: 0.1825 || 20iter: 22.3303 sec.\n",
            "iteration 2300 || Loss: 0.1057 || 20iter: 22.3881 sec.\n",
            "iteration 2320 || Loss: 0.3863 || 20iter: 22.2631 sec.\n",
            "iteration 2340 || Loss: 0.3239 || 20iter: 22.2285 sec.\n",
            "iteration 2360 || Loss: 0.1315 || 20iter: 22.2900 sec.\n",
            "iteration 2380 || Loss: 0.2404 || 20iter: 22.2981 sec.\n",
            "iteration 2400 || Loss: 0.3615 || 20iter: 22.3790 sec.\n",
            "iteration 2420 || Loss: 0.3970 || 20iter: 22.3542 sec.\n",
            "iteration 2440 || Loss: 0.1704 || 20iter: 22.3236 sec.\n",
            "iteration 2460 || Loss: 0.1489 || 20iter: 22.2962 sec.\n",
            "iteration 2480 || Loss: 0.2120 || 20iter: 22.2622 sec.\n",
            "iteration 2500 || Loss: 0.2437 || 20iter: 22.2149 sec.\n",
            "iteration 2520 || Loss: 0.1607 || 20iter: 22.2592 sec.\n",
            "iteration 2540 || Loss: 0.3290 || 20iter: 22.3085 sec.\n",
            "iteration 2560 || Loss: 0.2908 || 20iter: 22.2970 sec.\n",
            "--------------\n",
            "epoch 7 || Epoch_TRAIN_Loss: 0.2139 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 424.0024 sec.\n",
            "--------------\n",
            "Epoch 8/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 2580 || Loss: 0.1962 || 20iter: 19.9332 sec.\n",
            "iteration 2600 || Loss: 0.2369 || 20iter: 22.1931 sec.\n",
            "iteration 2620 || Loss: 0.1692 || 20iter: 22.2591 sec.\n",
            "iteration 2640 || Loss: 0.2493 || 20iter: 22.2760 sec.\n",
            "iteration 2660 || Loss: 0.4254 || 20iter: 22.2763 sec.\n",
            "iteration 2680 || Loss: 0.1617 || 20iter: 22.2805 sec.\n",
            "iteration 2700 || Loss: 0.5044 || 20iter: 22.2759 sec.\n",
            "iteration 2720 || Loss: 0.1492 || 20iter: 22.2671 sec.\n",
            "iteration 2740 || Loss: 0.2460 || 20iter: 22.2715 sec.\n",
            "iteration 2760 || Loss: 0.3526 || 20iter: 22.2446 sec.\n",
            "iteration 2780 || Loss: 0.1684 || 20iter: 22.2154 sec.\n",
            "iteration 2800 || Loss: 0.2447 || 20iter: 22.2461 sec.\n",
            "iteration 2820 || Loss: 0.1821 || 20iter: 22.2491 sec.\n",
            "iteration 2840 || Loss: 0.1545 || 20iter: 22.3610 sec.\n",
            "iteration 2860 || Loss: 0.1990 || 20iter: 22.2863 sec.\n",
            "iteration 2880 || Loss: 0.2076 || 20iter: 22.3153 sec.\n",
            "iteration 2900 || Loss: 0.1980 || 20iter: 22.3167 sec.\n",
            "iteration 2920 || Loss: 0.1614 || 20iter: 22.3719 sec.\n",
            "--------------\n",
            "epoch 8 || Epoch_TRAIN_Loss: 0.2080 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.6178 sec.\n",
            "--------------\n",
            "Epoch 9/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 2940 || Loss: 0.1074 || 20iter: 13.0539 sec.\n",
            "iteration 2960 || Loss: 0.2030 || 20iter: 22.1835 sec.\n",
            "iteration 2980 || Loss: 0.0772 || 20iter: 22.2543 sec.\n",
            "iteration 3000 || Loss: 0.1753 || 20iter: 22.2557 sec.\n",
            "iteration 3020 || Loss: 0.1748 || 20iter: 22.3818 sec.\n",
            "iteration 3040 || Loss: 0.1331 || 20iter: 22.2663 sec.\n",
            "iteration 3060 || Loss: 0.3825 || 20iter: 22.1870 sec.\n",
            "iteration 3080 || Loss: 0.2757 || 20iter: 22.2812 sec.\n",
            "iteration 3100 || Loss: 0.1969 || 20iter: 22.3577 sec.\n",
            "iteration 3120 || Loss: 0.2335 || 20iter: 22.2609 sec.\n",
            "iteration 3140 || Loss: 0.1008 || 20iter: 22.1618 sec.\n",
            "iteration 3160 || Loss: 0.3450 || 20iter: 22.2545 sec.\n",
            "iteration 3180 || Loss: 0.2246 || 20iter: 22.2094 sec.\n",
            "iteration 3200 || Loss: 0.3735 || 20iter: 22.3176 sec.\n",
            "iteration 3220 || Loss: 0.1839 || 20iter: 22.2014 sec.\n",
            "iteration 3240 || Loss: 0.1292 || 20iter: 22.1902 sec.\n",
            "iteration 3260 || Loss: 0.2034 || 20iter: 22.3316 sec.\n",
            "iteration 3280 || Loss: 0.1041 || 20iter: 22.2573 sec.\n",
            "--------------\n",
            "epoch 9 || Epoch_TRAIN_Loss: 0.2024 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.2815 sec.\n",
            "--------------\n",
            "Epoch 10/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 3300 || Loss: 0.4226 || 20iter: 6.0377 sec.\n",
            "iteration 3320 || Loss: 0.1008 || 20iter: 22.2640 sec.\n",
            "iteration 3340 || Loss: 0.1939 || 20iter: 22.2184 sec.\n",
            "iteration 3360 || Loss: 0.1508 || 20iter: 22.2344 sec.\n",
            "iteration 3380 || Loss: 0.1890 || 20iter: 22.2800 sec.\n",
            "iteration 3400 || Loss: 0.2036 || 20iter: 22.2018 sec.\n",
            "iteration 3420 || Loss: 0.3875 || 20iter: 22.2584 sec.\n",
            "iteration 3440 || Loss: 0.1859 || 20iter: 22.2305 sec.\n",
            "iteration 3460 || Loss: 0.2836 || 20iter: 22.2627 sec.\n",
            "iteration 3480 || Loss: 0.2259 || 20iter: 22.2485 sec.\n",
            "iteration 3500 || Loss: 0.2654 || 20iter: 22.2884 sec.\n",
            "iteration 3520 || Loss: 0.1921 || 20iter: 22.1589 sec.\n",
            "iteration 3540 || Loss: 0.3068 || 20iter: 22.1089 sec.\n",
            "iteration 3560 || Loss: 0.1605 || 20iter: 22.1673 sec.\n",
            "iteration 3580 || Loss: 0.1947 || 20iter: 22.2698 sec.\n",
            "iteration 3600 || Loss: 0.0820 || 20iter: 22.3002 sec.\n",
            "iteration 3620 || Loss: 0.1769 || 20iter: 22.2481 sec.\n",
            "iteration 3640 || Loss: 0.1723 || 20iter: 22.2121 sec.\n",
            "iteration 3660 || Loss: 0.2925 || 20iter: 22.2819 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 10 || Epoch_TRAIN_Loss: 0.1971 || Epoch_VAL_Loss: 0.2170\n",
            "timer: 541.9542 sec.\n",
            "--------------\n",
            "Epoch 11/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 3680 || Loss: 0.1917 || 20iter: 22.1929 sec.\n",
            "iteration 3700 || Loss: 0.0925 || 20iter: 22.3139 sec.\n",
            "iteration 3720 || Loss: 0.2617 || 20iter: 22.2518 sec.\n",
            "iteration 3740 || Loss: 0.1349 || 20iter: 22.1544 sec.\n",
            "iteration 3760 || Loss: 0.3206 || 20iter: 22.2463 sec.\n",
            "iteration 3780 || Loss: 0.4065 || 20iter: 22.2787 sec.\n",
            "iteration 3800 || Loss: 0.3887 || 20iter: 22.2189 sec.\n",
            "iteration 3820 || Loss: 0.2611 || 20iter: 22.1724 sec.\n",
            "iteration 3840 || Loss: 0.1779 || 20iter: 22.2613 sec.\n",
            "iteration 3860 || Loss: 0.2074 || 20iter: 22.3228 sec.\n",
            "iteration 3880 || Loss: 0.3418 || 20iter: 22.1995 sec.\n",
            "iteration 3900 || Loss: 0.2195 || 20iter: 22.3968 sec.\n",
            "iteration 3920 || Loss: 0.2442 || 20iter: 22.4068 sec.\n",
            "iteration 3940 || Loss: 0.1701 || 20iter: 22.1917 sec.\n",
            "iteration 3960 || Loss: 0.2438 || 20iter: 22.2415 sec.\n",
            "iteration 3980 || Loss: 0.1099 || 20iter: 22.2248 sec.\n",
            "iteration 4000 || Loss: 0.0903 || 20iter: 22.2344 sec.\n",
            "iteration 4020 || Loss: 0.3206 || 20iter: 22.2015 sec.\n",
            "--------------\n",
            "epoch 11 || Epoch_TRAIN_Loss: 0.1953 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.1951 sec.\n",
            "--------------\n",
            "Epoch 12/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 4040 || Loss: 0.0771 || 20iter: 15.3678 sec.\n",
            "iteration 4060 || Loss: 0.1597 || 20iter: 22.3235 sec.\n",
            "iteration 4080 || Loss: 0.1042 || 20iter: 22.2159 sec.\n",
            "iteration 4100 || Loss: 0.1623 || 20iter: 22.2586 sec.\n",
            "iteration 4120 || Loss: 0.1703 || 20iter: 22.2565 sec.\n",
            "iteration 4140 || Loss: 0.1604 || 20iter: 22.2932 sec.\n",
            "iteration 4160 || Loss: 0.1397 || 20iter: 22.3366 sec.\n",
            "iteration 4180 || Loss: 0.1979 || 20iter: 22.2766 sec.\n",
            "iteration 4200 || Loss: 0.1049 || 20iter: 22.2941 sec.\n",
            "iteration 4220 || Loss: 0.2364 || 20iter: 22.2852 sec.\n",
            "iteration 4240 || Loss: 0.3489 || 20iter: 22.2219 sec.\n",
            "iteration 4260 || Loss: 0.1447 || 20iter: 22.2531 sec.\n",
            "iteration 4280 || Loss: 0.1360 || 20iter: 22.2002 sec.\n",
            "iteration 4300 || Loss: 0.2300 || 20iter: 22.2493 sec.\n",
            "iteration 4320 || Loss: 0.1054 || 20iter: 22.3007 sec.\n",
            "iteration 4340 || Loss: 0.1990 || 20iter: 22.2862 sec.\n",
            "iteration 4360 || Loss: 0.1674 || 20iter: 22.2770 sec.\n",
            "iteration 4380 || Loss: 0.2649 || 20iter: 22.2921 sec.\n",
            "--------------\n",
            "epoch 12 || Epoch_TRAIN_Loss: 0.1916 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.5682 sec.\n",
            "--------------\n",
            "Epoch 13/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 4400 || Loss: 0.0720 || 20iter: 8.3802 sec.\n",
            "iteration 4420 || Loss: 0.3549 || 20iter: 22.2865 sec.\n",
            "iteration 4440 || Loss: 0.1467 || 20iter: 22.3315 sec.\n",
            "iteration 4460 || Loss: 0.3033 || 20iter: 22.3791 sec.\n",
            "iteration 4480 || Loss: 0.2834 || 20iter: 22.3099 sec.\n",
            "iteration 4500 || Loss: 0.0740 || 20iter: 22.2802 sec.\n",
            "iteration 4520 || Loss: 0.1481 || 20iter: 22.2540 sec.\n",
            "iteration 4540 || Loss: 0.1224 || 20iter: 22.2758 sec.\n",
            "iteration 4560 || Loss: 0.1635 || 20iter: 22.2404 sec.\n",
            "iteration 4580 || Loss: 0.1634 || 20iter: 22.2618 sec.\n",
            "iteration 4600 || Loss: 0.1461 || 20iter: 22.3113 sec.\n",
            "iteration 4620 || Loss: 0.1397 || 20iter: 22.2903 sec.\n",
            "iteration 4640 || Loss: 0.2389 || 20iter: 22.2880 sec.\n",
            "iteration 4660 || Loss: 0.1731 || 20iter: 22.2136 sec.\n",
            "iteration 4680 || Loss: 0.1221 || 20iter: 22.2827 sec.\n",
            "iteration 4700 || Loss: 0.1995 || 20iter: 22.2610 sec.\n",
            "iteration 4720 || Loss: 0.3479 || 20iter: 22.2937 sec.\n",
            "iteration 4740 || Loss: 0.1679 || 20iter: 22.3138 sec.\n",
            "--------------\n",
            "epoch 13 || Epoch_TRAIN_Loss: 0.1930 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.7376 sec.\n",
            "--------------\n",
            "Epoch 14/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 4760 || Loss: 0.1290 || 20iter: 1.4564 sec.\n",
            "iteration 4780 || Loss: 0.1674 || 20iter: 22.1765 sec.\n",
            "iteration 4800 || Loss: 0.2131 || 20iter: 22.2019 sec.\n",
            "iteration 4820 || Loss: 0.1026 || 20iter: 22.2504 sec.\n",
            "iteration 4840 || Loss: 0.1962 || 20iter: 22.2320 sec.\n",
            "iteration 4860 || Loss: 0.2545 || 20iter: 22.2033 sec.\n",
            "iteration 4880 || Loss: 0.1075 || 20iter: 22.2301 sec.\n",
            "iteration 4900 || Loss: 0.2205 || 20iter: 22.2498 sec.\n",
            "iteration 4920 || Loss: 0.1841 || 20iter: 22.2429 sec.\n",
            "iteration 4940 || Loss: 0.1646 || 20iter: 22.1671 sec.\n",
            "iteration 4960 || Loss: 0.0736 || 20iter: 22.3093 sec.\n",
            "iteration 4980 || Loss: 0.1316 || 20iter: 22.2636 sec.\n",
            "iteration 5000 || Loss: 0.1417 || 20iter: 22.2325 sec.\n",
            "iteration 5020 || Loss: 0.2834 || 20iter: 22.2805 sec.\n",
            "iteration 5040 || Loss: 0.1323 || 20iter: 22.2233 sec.\n",
            "iteration 5060 || Loss: 0.0946 || 20iter: 22.2601 sec.\n",
            "iteration 5080 || Loss: 0.1216 || 20iter: 22.2596 sec.\n",
            "iteration 5100 || Loss: 0.2286 || 20iter: 22.2310 sec.\n",
            "iteration 5120 || Loss: 0.1486 || 20iter: 22.2346 sec.\n",
            "--------------\n",
            "epoch 14 || Epoch_TRAIN_Loss: 0.1909 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 422.8614 sec.\n",
            "--------------\n",
            "Epoch 15/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 5140 || Loss: 0.2491 || 20iter: 17.6120 sec.\n",
            "iteration 5160 || Loss: 0.2432 || 20iter: 22.2639 sec.\n",
            "iteration 5180 || Loss: 0.1355 || 20iter: 22.2799 sec.\n",
            "iteration 5200 || Loss: 0.1142 || 20iter: 22.2753 sec.\n",
            "iteration 5220 || Loss: 0.1413 || 20iter: 22.2310 sec.\n",
            "iteration 5240 || Loss: 0.1209 || 20iter: 22.3406 sec.\n",
            "iteration 5260 || Loss: 0.2039 || 20iter: 22.2270 sec.\n",
            "iteration 5280 || Loss: 0.2947 || 20iter: 22.2231 sec.\n",
            "iteration 5300 || Loss: 0.1901 || 20iter: 22.2519 sec.\n",
            "iteration 5320 || Loss: 0.1355 || 20iter: 22.2394 sec.\n",
            "iteration 5340 || Loss: 0.1524 || 20iter: 22.2676 sec.\n",
            "iteration 5360 || Loss: 0.2041 || 20iter: 22.3133 sec.\n",
            "iteration 5380 || Loss: 0.0881 || 20iter: 22.2372 sec.\n",
            "iteration 5400 || Loss: 0.2482 || 20iter: 22.2758 sec.\n",
            "iteration 5420 || Loss: 0.2456 || 20iter: 22.2720 sec.\n",
            "iteration 5440 || Loss: 0.2054 || 20iter: 22.2752 sec.\n",
            "iteration 5460 || Loss: 0.1485 || 20iter: 22.1945 sec.\n",
            "iteration 5480 || Loss: 0.1171 || 20iter: 22.2769 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 15 || Epoch_TRAIN_Loss: 0.1880 || Epoch_VAL_Loss: 0.2105\n",
            "timer: 542.1478 sec.\n",
            "--------------\n",
            "Epoch 16/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 5500 || Loss: 0.0891 || 20iter: 10.7152 sec.\n",
            "iteration 5520 || Loss: 0.3974 || 20iter: 22.2123 sec.\n",
            "iteration 5540 || Loss: 0.1679 || 20iter: 22.2445 sec.\n",
            "iteration 5560 || Loss: 0.1182 || 20iter: 22.1740 sec.\n",
            "iteration 5580 || Loss: 0.1609 || 20iter: 22.1940 sec.\n",
            "iteration 5600 || Loss: 0.1580 || 20iter: 22.2446 sec.\n",
            "iteration 5620 || Loss: 0.2939 || 20iter: 22.2297 sec.\n",
            "iteration 5640 || Loss: 0.1481 || 20iter: 22.2268 sec.\n",
            "iteration 5660 || Loss: 0.0949 || 20iter: 22.2440 sec.\n",
            "iteration 5680 || Loss: 0.1654 || 20iter: 22.2558 sec.\n",
            "iteration 5700 || Loss: 0.2752 || 20iter: 22.2937 sec.\n",
            "iteration 5720 || Loss: 0.2030 || 20iter: 22.2836 sec.\n",
            "iteration 5740 || Loss: 0.1499 || 20iter: 22.2629 sec.\n",
            "iteration 5760 || Loss: 0.1094 || 20iter: 22.1569 sec.\n",
            "iteration 5780 || Loss: 0.2640 || 20iter: 22.2128 sec.\n",
            "iteration 5800 || Loss: 0.2406 || 20iter: 22.2487 sec.\n",
            "iteration 5820 || Loss: 0.0764 || 20iter: 22.2933 sec.\n",
            "iteration 5840 || Loss: 0.2727 || 20iter: 22.1462 sec.\n",
            "--------------\n",
            "epoch 16 || Epoch_TRAIN_Loss: 0.1859 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 422.8256 sec.\n",
            "--------------\n",
            "Epoch 17/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 5860 || Loss: 0.1570 || 20iter: 3.7617 sec.\n",
            "iteration 5880 || Loss: 0.1128 || 20iter: 22.2733 sec.\n",
            "iteration 5900 || Loss: 0.1519 || 20iter: 22.3453 sec.\n",
            "iteration 5920 || Loss: 0.2639 || 20iter: 22.2430 sec.\n",
            "iteration 5940 || Loss: 0.4977 || 20iter: 22.2168 sec.\n",
            "iteration 5960 || Loss: 0.2698 || 20iter: 22.2906 sec.\n",
            "iteration 5980 || Loss: 0.2758 || 20iter: 22.3197 sec.\n",
            "iteration 6000 || Loss: 0.1992 || 20iter: 22.3041 sec.\n",
            "iteration 6020 || Loss: 0.1030 || 20iter: 22.2950 sec.\n",
            "iteration 6040 || Loss: 0.1830 || 20iter: 22.3958 sec.\n",
            "iteration 6060 || Loss: 0.0805 || 20iter: 22.2318 sec.\n",
            "iteration 6080 || Loss: 0.1416 || 20iter: 22.3051 sec.\n",
            "iteration 6100 || Loss: 0.2135 || 20iter: 22.2544 sec.\n",
            "iteration 6120 || Loss: 0.2110 || 20iter: 22.2728 sec.\n",
            "iteration 6140 || Loss: 0.2050 || 20iter: 22.3005 sec.\n",
            "iteration 6160 || Loss: 0.2164 || 20iter: 22.2577 sec.\n",
            "iteration 6180 || Loss: 0.0947 || 20iter: 22.2185 sec.\n",
            "iteration 6200 || Loss: 0.1620 || 20iter: 22.3078 sec.\n",
            "iteration 6220 || Loss: 0.1336 || 20iter: 22.3405 sec.\n",
            "--------------\n",
            "epoch 17 || Epoch_TRAIN_Loss: 0.1845 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.8634 sec.\n",
            "--------------\n",
            "Epoch 18/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 6240 || Loss: 0.1236 || 20iter: 20.0043 sec.\n",
            "iteration 6260 || Loss: 0.1605 || 20iter: 22.3263 sec.\n",
            "iteration 6280 || Loss: 0.2576 || 20iter: 22.2848 sec.\n",
            "iteration 6300 || Loss: 0.1198 || 20iter: 22.2553 sec.\n",
            "iteration 6320 || Loss: 0.1396 || 20iter: 22.2631 sec.\n",
            "iteration 6340 || Loss: 0.0666 || 20iter: 22.2390 sec.\n",
            "iteration 6360 || Loss: 0.2505 || 20iter: 22.1976 sec.\n",
            "iteration 6380 || Loss: 0.1409 || 20iter: 22.2833 sec.\n",
            "iteration 6400 || Loss: 0.1714 || 20iter: 22.3005 sec.\n",
            "iteration 6420 || Loss: 0.1563 || 20iter: 22.2841 sec.\n",
            "iteration 6440 || Loss: 0.2548 || 20iter: 22.2878 sec.\n",
            "iteration 6460 || Loss: 0.2684 || 20iter: 22.2920 sec.\n",
            "iteration 6480 || Loss: 0.0549 || 20iter: 22.3655 sec.\n",
            "iteration 6500 || Loss: 0.1597 || 20iter: 22.2531 sec.\n",
            "iteration 6520 || Loss: 0.2052 || 20iter: 22.3152 sec.\n",
            "iteration 6540 || Loss: 0.1467 || 20iter: 22.3409 sec.\n",
            "iteration 6560 || Loss: 0.1029 || 20iter: 22.3092 sec.\n",
            "iteration 6580 || Loss: 0.0994 || 20iter: 22.2711 sec.\n",
            "--------------\n",
            "epoch 18 || Epoch_TRAIN_Loss: 0.1819 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.8728 sec.\n",
            "--------------\n",
            "Epoch 19/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 6600 || Loss: 0.1827 || 20iter: 13.0139 sec.\n",
            "iteration 6620 || Loss: 0.2461 || 20iter: 22.2600 sec.\n",
            "iteration 6640 || Loss: 0.1210 || 20iter: 22.1668 sec.\n",
            "iteration 6660 || Loss: 0.1063 || 20iter: 22.2391 sec.\n",
            "iteration 6680 || Loss: 0.1306 || 20iter: 22.2904 sec.\n",
            "iteration 6700 || Loss: 0.1509 || 20iter: 22.1799 sec.\n",
            "iteration 6720 || Loss: 0.1082 || 20iter: 22.2565 sec.\n",
            "iteration 6740 || Loss: 0.2162 || 20iter: 22.3433 sec.\n",
            "iteration 6760 || Loss: 0.1825 || 20iter: 22.3021 sec.\n",
            "iteration 6780 || Loss: 0.1326 || 20iter: 22.2481 sec.\n",
            "iteration 6800 || Loss: 0.2038 || 20iter: 22.3105 sec.\n",
            "iteration 6820 || Loss: 0.1130 || 20iter: 22.2495 sec.\n",
            "iteration 6840 || Loss: 0.1091 || 20iter: 22.3044 sec.\n",
            "iteration 6860 || Loss: 0.1280 || 20iter: 22.2624 sec.\n",
            "iteration 6880 || Loss: 0.4635 || 20iter: 22.3524 sec.\n",
            "iteration 6900 || Loss: 0.3260 || 20iter: 22.3405 sec.\n",
            "iteration 6920 || Loss: 0.1059 || 20iter: 22.2959 sec.\n",
            "iteration 6940 || Loss: 0.1091 || 20iter: 22.2742 sec.\n",
            "--------------\n",
            "epoch 19 || Epoch_TRAIN_Loss: 0.1841 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.6214 sec.\n",
            "--------------\n",
            "Epoch 20/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 6960 || Loss: 0.1816 || 20iter: 6.0770 sec.\n",
            "iteration 6980 || Loss: 0.1465 || 20iter: 22.2602 sec.\n",
            "iteration 7000 || Loss: 0.1748 || 20iter: 22.2324 sec.\n",
            "iteration 7020 || Loss: 0.0732 || 20iter: 22.2616 sec.\n",
            "iteration 7040 || Loss: 0.1435 || 20iter: 22.2800 sec.\n",
            "iteration 7060 || Loss: 0.2175 || 20iter: 22.2340 sec.\n",
            "iteration 7080 || Loss: 0.1507 || 20iter: 22.3892 sec.\n",
            "iteration 7100 || Loss: 0.3363 || 20iter: 22.3492 sec.\n",
            "iteration 7120 || Loss: 0.1881 || 20iter: 22.2914 sec.\n",
            "iteration 7140 || Loss: 0.3614 || 20iter: 22.2853 sec.\n",
            "iteration 7160 || Loss: 0.0991 || 20iter: 22.3122 sec.\n",
            "iteration 7180 || Loss: 0.1510 || 20iter: 22.3199 sec.\n",
            "iteration 7200 || Loss: 0.2598 || 20iter: 22.2842 sec.\n",
            "iteration 7220 || Loss: 0.2508 || 20iter: 22.2676 sec.\n",
            "iteration 7240 || Loss: 0.1851 || 20iter: 22.2133 sec.\n",
            "iteration 7260 || Loss: 0.1700 || 20iter: 22.2503 sec.\n",
            "iteration 7280 || Loss: 0.1191 || 20iter: 22.3859 sec.\n",
            "iteration 7300 || Loss: 0.1705 || 20iter: 22.2823 sec.\n",
            "iteration 7320 || Loss: 0.2402 || 20iter: 22.2665 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 20 || Epoch_TRAIN_Loss: 0.1840 || Epoch_VAL_Loss: 0.1979\n",
            "timer: 542.8455 sec.\n",
            "--------------\n",
            "Epoch 21/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 7340 || Loss: 0.3857 || 20iter: 22.2949 sec.\n",
            "iteration 7360 || Loss: 0.1500 || 20iter: 22.2752 sec.\n",
            "iteration 7380 || Loss: 0.2111 || 20iter: 22.2180 sec.\n",
            "iteration 7400 || Loss: 0.0674 || 20iter: 22.3031 sec.\n",
            "iteration 7420 || Loss: 0.0843 || 20iter: 22.3386 sec.\n",
            "iteration 7440 || Loss: 0.2940 || 20iter: 22.1929 sec.\n",
            "iteration 7460 || Loss: 0.3104 || 20iter: 22.2934 sec.\n",
            "iteration 7480 || Loss: 0.1126 || 20iter: 22.3388 sec.\n",
            "iteration 7500 || Loss: 0.1662 || 20iter: 22.3285 sec.\n",
            "iteration 7520 || Loss: 0.1277 || 20iter: 22.2933 sec.\n",
            "iteration 7540 || Loss: 0.1730 || 20iter: 22.3191 sec.\n",
            "iteration 7560 || Loss: 0.3989 || 20iter: 22.3738 sec.\n",
            "iteration 7580 || Loss: 0.1803 || 20iter: 22.2420 sec.\n",
            "iteration 7600 || Loss: 0.2015 || 20iter: 22.2164 sec.\n",
            "iteration 7620 || Loss: 0.4075 || 20iter: 22.2798 sec.\n",
            "iteration 7640 || Loss: 0.1296 || 20iter: 22.2428 sec.\n",
            "iteration 7660 || Loss: 0.1121 || 20iter: 22.2939 sec.\n",
            "iteration 7680 || Loss: 0.1361 || 20iter: 22.1792 sec.\n",
            "--------------\n",
            "epoch 21 || Epoch_TRAIN_Loss: 0.1812 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.7046 sec.\n",
            "--------------\n",
            "Epoch 22/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 7700 || Loss: 0.1684 || 20iter: 15.2773 sec.\n",
            "iteration 7720 || Loss: 0.1192 || 20iter: 22.2317 sec.\n",
            "iteration 7740 || Loss: 0.1223 || 20iter: 22.2393 sec.\n",
            "iteration 7760 || Loss: 0.2015 || 20iter: 22.2425 sec.\n",
            "iteration 7780 || Loss: 0.1005 || 20iter: 22.1893 sec.\n",
            "iteration 7800 || Loss: 0.3005 || 20iter: 22.1977 sec.\n",
            "iteration 7820 || Loss: 0.2168 || 20iter: 22.1925 sec.\n",
            "iteration 7840 || Loss: 0.0819 || 20iter: 22.1998 sec.\n",
            "iteration 7860 || Loss: 0.1979 || 20iter: 22.2222 sec.\n",
            "iteration 7880 || Loss: 0.2424 || 20iter: 22.3021 sec.\n",
            "iteration 7900 || Loss: 0.2985 || 20iter: 22.2394 sec.\n",
            "iteration 7920 || Loss: 0.4413 || 20iter: 22.2621 sec.\n",
            "iteration 7940 || Loss: 0.1610 || 20iter: 22.3143 sec.\n",
            "iteration 7960 || Loss: 0.1422 || 20iter: 22.2759 sec.\n",
            "iteration 7980 || Loss: 0.2525 || 20iter: 22.3438 sec.\n",
            "iteration 8000 || Loss: 0.1232 || 20iter: 22.3002 sec.\n",
            "iteration 8020 || Loss: 0.2193 || 20iter: 22.2823 sec.\n",
            "iteration 8040 || Loss: 0.1581 || 20iter: 22.2569 sec.\n",
            "--------------\n",
            "epoch 22 || Epoch_TRAIN_Loss: 0.1766 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 423.1507 sec.\n",
            "--------------\n",
            "Epoch 23/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 8060 || Loss: 0.1919 || 20iter: 8.4085 sec.\n",
            "iteration 8080 || Loss: 0.1862 || 20iter: 22.3298 sec.\n",
            "iteration 8100 || Loss: 0.1260 || 20iter: 22.2605 sec.\n",
            "iteration 8120 || Loss: 0.1094 || 20iter: 22.3182 sec.\n",
            "iteration 8140 || Loss: 0.3189 || 20iter: 22.3221 sec.\n",
            "iteration 8160 || Loss: 0.1408 || 20iter: 22.3827 sec.\n",
            "iteration 8180 || Loss: 0.2533 || 20iter: 22.3895 sec.\n",
            "iteration 8200 || Loss: 0.0619 || 20iter: 22.2644 sec.\n",
            "iteration 8220 || Loss: 0.2059 || 20iter: 22.3709 sec.\n",
            "iteration 8240 || Loss: 0.3007 || 20iter: 22.4130 sec.\n",
            "iteration 8260 || Loss: 0.1420 || 20iter: 22.4256 sec.\n",
            "iteration 8280 || Loss: 0.2412 || 20iter: 22.3731 sec.\n",
            "iteration 8300 || Loss: 0.1693 || 20iter: 22.4449 sec.\n",
            "iteration 8320 || Loss: 0.2183 || 20iter: 22.3826 sec.\n",
            "iteration 8340 || Loss: 0.3769 || 20iter: 22.3876 sec.\n",
            "iteration 8360 || Loss: 0.0656 || 20iter: 22.3976 sec.\n",
            "iteration 8380 || Loss: 0.1869 || 20iter: 22.3911 sec.\n",
            "iteration 8400 || Loss: 0.2824 || 20iter: 22.4649 sec.\n",
            "--------------\n",
            "epoch 23 || Epoch_TRAIN_Loss: 0.1805 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 425.3627 sec.\n",
            "--------------\n",
            "Epoch 24/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 8420 || Loss: 0.1396 || 20iter: 1.4750 sec.\n",
            "iteration 8440 || Loss: 0.1424 || 20iter: 22.5002 sec.\n",
            "iteration 8460 || Loss: 0.1666 || 20iter: 22.3549 sec.\n",
            "iteration 8480 || Loss: 0.1491 || 20iter: 22.4624 sec.\n",
            "iteration 8500 || Loss: 0.2415 || 20iter: 22.4324 sec.\n",
            "iteration 8520 || Loss: 0.2422 || 20iter: 22.4580 sec.\n",
            "iteration 8540 || Loss: 0.3605 || 20iter: 22.4812 sec.\n",
            "iteration 8560 || Loss: 0.1391 || 20iter: 22.4329 sec.\n",
            "iteration 8580 || Loss: 0.1979 || 20iter: 22.3902 sec.\n",
            "iteration 8600 || Loss: 0.2376 || 20iter: 22.4763 sec.\n",
            "iteration 8620 || Loss: 0.2110 || 20iter: 22.4692 sec.\n",
            "iteration 8640 || Loss: 0.0876 || 20iter: 22.3865 sec.\n",
            "iteration 8660 || Loss: 0.1228 || 20iter: 22.4032 sec.\n",
            "iteration 8680 || Loss: 0.0979 || 20iter: 22.3960 sec.\n",
            "iteration 8700 || Loss: 0.1470 || 20iter: 22.4106 sec.\n",
            "iteration 8720 || Loss: 0.1877 || 20iter: 22.4880 sec.\n",
            "iteration 8740 || Loss: 0.1718 || 20iter: 22.4276 sec.\n",
            "iteration 8760 || Loss: 0.1262 || 20iter: 22.4362 sec.\n",
            "iteration 8780 || Loss: 0.2325 || 20iter: 22.4380 sec.\n",
            "--------------\n",
            "epoch 24 || Epoch_TRAIN_Loss: 0.1795 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 426.6072 sec.\n",
            "--------------\n",
            "Epoch 25/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 8800 || Loss: 0.2646 || 20iter: 17.7355 sec.\n",
            "iteration 8820 || Loss: 0.1070 || 20iter: 22.3836 sec.\n",
            "iteration 8840 || Loss: 0.1735 || 20iter: 22.3751 sec.\n",
            "iteration 8860 || Loss: 0.0919 || 20iter: 22.3839 sec.\n",
            "iteration 8880 || Loss: 0.2346 || 20iter: 22.3963 sec.\n",
            "iteration 8900 || Loss: 0.1234 || 20iter: 22.4140 sec.\n",
            "iteration 8920 || Loss: 0.2228 || 20iter: 22.4339 sec.\n",
            "iteration 8940 || Loss: 0.1601 || 20iter: 22.3703 sec.\n",
            "iteration 8960 || Loss: 0.3042 || 20iter: 22.4908 sec.\n",
            "iteration 8980 || Loss: 0.2024 || 20iter: 22.3579 sec.\n",
            "iteration 9000 || Loss: 0.0809 || 20iter: 22.4256 sec.\n",
            "iteration 9020 || Loss: 0.2142 || 20iter: 22.4022 sec.\n",
            "iteration 9040 || Loss: 0.3549 || 20iter: 22.4252 sec.\n",
            "iteration 9060 || Loss: 0.1161 || 20iter: 22.4805 sec.\n",
            "iteration 9080 || Loss: 0.0765 || 20iter: 22.4227 sec.\n",
            "iteration 9100 || Loss: 0.1144 || 20iter: 22.3072 sec.\n",
            "iteration 9120 || Loss: 0.2484 || 20iter: 22.2313 sec.\n",
            "iteration 9140 || Loss: 0.2696 || 20iter: 22.3385 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 25 || Epoch_TRAIN_Loss: 0.1771 || Epoch_VAL_Loss: 0.1976\n",
            "timer: 545.4584 sec.\n",
            "--------------\n",
            "Epoch 26/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 9160 || Loss: 0.2043 || 20iter: 10.7733 sec.\n",
            "iteration 9180 || Loss: 0.2735 || 20iter: 22.3749 sec.\n",
            "iteration 9200 || Loss: 0.2291 || 20iter: 22.3817 sec.\n",
            "iteration 9220 || Loss: 0.1312 || 20iter: 22.5020 sec.\n",
            "iteration 9240 || Loss: 0.1303 || 20iter: 22.4586 sec.\n",
            "iteration 9260 || Loss: 0.2395 || 20iter: 22.3590 sec.\n",
            "iteration 9280 || Loss: 0.1993 || 20iter: 22.3557 sec.\n",
            "iteration 9300 || Loss: 0.1549 || 20iter: 22.4094 sec.\n",
            "iteration 9320 || Loss: 0.1567 || 20iter: 22.4008 sec.\n",
            "iteration 9340 || Loss: 0.3994 || 20iter: 22.4874 sec.\n",
            "iteration 9360 || Loss: 0.1282 || 20iter: 22.4269 sec.\n",
            "iteration 9380 || Loss: 0.1042 || 20iter: 22.4532 sec.\n",
            "iteration 9400 || Loss: 0.1659 || 20iter: 22.4651 sec.\n",
            "iteration 9420 || Loss: 0.1174 || 20iter: 22.4650 sec.\n",
            "iteration 9440 || Loss: 0.1662 || 20iter: 22.5205 sec.\n",
            "iteration 9460 || Loss: 0.0986 || 20iter: 22.4893 sec.\n",
            "iteration 9480 || Loss: 0.0733 || 20iter: 22.4583 sec.\n",
            "iteration 9500 || Loss: 0.2579 || 20iter: 22.3899 sec.\n",
            "--------------\n",
            "epoch 26 || Epoch_TRAIN_Loss: 0.1777 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 426.4947 sec.\n",
            "--------------\n",
            "Epoch 27/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 9520 || Loss: 0.1047 || 20iter: 3.8007 sec.\n",
            "iteration 9540 || Loss: 0.1273 || 20iter: 22.4089 sec.\n",
            "iteration 9560 || Loss: 0.1354 || 20iter: 22.4445 sec.\n",
            "iteration 9580 || Loss: 0.1082 || 20iter: 22.3072 sec.\n",
            "iteration 9600 || Loss: 0.0621 || 20iter: 22.2233 sec.\n",
            "iteration 9620 || Loss: 0.1989 || 20iter: 22.4041 sec.\n",
            "iteration 9640 || Loss: 0.1066 || 20iter: 22.3037 sec.\n",
            "iteration 9660 || Loss: 0.1241 || 20iter: 22.3051 sec.\n",
            "iteration 9680 || Loss: 0.2257 || 20iter: 22.3325 sec.\n",
            "iteration 9700 || Loss: 0.1212 || 20iter: 22.3243 sec.\n",
            "iteration 9720 || Loss: 0.2000 || 20iter: 22.3048 sec.\n",
            "iteration 9740 || Loss: 0.1555 || 20iter: 22.3827 sec.\n",
            "iteration 9760 || Loss: 0.1041 || 20iter: 22.4197 sec.\n",
            "iteration 9780 || Loss: 0.2366 || 20iter: 22.3291 sec.\n",
            "iteration 9800 || Loss: 0.2406 || 20iter: 22.4933 sec.\n",
            "iteration 9820 || Loss: 0.1562 || 20iter: 22.3506 sec.\n",
            "iteration 9840 || Loss: 0.1910 || 20iter: 22.3801 sec.\n",
            "iteration 9860 || Loss: 0.1295 || 20iter: 22.4080 sec.\n",
            "iteration 9880 || Loss: 0.0486 || 20iter: 22.4465 sec.\n",
            "--------------\n",
            "epoch 27 || Epoch_TRAIN_Loss: 0.1685 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 425.2638 sec.\n",
            "--------------\n",
            "Epoch 28/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 9900 || Loss: 0.1366 || 20iter: 20.1388 sec.\n",
            "iteration 9920 || Loss: 0.2103 || 20iter: 22.4810 sec.\n",
            "iteration 9940 || Loss: 0.2199 || 20iter: 22.4633 sec.\n",
            "iteration 9960 || Loss: 0.2001 || 20iter: 22.3662 sec.\n",
            "iteration 9980 || Loss: 0.2804 || 20iter: 22.4381 sec.\n",
            "iteration 10000 || Loss: 0.2779 || 20iter: 22.4055 sec.\n",
            "iteration 10020 || Loss: 0.1253 || 20iter: 22.4618 sec.\n",
            "iteration 10040 || Loss: 0.2837 || 20iter: 22.5335 sec.\n",
            "iteration 10060 || Loss: 0.0888 || 20iter: 22.5316 sec.\n",
            "iteration 10080 || Loss: 0.1581 || 20iter: 22.5298 sec.\n",
            "iteration 10100 || Loss: 0.1101 || 20iter: 22.4226 sec.\n",
            "iteration 10120 || Loss: 0.1707 || 20iter: 22.5145 sec.\n",
            "iteration 10140 || Loss: 0.2353 || 20iter: 22.3932 sec.\n",
            "iteration 10160 || Loss: 0.0803 || 20iter: 22.5055 sec.\n",
            "iteration 10180 || Loss: 0.2023 || 20iter: 22.5022 sec.\n",
            "iteration 10200 || Loss: 0.1430 || 20iter: 22.4666 sec.\n",
            "iteration 10220 || Loss: 0.0893 || 20iter: 22.4314 sec.\n",
            "iteration 10240 || Loss: 0.1325 || 20iter: 22.4314 sec.\n",
            "--------------\n",
            "epoch 28 || Epoch_TRAIN_Loss: 0.1724 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 427.0978 sec.\n",
            "--------------\n",
            "Epoch 29/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 10260 || Loss: 0.1325 || 20iter: 13.1570 sec.\n",
            "iteration 10280 || Loss: 0.1597 || 20iter: 22.6215 sec.\n",
            "iteration 10300 || Loss: 0.2549 || 20iter: 22.4739 sec.\n",
            "iteration 10320 || Loss: 0.1569 || 20iter: 22.4660 sec.\n",
            "iteration 10340 || Loss: 0.2327 || 20iter: 22.5172 sec.\n",
            "iteration 10360 || Loss: 0.3105 || 20iter: 22.5377 sec.\n",
            "iteration 10380 || Loss: 0.3772 || 20iter: 22.3927 sec.\n",
            "iteration 10400 || Loss: 0.1294 || 20iter: 22.2890 sec.\n",
            "iteration 10420 || Loss: 0.1493 || 20iter: 22.4033 sec.\n",
            "iteration 10440 || Loss: 0.1808 || 20iter: 22.3625 sec.\n",
            "iteration 10460 || Loss: 0.0779 || 20iter: 22.2727 sec.\n",
            "iteration 10480 || Loss: 0.2441 || 20iter: 22.2527 sec.\n",
            "iteration 10500 || Loss: 0.1904 || 20iter: 22.3558 sec.\n",
            "iteration 10520 || Loss: 0.0739 || 20iter: 22.2807 sec.\n",
            "iteration 10540 || Loss: 0.1109 || 20iter: 22.3234 sec.\n",
            "iteration 10560 || Loss: 0.0674 || 20iter: 22.2441 sec.\n",
            "iteration 10580 || Loss: 0.0761 || 20iter: 22.2089 sec.\n",
            "iteration 10600 || Loss: 0.2939 || 20iter: 22.2099 sec.\n",
            "--------------\n",
            "epoch 29 || Epoch_TRAIN_Loss: 0.1753 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 425.2171 sec.\n",
            "--------------\n",
            "Epoch 30/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 10620 || Loss: 0.0869 || 20iter: 6.0682 sec.\n",
            "iteration 10640 || Loss: 0.0666 || 20iter: 22.2898 sec.\n",
            "iteration 10660 || Loss: 0.1936 || 20iter: 22.1727 sec.\n",
            "iteration 10680 || Loss: 0.2815 || 20iter: 22.2227 sec.\n",
            "iteration 10700 || Loss: 0.1616 || 20iter: 22.2914 sec.\n",
            "iteration 10720 || Loss: 0.1257 || 20iter: 22.2443 sec.\n",
            "iteration 10740 || Loss: 0.1382 || 20iter: 22.2615 sec.\n",
            "iteration 10760 || Loss: 0.0531 || 20iter: 22.2946 sec.\n",
            "iteration 10780 || Loss: 0.1416 || 20iter: 22.2776 sec.\n",
            "iteration 10800 || Loss: 0.2617 || 20iter: 22.2889 sec.\n",
            "iteration 10820 || Loss: 0.1270 || 20iter: 22.3957 sec.\n",
            "iteration 10840 || Loss: 0.1667 || 20iter: 22.3355 sec.\n",
            "iteration 10860 || Loss: 0.2401 || 20iter: 22.3355 sec.\n",
            "iteration 10880 || Loss: 0.0799 || 20iter: 22.3340 sec.\n",
            "iteration 10900 || Loss: 0.0962 || 20iter: 22.4554 sec.\n",
            "iteration 10920 || Loss: 0.3119 || 20iter: 22.3400 sec.\n",
            "iteration 10940 || Loss: 0.1728 || 20iter: 22.3346 sec.\n",
            "iteration 10960 || Loss: 0.1460 || 20iter: 22.4307 sec.\n",
            "iteration 10980 || Loss: 0.3976 || 20iter: 22.3711 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 30 || Epoch_TRAIN_Loss: 0.1708 || Epoch_VAL_Loss: 0.1931\n",
            "timer: 544.9642 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZJWV8b_-i4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqPTK67w6J5J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}