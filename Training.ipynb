{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "mount_file_id": "1uOYzpLivT1f2eHGONfUCpuNk5rc9z-6S",
      "authorship_tag": "ABX9TyM4LkgvpAZsVpbZKiQAf6EV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KokiNiimura/study/blob/master/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Dtu9s5Lkxp",
        "outputId": "91cfd101-7f20-43ff-fe0c-4b5199978389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/My Drive/study/PyTorch_Advanced/03"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/study/PyTorch_Advanced/03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXKa0gG4MioW"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFCgKVvvdssn"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_nZJeoZd0rq"
      },
      "source": [
        "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
        "\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath=rootpath)\n",
        "\n",
        "color_mean = (0.485, 0.456, 0.406)\n",
        "color_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", \n",
        "                           transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", \n",
        "                           transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK50NyXSeuzo"
      },
      "source": [
        "from utils.pspnet import PSPNet\n",
        "\n",
        "net = PSPNet(n_classes=21)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOixAacFpxma",
        "outputId": "6cdf1872-e369-483f-b9db-7728e903f105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PSPNet(\n",
              "  (feature_conv): FeatureMap_convolution(\n",
              "    (cbnr_1): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (cbnr_2): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (cbnr_3): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_res_1): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (feature_res_2): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block4): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (feature_dilated_res_1): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block4): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block5): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block6): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (feature_dilated_res_2): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (pyramid_pooling): PyramidPooling(\n",
              "    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n",
              "    (cbr_1): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n",
              "    (cbr_2): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n",
              "    (cbr_3): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n",
              "    (cbr_4): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (decode_feature): DecodePSPFeature(\n",
              "    (cbr): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (aux): AuxiliaryPSPlayers(\n",
              "    (cbr): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgDZepO_p2sW"
      },
      "source": [
        "class PSPLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, aux_weight=0.4):\n",
        "        super(PSPLoss, self).__init__()\n",
        "        self.aux_weight = aux_weight\n",
        "\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
        "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
        "\n",
        "        return loss + self.aux_weight*loss_aux\n",
        "\n",
        "criterion = PSPLoss(aux_weight=0.4)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EXrJsEYt1ND"
      },
      "source": [
        "optimizer = optim.SGD([\n",
        "    {'params': net.feature_conv.parameters(), 'lr': 1e-3}, \n",
        "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3}, \n",
        "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3}, \n",
        "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3}, \n",
        "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3}, \n",
        "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3}, \n",
        "    {'params': net.decode_feature.parameters(), 'lr': 1e-2}, \n",
        "    {'params': net.aux.parameters(), 'lr': 1e-2}, \n",
        "], momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "def lambda_epoch(epoch):\n",
        "    max_epoch = 30\n",
        "    return math.pow((1-epoch/max_epoch), 0.9)\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GguQkl7qFT6"
      },
      "source": [
        "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device: {}\".format(device))\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
        "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    iteration = 1\n",
        "    logs = []\n",
        "\n",
        "    batch_multiplier = 6\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_val_loss = 0.0\n",
        "\n",
        "        print('--------------')\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('--------------')\n",
        "        \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                print('(train)')\n",
        "\n",
        "            else:\n",
        "                if ((epoch+1) % 5 == 0):\n",
        "                    net.eval()\n",
        "                    print('--------------')\n",
        "                    print('(val)')\n",
        "                else:\n",
        "                    continue\n",
        "        \n",
        "            count = 0\n",
        "            for images, anno_class_images in dataloaders_dict[phase]:\n",
        "                if images.size()[0] == 1:\n",
        "                    continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                anno_class_images = anno_class_images.to(device)\n",
        "\n",
        "                if (phase == 'train') and (count == 0):\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    count = batch_multiplier\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(images)\n",
        "                    loss = criterion(outputs, anno_class_images.long()) / batch_multiplier\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        count -= 1\n",
        "\n",
        "                        if (iteration % 20 == 0):\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('iteration {} || Loss: {:.4f} || 20iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                        epoch_train_loss += loss.item() * batch_multiplier\n",
        "                        iteration += 1\n",
        "\n",
        "                    else:\n",
        "                        epoch_val_loss += loss.item() * batch_multiplier\n",
        "\n",
        "        t_epoch_finish = time.time()\n",
        "        print('--------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss: {:.4f} || Epoch_VAL_Loss: {:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
        "        print('timer: {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss/num_train_imgs, \n",
        "                        'val_loss': epoch_val_loss/num_val_imgs}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(\"log_output.csv\")\n",
        "\n",
        "        torch.save(net.state_dict(), 'weights/pspnet50_' + str(epoch+1) + '.pth')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRaChXgJrefI",
        "outputId": "a8c6c29e-9a92-4f94-b49b-e7e8e1f617ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_epochs = 30\n",
        "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda:0\n",
            "--------------\n",
            "Epoch 1/30\n",
            "--------------\n",
            "(train)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 20 || Loss: 0.8095 || 20iter: 34.8563 sec.\n",
            "iteration 40 || Loss: 0.4639 || 20iter: 26.5321 sec.\n",
            "iteration 60 || Loss: 0.2629 || 20iter: 25.9936 sec.\n",
            "iteration 80 || Loss: 0.5196 || 20iter: 26.9041 sec.\n",
            "iteration 100 || Loss: 0.1393 || 20iter: 26.6143 sec.\n",
            "iteration 120 || Loss: 0.4731 || 20iter: 26.3384 sec.\n",
            "iteration 140 || Loss: 0.3700 || 20iter: 26.5787 sec.\n",
            "iteration 160 || Loss: 0.2124 || 20iter: 26.5514 sec.\n",
            "iteration 180 || Loss: 0.3651 || 20iter: 26.4365 sec.\n",
            "iteration 200 || Loss: 0.4846 || 20iter: 26.4562 sec.\n",
            "iteration 220 || Loss: 0.6219 || 20iter: 26.4665 sec.\n",
            "iteration 240 || Loss: 0.3101 || 20iter: 26.5571 sec.\n",
            "iteration 260 || Loss: 0.2635 || 20iter: 26.4749 sec.\n",
            "iteration 280 || Loss: 0.5002 || 20iter: 26.4806 sec.\n",
            "iteration 300 || Loss: 0.3085 || 20iter: 26.4540 sec.\n",
            "iteration 320 || Loss: 0.8929 || 20iter: 26.5060 sec.\n",
            "iteration 340 || Loss: 0.6758 || 20iter: 26.5055 sec.\n",
            "iteration 360 || Loss: 0.4334 || 20iter: 26.5071 sec.\n",
            "--------------\n",
            "epoch 1 || Epoch_TRAIN_Loss: 0.4635 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 515.7392 sec.\n",
            "--------------\n",
            "Epoch 2/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 380 || Loss: 0.3170 || 20iter: 18.1506 sec.\n",
            "iteration 400 || Loss: 0.2203 || 20iter: 26.3950 sec.\n",
            "iteration 420 || Loss: 0.2484 || 20iter: 26.4771 sec.\n",
            "iteration 440 || Loss: 0.4485 || 20iter: 26.4553 sec.\n",
            "iteration 460 || Loss: 0.3185 || 20iter: 26.4381 sec.\n",
            "iteration 480 || Loss: 0.1858 || 20iter: 26.5741 sec.\n",
            "iteration 500 || Loss: 0.2986 || 20iter: 26.5859 sec.\n",
            "iteration 520 || Loss: 0.2156 || 20iter: 26.5116 sec.\n",
            "iteration 540 || Loss: 0.3657 || 20iter: 26.5402 sec.\n",
            "iteration 560 || Loss: 0.2084 || 20iter: 26.4788 sec.\n",
            "iteration 580 || Loss: 0.5009 || 20iter: 26.5071 sec.\n",
            "iteration 600 || Loss: 0.2421 || 20iter: 26.4866 sec.\n",
            "iteration 620 || Loss: 0.4122 || 20iter: 26.5391 sec.\n",
            "iteration 640 || Loss: 0.5712 || 20iter: 26.5430 sec.\n",
            "iteration 660 || Loss: 0.4938 || 20iter: 26.4861 sec.\n",
            "iteration 680 || Loss: 0.2860 || 20iter: 26.4873 sec.\n",
            "iteration 700 || Loss: 0.4865 || 20iter: 26.5459 sec.\n",
            "iteration 720 || Loss: 0.9522 || 20iter: 26.3535 sec.\n",
            "--------------\n",
            "epoch 2 || Epoch_TRAIN_Loss: 0.3820 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.2484 sec.\n",
            "--------------\n",
            "Epoch 3/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 740 || Loss: 0.3359 || 20iter: 9.9121 sec.\n",
            "iteration 760 || Loss: 0.3059 || 20iter: 26.4066 sec.\n",
            "iteration 780 || Loss: 0.2932 || 20iter: 26.5305 sec.\n",
            "iteration 800 || Loss: 0.4569 || 20iter: 26.5350 sec.\n",
            "iteration 820 || Loss: 0.3900 || 20iter: 26.5608 sec.\n",
            "iteration 840 || Loss: 0.3559 || 20iter: 26.5622 sec.\n",
            "iteration 860 || Loss: 0.9358 || 20iter: 26.5541 sec.\n",
            "iteration 880 || Loss: 0.3041 || 20iter: 26.5341 sec.\n",
            "iteration 900 || Loss: 0.6990 || 20iter: 26.5290 sec.\n",
            "iteration 920 || Loss: 0.2620 || 20iter: 26.6379 sec.\n",
            "iteration 940 || Loss: 0.5227 || 20iter: 26.5955 sec.\n",
            "iteration 960 || Loss: 0.2332 || 20iter: 26.5305 sec.\n",
            "iteration 980 || Loss: 0.3865 || 20iter: 26.6210 sec.\n",
            "iteration 1000 || Loss: 0.2713 || 20iter: 26.4346 sec.\n",
            "iteration 1020 || Loss: 0.5676 || 20iter: 26.4442 sec.\n",
            "iteration 1040 || Loss: 0.4892 || 20iter: 26.5294 sec.\n",
            "iteration 1060 || Loss: 0.1656 || 20iter: 26.4840 sec.\n",
            "iteration 1080 || Loss: 0.4503 || 20iter: 26.4653 sec.\n",
            "--------------\n",
            "epoch 3 || Epoch_TRAIN_Loss: 0.3749 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.9465 sec.\n",
            "--------------\n",
            "Epoch 4/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 1100 || Loss: 0.5098 || 20iter: 1.5947 sec.\n",
            "iteration 1120 || Loss: 0.2181 || 20iter: 26.4818 sec.\n",
            "iteration 1140 || Loss: 0.3290 || 20iter: 26.5429 sec.\n",
            "iteration 1160 || Loss: 0.3916 || 20iter: 26.4945 sec.\n",
            "iteration 1180 || Loss: 0.4358 || 20iter: 26.4338 sec.\n",
            "iteration 1200 || Loss: 0.2790 || 20iter: 26.5122 sec.\n",
            "iteration 1220 || Loss: 0.8986 || 20iter: 26.4805 sec.\n",
            "iteration 1240 || Loss: 0.2335 || 20iter: 26.5546 sec.\n",
            "iteration 1260 || Loss: 0.3887 || 20iter: 26.5038 sec.\n",
            "iteration 1280 || Loss: 0.4160 || 20iter: 26.5867 sec.\n",
            "iteration 1300 || Loss: 0.3133 || 20iter: 26.5169 sec.\n",
            "iteration 1320 || Loss: 0.4976 || 20iter: 26.5358 sec.\n",
            "iteration 1340 || Loss: 0.5440 || 20iter: 26.5205 sec.\n",
            "iteration 1360 || Loss: 0.4809 || 20iter: 26.5238 sec.\n",
            "iteration 1380 || Loss: 0.4687 || 20iter: 26.4759 sec.\n",
            "iteration 1400 || Loss: 0.4004 || 20iter: 26.5169 sec.\n",
            "iteration 1420 || Loss: 0.2846 || 20iter: 26.5412 sec.\n",
            "iteration 1440 || Loss: 0.2463 || 20iter: 26.4517 sec.\n",
            "iteration 1460 || Loss: 0.2992 || 20iter: 26.4662 sec.\n",
            "--------------\n",
            "epoch 4 || Epoch_TRAIN_Loss: 0.3638 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.6866 sec.\n",
            "--------------\n",
            "Epoch 5/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 1480 || Loss: 0.2196 || 20iter: 21.0683 sec.\n",
            "iteration 1500 || Loss: 0.3145 || 20iter: 26.6218 sec.\n",
            "iteration 1520 || Loss: 0.6168 || 20iter: 26.5647 sec.\n",
            "iteration 1540 || Loss: 0.1952 || 20iter: 26.4121 sec.\n",
            "iteration 1560 || Loss: 0.2822 || 20iter: 26.3993 sec.\n",
            "iteration 1580 || Loss: 0.8320 || 20iter: 26.5115 sec.\n",
            "iteration 1600 || Loss: 0.5694 || 20iter: 26.5357 sec.\n",
            "iteration 1620 || Loss: 0.3177 || 20iter: 26.5234 sec.\n",
            "iteration 1640 || Loss: 0.7061 || 20iter: 26.4835 sec.\n",
            "iteration 1660 || Loss: 0.3888 || 20iter: 26.5273 sec.\n",
            "iteration 1680 || Loss: 0.4437 || 20iter: 26.4708 sec.\n",
            "iteration 1700 || Loss: 0.3024 || 20iter: 26.5110 sec.\n",
            "iteration 1720 || Loss: 0.2526 || 20iter: 26.4769 sec.\n",
            "iteration 1740 || Loss: 0.2668 || 20iter: 26.5427 sec.\n",
            "iteration 1760 || Loss: 0.5377 || 20iter: 26.5290 sec.\n",
            "iteration 1780 || Loss: 0.2592 || 20iter: 26.4885 sec.\n",
            "iteration 1800 || Loss: 0.2025 || 20iter: 26.4142 sec.\n",
            "iteration 1820 || Loss: 0.4530 || 20iter: 26.4763 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 5 || Epoch_TRAIN_Loss: 0.3651 || Epoch_VAL_Loss: 0.4223\n",
            "timer: 665.7899 sec.\n",
            "--------------\n",
            "Epoch 6/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 1840 || Loss: 0.5160 || 20iter: 12.6714 sec.\n",
            "iteration 1860 || Loss: 0.5865 || 20iter: 26.5658 sec.\n",
            "iteration 1880 || Loss: 0.4239 || 20iter: 26.4492 sec.\n",
            "iteration 1900 || Loss: 0.2655 || 20iter: 26.4232 sec.\n",
            "iteration 1920 || Loss: 0.2696 || 20iter: 26.5486 sec.\n",
            "iteration 1940 || Loss: 0.4261 || 20iter: 26.4889 sec.\n",
            "iteration 1960 || Loss: 0.4724 || 20iter: 26.4295 sec.\n",
            "iteration 1980 || Loss: 0.2299 || 20iter: 26.4640 sec.\n",
            "iteration 2000 || Loss: 0.6871 || 20iter: 26.4995 sec.\n",
            "iteration 2020 || Loss: 0.3455 || 20iter: 26.5001 sec.\n",
            "iteration 2040 || Loss: 0.3483 || 20iter: 26.4034 sec.\n",
            "iteration 2060 || Loss: 0.4196 || 20iter: 26.4598 sec.\n",
            "iteration 2080 || Loss: 0.3512 || 20iter: 26.5498 sec.\n",
            "iteration 2100 || Loss: 0.2217 || 20iter: 26.5272 sec.\n",
            "iteration 2120 || Loss: 0.2972 || 20iter: 26.4486 sec.\n",
            "iteration 2140 || Loss: 0.2608 || 20iter: 26.4968 sec.\n",
            "iteration 2160 || Loss: 0.3182 || 20iter: 26.5105 sec.\n",
            "iteration 2180 || Loss: 0.2832 || 20iter: 26.4208 sec.\n",
            "--------------\n",
            "epoch 6 || Epoch_TRAIN_Loss: 0.3578 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.0942 sec.\n",
            "--------------\n",
            "Epoch 7/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 2200 || Loss: 0.2827 || 20iter: 4.3320 sec.\n",
            "iteration 2220 || Loss: 0.6513 || 20iter: 26.5293 sec.\n",
            "iteration 2240 || Loss: 0.3727 || 20iter: 26.5930 sec.\n",
            "iteration 2260 || Loss: 0.2249 || 20iter: 26.5174 sec.\n",
            "iteration 2280 || Loss: 0.6433 || 20iter: 26.4878 sec.\n",
            "iteration 2300 || Loss: 0.3683 || 20iter: 26.4915 sec.\n",
            "iteration 2320 || Loss: 0.4016 || 20iter: 26.4710 sec.\n",
            "iteration 2340 || Loss: 0.3923 || 20iter: 26.4996 sec.\n",
            "iteration 2360 || Loss: 0.4516 || 20iter: 26.5750 sec.\n",
            "iteration 2380 || Loss: 0.3734 || 20iter: 26.4362 sec.\n",
            "iteration 2400 || Loss: 0.5785 || 20iter: 26.4434 sec.\n",
            "iteration 2420 || Loss: 0.4545 || 20iter: 26.4115 sec.\n",
            "iteration 2440 || Loss: 0.2820 || 20iter: 26.4804 sec.\n",
            "iteration 2460 || Loss: 0.3689 || 20iter: 26.5557 sec.\n",
            "iteration 2480 || Loss: 0.6324 || 20iter: 26.4681 sec.\n",
            "iteration 2500 || Loss: 0.3029 || 20iter: 26.4345 sec.\n",
            "iteration 2520 || Loss: 0.5039 || 20iter: 26.4479 sec.\n",
            "iteration 2540 || Loss: 0.2737 || 20iter: 26.4786 sec.\n",
            "iteration 2560 || Loss: 0.3109 || 20iter: 26.4664 sec.\n",
            "--------------\n",
            "epoch 7 || Epoch_TRAIN_Loss: 0.3610 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.2443 sec.\n",
            "--------------\n",
            "Epoch 8/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 2580 || Loss: 0.2959 || 20iter: 23.6768 sec.\n",
            "iteration 2600 || Loss: 0.2377 || 20iter: 26.4538 sec.\n",
            "iteration 2620 || Loss: 0.1717 || 20iter: 26.4698 sec.\n",
            "iteration 2640 || Loss: 0.5254 || 20iter: 26.4378 sec.\n",
            "iteration 2660 || Loss: 0.3859 || 20iter: 26.4310 sec.\n",
            "iteration 2680 || Loss: 0.4946 || 20iter: 26.4740 sec.\n",
            "iteration 2700 || Loss: 0.2550 || 20iter: 26.4551 sec.\n",
            "iteration 2720 || Loss: 0.3195 || 20iter: 26.4813 sec.\n",
            "iteration 2740 || Loss: 0.3567 || 20iter: 26.4923 sec.\n",
            "iteration 2760 || Loss: 0.4399 || 20iter: 26.4150 sec.\n",
            "iteration 2780 || Loss: 0.4518 || 20iter: 26.4706 sec.\n",
            "iteration 2800 || Loss: 0.7846 || 20iter: 26.4084 sec.\n",
            "iteration 2820 || Loss: 0.4339 || 20iter: 26.4948 sec.\n",
            "iteration 2840 || Loss: 0.3283 || 20iter: 26.5756 sec.\n",
            "iteration 2860 || Loss: 0.5715 || 20iter: 26.4924 sec.\n",
            "iteration 2880 || Loss: 0.2564 || 20iter: 26.4500 sec.\n",
            "iteration 2900 || Loss: 0.3237 || 20iter: 26.5109 sec.\n",
            "iteration 2920 || Loss: 0.2004 || 20iter: 26.4522 sec.\n",
            "--------------\n",
            "epoch 8 || Epoch_TRAIN_Loss: 0.3565 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 506.8687 sec.\n",
            "--------------\n",
            "Epoch 9/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 2940 || Loss: 0.3474 || 20iter: 15.4366 sec.\n",
            "iteration 2960 || Loss: 0.1757 || 20iter: 26.4045 sec.\n",
            "iteration 2980 || Loss: 0.4065 || 20iter: 26.4621 sec.\n",
            "iteration 3000 || Loss: 0.3875 || 20iter: 26.4105 sec.\n",
            "iteration 3020 || Loss: 0.4196 || 20iter: 26.3787 sec.\n",
            "iteration 3040 || Loss: 0.2809 || 20iter: 26.4858 sec.\n",
            "iteration 3060 || Loss: 0.5174 || 20iter: 26.4858 sec.\n",
            "iteration 3080 || Loss: 0.3752 || 20iter: 26.4423 sec.\n",
            "iteration 3100 || Loss: 0.2571 || 20iter: 26.5787 sec.\n",
            "iteration 3120 || Loss: 0.2079 || 20iter: 26.4462 sec.\n",
            "iteration 3140 || Loss: 0.2864 || 20iter: 26.4314 sec.\n",
            "iteration 3160 || Loss: 0.3655 || 20iter: 26.4326 sec.\n",
            "iteration 3180 || Loss: 0.3953 || 20iter: 26.4376 sec.\n",
            "iteration 3200 || Loss: 0.3962 || 20iter: 26.4433 sec.\n",
            "iteration 3220 || Loss: 0.1753 || 20iter: 26.4229 sec.\n",
            "iteration 3240 || Loss: 0.5563 || 20iter: 26.4481 sec.\n",
            "iteration 3260 || Loss: 0.3871 || 20iter: 26.4884 sec.\n",
            "iteration 3280 || Loss: 0.2951 || 20iter: 26.3937 sec.\n",
            "--------------\n",
            "epoch 9 || Epoch_TRAIN_Loss: 0.3465 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 506.5642 sec.\n",
            "--------------\n",
            "Epoch 10/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 3300 || Loss: 0.6739 || 20iter: 7.1423 sec.\n",
            "iteration 3320 || Loss: 0.3643 || 20iter: 26.4538 sec.\n",
            "iteration 3340 || Loss: 0.3107 || 20iter: 26.4548 sec.\n",
            "iteration 3360 || Loss: 0.2651 || 20iter: 26.4163 sec.\n",
            "iteration 3380 || Loss: 0.4843 || 20iter: 26.4370 sec.\n",
            "iteration 3400 || Loss: 0.3154 || 20iter: 26.4840 sec.\n",
            "iteration 3420 || Loss: 0.6040 || 20iter: 26.5167 sec.\n",
            "iteration 3440 || Loss: 0.1558 || 20iter: 26.4107 sec.\n",
            "iteration 3460 || Loss: 0.5509 || 20iter: 26.4453 sec.\n",
            "iteration 3480 || Loss: 0.2105 || 20iter: 26.5963 sec.\n",
            "iteration 3500 || Loss: 0.4186 || 20iter: 26.5320 sec.\n",
            "iteration 3520 || Loss: 0.3250 || 20iter: 26.4230 sec.\n",
            "iteration 3540 || Loss: 0.5509 || 20iter: 26.4508 sec.\n",
            "iteration 3560 || Loss: 0.3155 || 20iter: 26.4370 sec.\n",
            "iteration 3580 || Loss: 0.4060 || 20iter: 26.5364 sec.\n",
            "iteration 3600 || Loss: 0.2239 || 20iter: 26.3324 sec.\n",
            "iteration 3620 || Loss: 0.3297 || 20iter: 26.4500 sec.\n",
            "iteration 3640 || Loss: 0.3680 || 20iter: 26.4118 sec.\n",
            "iteration 3660 || Loss: 0.3202 || 20iter: 26.4315 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 10 || Epoch_TRAIN_Loss: 0.3391 || Epoch_VAL_Loss: 0.3922\n",
            "timer: 664.5847 sec.\n",
            "--------------\n",
            "Epoch 11/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 3680 || Loss: 0.4439 || 20iter: 26.4155 sec.\n",
            "iteration 3700 || Loss: 0.2471 || 20iter: 26.4988 sec.\n",
            "iteration 3720 || Loss: 0.3123 || 20iter: 26.4692 sec.\n",
            "iteration 3740 || Loss: 0.3396 || 20iter: 26.3238 sec.\n",
            "iteration 3760 || Loss: 0.1763 || 20iter: 26.4017 sec.\n",
            "iteration 3780 || Loss: 0.5181 || 20iter: 26.4250 sec.\n",
            "iteration 3800 || Loss: 0.3819 || 20iter: 26.4175 sec.\n",
            "iteration 3820 || Loss: 0.3833 || 20iter: 26.5485 sec.\n",
            "iteration 3840 || Loss: 0.3070 || 20iter: 26.4619 sec.\n",
            "iteration 3860 || Loss: 0.3750 || 20iter: 26.4119 sec.\n",
            "iteration 3880 || Loss: 0.4939 || 20iter: 26.4776 sec.\n",
            "iteration 3900 || Loss: 0.3826 || 20iter: 26.4387 sec.\n",
            "iteration 3920 || Loss: 0.3170 || 20iter: 26.5175 sec.\n",
            "iteration 3940 || Loss: 0.3602 || 20iter: 26.5404 sec.\n",
            "iteration 3960 || Loss: 0.2922 || 20iter: 26.4977 sec.\n",
            "iteration 3980 || Loss: 0.1549 || 20iter: 26.4475 sec.\n",
            "iteration 4000 || Loss: 0.4077 || 20iter: 26.4388 sec.\n",
            "iteration 4020 || Loss: 0.4435 || 20iter: 26.4420 sec.\n",
            "--------------\n",
            "epoch 11 || Epoch_TRAIN_Loss: 0.3365 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 506.6145 sec.\n",
            "--------------\n",
            "Epoch 12/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 4040 || Loss: 0.2587 || 20iter: 18.2375 sec.\n",
            "iteration 4060 || Loss: 0.1609 || 20iter: 26.5664 sec.\n",
            "iteration 4080 || Loss: 0.3948 || 20iter: 26.5127 sec.\n",
            "iteration 4100 || Loss: 0.1479 || 20iter: 26.4261 sec.\n",
            "iteration 4120 || Loss: 0.1255 || 20iter: 26.4106 sec.\n",
            "iteration 4140 || Loss: 0.5613 || 20iter: 26.4796 sec.\n",
            "iteration 4160 || Loss: 0.2604 || 20iter: 26.4770 sec.\n",
            "iteration 4180 || Loss: 0.1515 || 20iter: 26.4569 sec.\n",
            "iteration 4200 || Loss: 0.2832 || 20iter: 26.4601 sec.\n",
            "iteration 4220 || Loss: 0.3934 || 20iter: 26.4652 sec.\n",
            "iteration 4240 || Loss: 0.3574 || 20iter: 26.4843 sec.\n",
            "iteration 4260 || Loss: 0.2427 || 20iter: 26.4125 sec.\n",
            "iteration 4280 || Loss: 0.8011 || 20iter: 26.4616 sec.\n",
            "iteration 4300 || Loss: 0.5847 || 20iter: 26.5527 sec.\n",
            "iteration 4320 || Loss: 0.2378 || 20iter: 26.4521 sec.\n",
            "iteration 4340 || Loss: 0.3198 || 20iter: 26.5634 sec.\n",
            "iteration 4360 || Loss: 0.6551 || 20iter: 26.4624 sec.\n",
            "iteration 4380 || Loss: 0.5269 || 20iter: 26.3993 sec.\n",
            "--------------\n",
            "epoch 12 || Epoch_TRAIN_Loss: 0.3294 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 506.9573 sec.\n",
            "--------------\n",
            "Epoch 13/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 4400 || Loss: 0.2244 || 20iter: 9.8527 sec.\n",
            "iteration 4420 || Loss: 0.3728 || 20iter: 26.4210 sec.\n",
            "iteration 4440 || Loss: 0.5497 || 20iter: 26.5606 sec.\n",
            "iteration 4460 || Loss: 0.1935 || 20iter: 26.5066 sec.\n",
            "iteration 4480 || Loss: 0.3875 || 20iter: 26.4404 sec.\n",
            "iteration 4500 || Loss: 0.2687 || 20iter: 26.3846 sec.\n",
            "iteration 4520 || Loss: 0.6113 || 20iter: 26.4391 sec.\n",
            "iteration 4540 || Loss: 0.3201 || 20iter: 26.4400 sec.\n",
            "iteration 4560 || Loss: 0.1922 || 20iter: 26.3965 sec.\n",
            "iteration 4580 || Loss: 0.2829 || 20iter: 26.4441 sec.\n",
            "iteration 4600 || Loss: 0.4707 || 20iter: 26.5043 sec.\n",
            "iteration 4620 || Loss: 0.2018 || 20iter: 26.4353 sec.\n",
            "iteration 4640 || Loss: 0.2940 || 20iter: 26.5110 sec.\n",
            "iteration 4660 || Loss: 0.3087 || 20iter: 26.4301 sec.\n",
            "iteration 4680 || Loss: 0.2460 || 20iter: 26.4838 sec.\n",
            "iteration 4700 || Loss: 0.1655 || 20iter: 26.5034 sec.\n",
            "iteration 4720 || Loss: 0.4969 || 20iter: 26.5692 sec.\n",
            "iteration 4740 || Loss: 0.3983 || 20iter: 26.4991 sec.\n",
            "--------------\n",
            "epoch 13 || Epoch_TRAIN_Loss: 0.3302 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 506.9024 sec.\n",
            "--------------\n",
            "Epoch 14/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 4760 || Loss: 0.1291 || 20iter: 1.5855 sec.\n",
            "iteration 4780 || Loss: 0.3031 || 20iter: 26.5454 sec.\n",
            "iteration 4800 || Loss: 0.3663 || 20iter: 26.4865 sec.\n",
            "iteration 4820 || Loss: 0.2681 || 20iter: 26.5063 sec.\n",
            "iteration 4840 || Loss: 0.4553 || 20iter: 26.4777 sec.\n",
            "iteration 4860 || Loss: 0.3131 || 20iter: 26.5862 sec.\n",
            "iteration 4880 || Loss: 0.2752 || 20iter: 26.4968 sec.\n",
            "iteration 4900 || Loss: 0.3743 || 20iter: 26.3994 sec.\n",
            "iteration 4920 || Loss: 0.5116 || 20iter: 26.5133 sec.\n",
            "iteration 4940 || Loss: 0.2514 || 20iter: 26.4541 sec.\n",
            "iteration 4960 || Loss: 0.2194 || 20iter: 26.3679 sec.\n",
            "iteration 4980 || Loss: 0.2506 || 20iter: 26.5142 sec.\n",
            "iteration 5000 || Loss: 0.3849 || 20iter: 26.4645 sec.\n",
            "iteration 5020 || Loss: 0.2705 || 20iter: 26.4815 sec.\n",
            "iteration 5040 || Loss: 0.5587 || 20iter: 26.3954 sec.\n",
            "iteration 5060 || Loss: 0.3560 || 20iter: 26.3768 sec.\n",
            "iteration 5080 || Loss: 0.4768 || 20iter: 26.4358 sec.\n",
            "iteration 5100 || Loss: 0.6338 || 20iter: 26.4360 sec.\n",
            "iteration 5120 || Loss: 0.1627 || 20iter: 26.4262 sec.\n",
            "--------------\n",
            "epoch 14 || Epoch_TRAIN_Loss: 0.3310 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 506.7587 sec.\n",
            "--------------\n",
            "Epoch 15/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 5140 || Loss: 0.3191 || 20iter: 20.9496 sec.\n",
            "iteration 5160 || Loss: 0.2279 || 20iter: 26.4735 sec.\n",
            "iteration 5180 || Loss: 0.1696 || 20iter: 26.3936 sec.\n",
            "iteration 5200 || Loss: 0.1772 || 20iter: 26.3979 sec.\n",
            "iteration 5220 || Loss: 0.3246 || 20iter: 26.4643 sec.\n",
            "iteration 5240 || Loss: 0.2927 || 20iter: 26.4953 sec.\n",
            "iteration 5260 || Loss: 0.3343 || 20iter: 26.4606 sec.\n",
            "iteration 5280 || Loss: 0.5076 || 20iter: 26.5457 sec.\n",
            "iteration 5300 || Loss: 0.1746 || 20iter: 26.5793 sec.\n",
            "iteration 5320 || Loss: 0.2781 || 20iter: 26.5320 sec.\n",
            "iteration 5340 || Loss: 0.3043 || 20iter: 26.6219 sec.\n",
            "iteration 5360 || Loss: 0.3655 || 20iter: 26.5888 sec.\n",
            "iteration 5380 || Loss: 0.2302 || 20iter: 26.5806 sec.\n",
            "iteration 5400 || Loss: 0.2597 || 20iter: 26.3860 sec.\n",
            "iteration 5420 || Loss: 0.1817 || 20iter: 26.6159 sec.\n",
            "iteration 5440 || Loss: 0.3813 || 20iter: 26.5967 sec.\n",
            "iteration 5460 || Loss: 0.3767 || 20iter: 26.4768 sec.\n",
            "iteration 5480 || Loss: 0.2014 || 20iter: 26.5605 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 15 || Epoch_TRAIN_Loss: 0.3315 || Epoch_VAL_Loss: 0.3804\n",
            "timer: 665.9651 sec.\n",
            "--------------\n",
            "Epoch 16/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 5500 || Loss: 0.2235 || 20iter: 12.7274 sec.\n",
            "iteration 5520 || Loss: 0.3537 || 20iter: 26.4555 sec.\n",
            "iteration 5540 || Loss: 0.3086 || 20iter: 26.5216 sec.\n",
            "iteration 5560 || Loss: 0.2803 || 20iter: 26.4619 sec.\n",
            "iteration 5580 || Loss: 0.2820 || 20iter: 26.3781 sec.\n",
            "iteration 5600 || Loss: 0.1983 || 20iter: 26.4705 sec.\n",
            "iteration 5620 || Loss: 0.2672 || 20iter: 26.5112 sec.\n",
            "iteration 5640 || Loss: 0.3496 || 20iter: 26.4100 sec.\n",
            "iteration 5660 || Loss: 0.2272 || 20iter: 26.3807 sec.\n",
            "iteration 5680 || Loss: 0.4455 || 20iter: 26.4740 sec.\n",
            "iteration 5700 || Loss: 0.2480 || 20iter: 26.4518 sec.\n",
            "iteration 5720 || Loss: 0.4474 || 20iter: 26.5127 sec.\n",
            "iteration 5740 || Loss: 0.3762 || 20iter: 26.4646 sec.\n",
            "iteration 5760 || Loss: 0.5839 || 20iter: 26.3557 sec.\n",
            "iteration 5780 || Loss: 0.1944 || 20iter: 26.4934 sec.\n",
            "iteration 5800 || Loss: 0.5824 || 20iter: 26.4219 sec.\n",
            "iteration 5820 || Loss: 0.1659 || 20iter: 26.4141 sec.\n",
            "iteration 5840 || Loss: 0.2223 || 20iter: 26.4557 sec.\n",
            "--------------\n",
            "epoch 16 || Epoch_TRAIN_Loss: 0.3254 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 506.6729 sec.\n",
            "--------------\n",
            "Epoch 17/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 5860 || Loss: 0.3817 || 20iter: 4.3207 sec.\n",
            "iteration 5880 || Loss: 0.9418 || 20iter: 26.4520 sec.\n",
            "iteration 5900 || Loss: 0.2799 || 20iter: 26.5663 sec.\n",
            "iteration 5920 || Loss: 0.2697 || 20iter: 26.5083 sec.\n",
            "iteration 5940 || Loss: 0.3884 || 20iter: 26.5047 sec.\n",
            "iteration 5960 || Loss: 0.4867 || 20iter: 26.4527 sec.\n",
            "iteration 5980 || Loss: 0.4669 || 20iter: 26.4455 sec.\n",
            "iteration 6000 || Loss: 0.5302 || 20iter: 26.4452 sec.\n",
            "iteration 6020 || Loss: 0.2153 || 20iter: 26.5336 sec.\n",
            "iteration 6040 || Loss: 0.3779 || 20iter: 26.5678 sec.\n",
            "iteration 6060 || Loss: 0.2340 || 20iter: 26.4519 sec.\n",
            "iteration 6080 || Loss: 0.1996 || 20iter: 26.5232 sec.\n",
            "iteration 6100 || Loss: 0.3212 || 20iter: 26.4852 sec.\n",
            "iteration 6120 || Loss: 0.2668 || 20iter: 26.4732 sec.\n",
            "iteration 6140 || Loss: 0.2424 || 20iter: 26.5942 sec.\n",
            "iteration 6160 || Loss: 0.5777 || 20iter: 26.4783 sec.\n",
            "iteration 6180 || Loss: 0.3196 || 20iter: 26.5688 sec.\n",
            "iteration 6200 || Loss: 0.2451 || 20iter: 26.6127 sec.\n",
            "iteration 6220 || Loss: 0.3702 || 20iter: 26.4662 sec.\n",
            "--------------\n",
            "epoch 17 || Epoch_TRAIN_Loss: 0.3239 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.6209 sec.\n",
            "--------------\n",
            "Epoch 18/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 6240 || Loss: 0.2147 || 20iter: 23.7574 sec.\n",
            "iteration 6260 || Loss: 0.2199 || 20iter: 26.5603 sec.\n",
            "iteration 6280 || Loss: 0.5239 || 20iter: 26.4841 sec.\n",
            "iteration 6300 || Loss: 0.2721 || 20iter: 26.4486 sec.\n",
            "iteration 6320 || Loss: 0.1789 || 20iter: 26.4418 sec.\n",
            "iteration 6340 || Loss: 0.5233 || 20iter: 26.4423 sec.\n",
            "iteration 6360 || Loss: 0.2676 || 20iter: 26.4293 sec.\n",
            "iteration 6380 || Loss: 0.2224 || 20iter: 26.4261 sec.\n",
            "iteration 6400 || Loss: 0.3642 || 20iter: 26.4948 sec.\n",
            "iteration 6420 || Loss: 0.9115 || 20iter: 26.4958 sec.\n",
            "iteration 6440 || Loss: 0.2928 || 20iter: 26.5615 sec.\n",
            "iteration 6460 || Loss: 0.2752 || 20iter: 26.4807 sec.\n",
            "iteration 6480 || Loss: 0.2750 || 20iter: 26.4347 sec.\n",
            "iteration 6500 || Loss: 0.3259 || 20iter: 26.4552 sec.\n",
            "iteration 6520 || Loss: 0.5674 || 20iter: 26.5726 sec.\n",
            "iteration 6540 || Loss: 0.2751 || 20iter: 26.4423 sec.\n",
            "iteration 6560 || Loss: 0.2273 || 20iter: 26.4781 sec.\n",
            "iteration 6580 || Loss: 0.1727 || 20iter: 26.4446 sec.\n",
            "--------------\n",
            "epoch 18 || Epoch_TRAIN_Loss: 0.3263 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.0600 sec.\n",
            "--------------\n",
            "Epoch 19/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 6600 || Loss: 0.4044 || 20iter: 15.5016 sec.\n",
            "iteration 6620 || Loss: 0.3216 || 20iter: 26.5613 sec.\n",
            "iteration 6640 || Loss: 0.2264 || 20iter: 26.5074 sec.\n",
            "iteration 6660 || Loss: 0.2580 || 20iter: 26.4180 sec.\n",
            "iteration 6680 || Loss: 0.3709 || 20iter: 26.5823 sec.\n",
            "iteration 6700 || Loss: 0.2887 || 20iter: 26.4914 sec.\n",
            "iteration 6720 || Loss: 0.4688 || 20iter: 26.4234 sec.\n",
            "iteration 6740 || Loss: 0.1802 || 20iter: 26.5275 sec.\n",
            "iteration 6760 || Loss: 0.2380 || 20iter: 26.5135 sec.\n",
            "iteration 6780 || Loss: 0.5008 || 20iter: 26.5404 sec.\n",
            "iteration 6800 || Loss: 0.2198 || 20iter: 26.4792 sec.\n",
            "iteration 6820 || Loss: 0.3100 || 20iter: 26.4188 sec.\n",
            "iteration 6840 || Loss: 0.2150 || 20iter: 26.4216 sec.\n",
            "iteration 6860 || Loss: 0.1585 || 20iter: 26.4853 sec.\n",
            "iteration 6880 || Loss: 0.3049 || 20iter: 26.4968 sec.\n",
            "iteration 6900 || Loss: 0.4468 || 20iter: 26.5003 sec.\n",
            "iteration 6920 || Loss: 0.1537 || 20iter: 26.4665 sec.\n",
            "iteration 6940 || Loss: 0.1137 || 20iter: 26.4401 sec.\n",
            "--------------\n",
            "epoch 19 || Epoch_TRAIN_Loss: 0.3207 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.3487 sec.\n",
            "--------------\n",
            "Epoch 20/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 6960 || Loss: 0.2772 || 20iter: 7.1721 sec.\n",
            "iteration 6980 || Loss: 0.1813 || 20iter: 26.4106 sec.\n",
            "iteration 7000 || Loss: 0.1920 || 20iter: 26.5605 sec.\n",
            "iteration 7020 || Loss: 0.1480 || 20iter: 26.3880 sec.\n",
            "iteration 7040 || Loss: 0.1817 || 20iter: 26.4921 sec.\n",
            "iteration 7060 || Loss: 0.4051 || 20iter: 26.5843 sec.\n",
            "iteration 7080 || Loss: 0.4194 || 20iter: 26.5945 sec.\n",
            "iteration 7100 || Loss: 0.2928 || 20iter: 26.4886 sec.\n",
            "iteration 7120 || Loss: 0.2249 || 20iter: 26.4170 sec.\n",
            "iteration 7140 || Loss: 0.2487 || 20iter: 26.5059 sec.\n",
            "iteration 7160 || Loss: 0.3145 || 20iter: 26.4516 sec.\n",
            "iteration 7180 || Loss: 0.2888 || 20iter: 26.4788 sec.\n",
            "iteration 7200 || Loss: 0.3422 || 20iter: 26.5723 sec.\n",
            "iteration 7220 || Loss: 0.3918 || 20iter: 26.3723 sec.\n",
            "iteration 7240 || Loss: 0.2720 || 20iter: 26.3987 sec.\n",
            "iteration 7260 || Loss: 0.2928 || 20iter: 26.4745 sec.\n",
            "iteration 7280 || Loss: 0.3307 || 20iter: 26.5014 sec.\n",
            "iteration 7300 || Loss: 0.2278 || 20iter: 26.4488 sec.\n",
            "iteration 7320 || Loss: 0.5830 || 20iter: 26.4599 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 20 || Epoch_TRAIN_Loss: 0.3257 || Epoch_VAL_Loss: 0.3774\n",
            "timer: 665.7564 sec.\n",
            "--------------\n",
            "Epoch 21/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 7340 || Loss: 0.2557 || 20iter: 26.4260 sec.\n",
            "iteration 7360 || Loss: 0.4421 || 20iter: 26.5550 sec.\n",
            "iteration 7380 || Loss: 0.2078 || 20iter: 26.5562 sec.\n",
            "iteration 7400 || Loss: 0.1244 || 20iter: 26.4670 sec.\n",
            "iteration 7420 || Loss: 0.1574 || 20iter: 26.3760 sec.\n",
            "iteration 7440 || Loss: 0.3453 || 20iter: 26.4920 sec.\n",
            "iteration 7460 || Loss: 0.5240 || 20iter: 26.5345 sec.\n",
            "iteration 7480 || Loss: 0.3151 || 20iter: 26.4503 sec.\n",
            "iteration 7500 || Loss: 0.3336 || 20iter: 26.4772 sec.\n",
            "iteration 7520 || Loss: 0.5574 || 20iter: 26.4253 sec.\n",
            "iteration 7540 || Loss: 0.1997 || 20iter: 26.4812 sec.\n",
            "iteration 7560 || Loss: 0.5020 || 20iter: 26.5736 sec.\n",
            "iteration 7580 || Loss: 0.2252 || 20iter: 26.5039 sec.\n",
            "iteration 7600 || Loss: 0.4677 || 20iter: 26.5012 sec.\n",
            "iteration 7620 || Loss: 0.2500 || 20iter: 26.5008 sec.\n",
            "iteration 7640 || Loss: 0.2591 || 20iter: 26.5235 sec.\n",
            "iteration 7660 || Loss: 0.2395 || 20iter: 26.5127 sec.\n",
            "iteration 7680 || Loss: 0.3634 || 20iter: 26.5130 sec.\n",
            "--------------\n",
            "epoch 21 || Epoch_TRAIN_Loss: 0.3176 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.3300 sec.\n",
            "--------------\n",
            "Epoch 22/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 7700 || Loss: 0.5563 || 20iter: 18.2743 sec.\n",
            "iteration 7720 || Loss: 0.3232 || 20iter: 26.5675 sec.\n",
            "iteration 7740 || Loss: 0.2036 || 20iter: 26.4794 sec.\n",
            "iteration 7760 || Loss: 0.1918 || 20iter: 26.5047 sec.\n",
            "iteration 7780 || Loss: 0.1719 || 20iter: 26.5013 sec.\n",
            "iteration 7800 || Loss: 0.4582 || 20iter: 26.5404 sec.\n",
            "iteration 7820 || Loss: 0.3411 || 20iter: 26.5630 sec.\n",
            "iteration 7840 || Loss: 0.1533 || 20iter: 26.6124 sec.\n",
            "iteration 7860 || Loss: 0.2893 || 20iter: 26.5777 sec.\n",
            "iteration 7880 || Loss: 0.2922 || 20iter: 26.5091 sec.\n",
            "iteration 7900 || Loss: 0.4306 || 20iter: 26.4402 sec.\n",
            "iteration 7920 || Loss: 0.2127 || 20iter: 26.4623 sec.\n",
            "iteration 7940 || Loss: 0.4557 || 20iter: 26.5208 sec.\n",
            "iteration 7960 || Loss: 0.2889 || 20iter: 26.5277 sec.\n",
            "iteration 7980 || Loss: 0.6197 || 20iter: 26.5440 sec.\n",
            "iteration 8000 || Loss: 0.2216 || 20iter: 26.4982 sec.\n",
            "iteration 8020 || Loss: 0.2013 || 20iter: 26.5148 sec.\n",
            "iteration 8040 || Loss: 0.3275 || 20iter: 26.4365 sec.\n",
            "--------------\n",
            "epoch 22 || Epoch_TRAIN_Loss: 0.3099 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.8463 sec.\n",
            "--------------\n",
            "Epoch 23/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 8060 || Loss: 0.8843 || 20iter: 9.9112 sec.\n",
            "iteration 8080 || Loss: 0.2621 || 20iter: 26.6093 sec.\n",
            "iteration 8100 || Loss: 0.2213 || 20iter: 26.4749 sec.\n",
            "iteration 8120 || Loss: 0.1730 || 20iter: 26.4833 sec.\n",
            "iteration 8140 || Loss: 0.4361 || 20iter: 26.4627 sec.\n",
            "iteration 8160 || Loss: 0.1614 || 20iter: 26.4857 sec.\n",
            "iteration 8180 || Loss: 0.1409 || 20iter: 26.4452 sec.\n",
            "iteration 8200 || Loss: 0.1531 || 20iter: 26.4696 sec.\n",
            "iteration 8220 || Loss: 0.1671 || 20iter: 26.5094 sec.\n",
            "iteration 8240 || Loss: 0.4113 || 20iter: 26.5262 sec.\n",
            "iteration 8260 || Loss: 0.4710 || 20iter: 26.4743 sec.\n",
            "iteration 8280 || Loss: 0.4760 || 20iter: 26.5370 sec.\n",
            "iteration 8300 || Loss: 0.2418 || 20iter: 26.5702 sec.\n",
            "iteration 8320 || Loss: 0.2372 || 20iter: 26.4546 sec.\n",
            "iteration 8340 || Loss: 0.2807 || 20iter: 26.4451 sec.\n",
            "iteration 8360 || Loss: 0.1378 || 20iter: 26.5067 sec.\n",
            "iteration 8380 || Loss: 0.2847 || 20iter: 26.4258 sec.\n",
            "iteration 8400 || Loss: 0.3036 || 20iter: 26.5025 sec.\n",
            "--------------\n",
            "epoch 23 || Epoch_TRAIN_Loss: 0.3155 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.3603 sec.\n",
            "--------------\n",
            "Epoch 24/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 8420 || Loss: 0.4542 || 20iter: 1.5667 sec.\n",
            "iteration 8440 || Loss: 0.1858 || 20iter: 26.5750 sec.\n",
            "iteration 8460 || Loss: 0.4365 || 20iter: 26.4895 sec.\n",
            "iteration 8480 || Loss: 0.3515 || 20iter: 26.4566 sec.\n",
            "iteration 8500 || Loss: 0.2516 || 20iter: 26.4877 sec.\n",
            "iteration 8520 || Loss: 0.8207 || 20iter: 26.5954 sec.\n",
            "iteration 8540 || Loss: 0.2495 || 20iter: 26.4695 sec.\n",
            "iteration 8560 || Loss: 0.3731 || 20iter: 26.5294 sec.\n",
            "iteration 8580 || Loss: 0.3170 || 20iter: 26.4885 sec.\n",
            "iteration 8600 || Loss: 0.3267 || 20iter: 26.5655 sec.\n",
            "iteration 8620 || Loss: 0.2346 || 20iter: 26.5735 sec.\n",
            "iteration 8640 || Loss: 0.2756 || 20iter: 26.4259 sec.\n",
            "iteration 8660 || Loss: 0.3340 || 20iter: 26.5364 sec.\n",
            "iteration 8680 || Loss: 0.2588 || 20iter: 26.4211 sec.\n",
            "iteration 8700 || Loss: 0.2576 || 20iter: 26.5608 sec.\n",
            "iteration 8720 || Loss: 0.1915 || 20iter: 26.4433 sec.\n",
            "iteration 8740 || Loss: 0.1675 || 20iter: 26.4755 sec.\n",
            "iteration 8760 || Loss: 0.2521 || 20iter: 26.6195 sec.\n",
            "iteration 8780 || Loss: 0.3647 || 20iter: 26.5543 sec.\n",
            "--------------\n",
            "epoch 24 || Epoch_TRAIN_Loss: 0.3210 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.7590 sec.\n",
            "--------------\n",
            "Epoch 25/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 8800 || Loss: 0.8705 || 20iter: 20.9571 sec.\n",
            "iteration 8820 || Loss: 0.2980 || 20iter: 26.5472 sec.\n",
            "iteration 8840 || Loss: 0.2672 || 20iter: 26.5002 sec.\n",
            "iteration 8860 || Loss: 0.1098 || 20iter: 26.4906 sec.\n",
            "iteration 8880 || Loss: 0.3678 || 20iter: 26.4642 sec.\n",
            "iteration 8900 || Loss: 0.1543 || 20iter: 26.5673 sec.\n",
            "iteration 8920 || Loss: 0.5608 || 20iter: 26.5012 sec.\n",
            "iteration 8940 || Loss: 0.1587 || 20iter: 26.5434 sec.\n",
            "iteration 8960 || Loss: 0.2863 || 20iter: 26.5397 sec.\n",
            "iteration 8980 || Loss: 0.2669 || 20iter: 26.4799 sec.\n",
            "iteration 9000 || Loss: 0.1194 || 20iter: 26.4624 sec.\n",
            "iteration 9020 || Loss: 0.4010 || 20iter: 26.4990 sec.\n",
            "iteration 9040 || Loss: 0.2086 || 20iter: 26.5114 sec.\n",
            "iteration 9060 || Loss: 0.2701 || 20iter: 26.4831 sec.\n",
            "iteration 9080 || Loss: 0.1606 || 20iter: 26.4827 sec.\n",
            "iteration 9100 || Loss: 0.2755 || 20iter: 26.4097 sec.\n",
            "iteration 9120 || Loss: 0.2406 || 20iter: 26.4684 sec.\n",
            "iteration 9140 || Loss: 0.4509 || 20iter: 26.5665 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 25 || Epoch_TRAIN_Loss: 0.3120 || Epoch_VAL_Loss: 0.3691\n",
            "timer: 666.1310 sec.\n",
            "--------------\n",
            "Epoch 26/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 9160 || Loss: 0.3254 || 20iter: 12.6660 sec.\n",
            "iteration 9180 || Loss: 0.5093 || 20iter: 26.4906 sec.\n",
            "iteration 9200 || Loss: 0.2806 || 20iter: 26.5990 sec.\n",
            "iteration 9220 || Loss: 0.3651 || 20iter: 26.4314 sec.\n",
            "iteration 9240 || Loss: 0.2696 || 20iter: 26.4244 sec.\n",
            "iteration 9260 || Loss: 0.2256 || 20iter: 26.5163 sec.\n",
            "iteration 9280 || Loss: 0.2337 || 20iter: 26.5120 sec.\n",
            "iteration 9300 || Loss: 0.3988 || 20iter: 26.5195 sec.\n",
            "iteration 9320 || Loss: 0.5274 || 20iter: 26.4667 sec.\n",
            "iteration 9340 || Loss: 0.2644 || 20iter: 26.5121 sec.\n",
            "iteration 9360 || Loss: 0.1883 || 20iter: 26.4308 sec.\n",
            "iteration 9380 || Loss: 0.4053 || 20iter: 26.4941 sec.\n",
            "iteration 9400 || Loss: 0.2113 || 20iter: 26.4601 sec.\n",
            "iteration 9420 || Loss: 0.4086 || 20iter: 26.4795 sec.\n",
            "iteration 9440 || Loss: 0.2783 || 20iter: 26.5057 sec.\n",
            "iteration 9460 || Loss: 0.4720 || 20iter: 26.4466 sec.\n",
            "iteration 9480 || Loss: 0.2513 || 20iter: 26.4387 sec.\n",
            "iteration 9500 || Loss: 0.1965 || 20iter: 26.4893 sec.\n",
            "--------------\n",
            "epoch 26 || Epoch_TRAIN_Loss: 0.3061 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.0776 sec.\n",
            "--------------\n",
            "Epoch 27/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 9520 || Loss: 0.1710 || 20iter: 4.3036 sec.\n",
            "iteration 9540 || Loss: 0.3867 || 20iter: 26.4369 sec.\n",
            "iteration 9560 || Loss: 0.2368 || 20iter: 26.5212 sec.\n",
            "iteration 9580 || Loss: 0.2069 || 20iter: 26.4713 sec.\n",
            "iteration 9600 || Loss: 0.1738 || 20iter: 26.4922 sec.\n",
            "iteration 9620 || Loss: 0.3441 || 20iter: 26.5568 sec.\n",
            "iteration 9640 || Loss: 0.2211 || 20iter: 26.4387 sec.\n",
            "iteration 9660 || Loss: 0.3132 || 20iter: 26.4338 sec.\n",
            "iteration 9680 || Loss: 0.3032 || 20iter: 26.4588 sec.\n",
            "iteration 9700 || Loss: 0.1645 || 20iter: 26.5012 sec.\n",
            "iteration 9720 || Loss: 0.2368 || 20iter: 26.4278 sec.\n",
            "iteration 9740 || Loss: 0.1458 || 20iter: 26.3779 sec.\n",
            "iteration 9760 || Loss: 0.2513 || 20iter: 26.4434 sec.\n",
            "iteration 9780 || Loss: 0.2128 || 20iter: 26.4389 sec.\n",
            "iteration 9800 || Loss: 0.5805 || 20iter: 26.4972 sec.\n",
            "iteration 9820 || Loss: 0.2249 || 20iter: 26.4629 sec.\n",
            "iteration 9840 || Loss: 0.1108 || 20iter: 26.5324 sec.\n",
            "iteration 9860 || Loss: 0.3176 || 20iter: 26.5397 sec.\n",
            "iteration 9880 || Loss: 0.1526 || 20iter: 26.4204 sec.\n",
            "--------------\n",
            "epoch 27 || Epoch_TRAIN_Loss: 0.2989 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 506.9154 sec.\n",
            "--------------\n",
            "Epoch 28/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 9900 || Loss: 0.3830 || 20iter: 23.7230 sec.\n",
            "iteration 9920 || Loss: 0.5778 || 20iter: 26.5959 sec.\n",
            "iteration 9940 || Loss: 0.4097 || 20iter: 26.5398 sec.\n",
            "iteration 9960 || Loss: 0.1699 || 20iter: 26.4584 sec.\n",
            "iteration 9980 || Loss: 0.3463 || 20iter: 26.5095 sec.\n",
            "iteration 10000 || Loss: 0.5630 || 20iter: 26.4486 sec.\n",
            "iteration 10020 || Loss: 0.2552 || 20iter: 26.3957 sec.\n",
            "iteration 10040 || Loss: 0.7132 || 20iter: 26.6041 sec.\n",
            "iteration 10060 || Loss: 0.3061 || 20iter: 26.4633 sec.\n",
            "iteration 10080 || Loss: 0.3014 || 20iter: 26.4337 sec.\n",
            "iteration 10100 || Loss: 0.4645 || 20iter: 26.5775 sec.\n",
            "iteration 10120 || Loss: 0.2677 || 20iter: 26.4648 sec.\n",
            "iteration 10140 || Loss: 0.2751 || 20iter: 26.5139 sec.\n",
            "iteration 10160 || Loss: 0.1991 || 20iter: 26.5300 sec.\n",
            "iteration 10180 || Loss: 0.3661 || 20iter: 26.5379 sec.\n",
            "iteration 10200 || Loss: 0.3749 || 20iter: 26.5380 sec.\n",
            "iteration 10220 || Loss: 0.2616 || 20iter: 26.4601 sec.\n",
            "iteration 10240 || Loss: 0.4554 || 20iter: 26.4179 sec.\n",
            "--------------\n",
            "epoch 28 || Epoch_TRAIN_Loss: 0.3076 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.4097 sec.\n",
            "--------------\n",
            "Epoch 29/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 10260 || Loss: 0.3546 || 20iter: 15.3977 sec.\n",
            "iteration 10280 || Loss: 0.3357 || 20iter: 26.6013 sec.\n",
            "iteration 10300 || Loss: 0.3019 || 20iter: 26.4756 sec.\n",
            "iteration 10320 || Loss: 0.5034 || 20iter: 26.4325 sec.\n",
            "iteration 10340 || Loss: 0.4678 || 20iter: 26.5086 sec.\n",
            "iteration 10360 || Loss: 0.3082 || 20iter: 26.5108 sec.\n",
            "iteration 10380 || Loss: 0.3347 || 20iter: 26.4857 sec.\n",
            "iteration 10400 || Loss: 0.3371 || 20iter: 26.5598 sec.\n",
            "iteration 10420 || Loss: 0.5501 || 20iter: 26.4651 sec.\n",
            "iteration 10440 || Loss: 0.4444 || 20iter: 26.4357 sec.\n",
            "iteration 10460 || Loss: 0.3465 || 20iter: 26.5140 sec.\n",
            "iteration 10480 || Loss: 0.6087 || 20iter: 26.5224 sec.\n",
            "iteration 10500 || Loss: 0.3078 || 20iter: 26.4678 sec.\n",
            "iteration 10520 || Loss: 0.1972 || 20iter: 26.4958 sec.\n",
            "iteration 10540 || Loss: 0.4481 || 20iter: 26.5036 sec.\n",
            "iteration 10560 || Loss: 0.5415 || 20iter: 26.4944 sec.\n",
            "iteration 10580 || Loss: 0.1681 || 20iter: 26.4322 sec.\n",
            "iteration 10600 || Loss: 0.4731 || 20iter: 26.4707 sec.\n",
            "--------------\n",
            "epoch 29 || Epoch_TRAIN_Loss: 0.3050 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 507.3532 sec.\n",
            "--------------\n",
            "Epoch 30/30\n",
            "--------------\n",
            "(train)\n",
            "iteration 10620 || Loss: 0.3296 || 20iter: 7.1006 sec.\n",
            "iteration 10640 || Loss: 0.1737 || 20iter: 26.6601 sec.\n",
            "iteration 10660 || Loss: 0.2054 || 20iter: 26.4780 sec.\n",
            "iteration 10680 || Loss: 0.3663 || 20iter: 26.5490 sec.\n",
            "iteration 10700 || Loss: 0.1966 || 20iter: 26.4518 sec.\n",
            "iteration 10720 || Loss: 0.3897 || 20iter: 26.4350 sec.\n",
            "iteration 10740 || Loss: 0.2157 || 20iter: 26.4676 sec.\n",
            "iteration 10760 || Loss: 0.1474 || 20iter: 26.5464 sec.\n",
            "iteration 10780 || Loss: 0.3084 || 20iter: 26.5354 sec.\n",
            "iteration 10800 || Loss: 0.2004 || 20iter: 26.4591 sec.\n",
            "iteration 10820 || Loss: 0.3561 || 20iter: 26.4550 sec.\n",
            "iteration 10840 || Loss: 0.2371 || 20iter: 26.4132 sec.\n",
            "iteration 10860 || Loss: 0.4529 || 20iter: 26.4974 sec.\n",
            "iteration 10880 || Loss: 0.1700 || 20iter: 26.5072 sec.\n",
            "iteration 10900 || Loss: 0.3182 || 20iter: 26.5742 sec.\n",
            "iteration 10920 || Loss: 0.3632 || 20iter: 26.4832 sec.\n",
            "iteration 10940 || Loss: 0.1909 || 20iter: 26.4906 sec.\n",
            "iteration 10960 || Loss: 0.4004 || 20iter: 26.5991 sec.\n",
            "iteration 10980 || Loss: 0.5355 || 20iter: 26.5132 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 30 || Epoch_TRAIN_Loss: 0.3025 || Epoch_VAL_Loss: 0.3665\n",
            "timer: 666.1629 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqPTK67w6J5J"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}