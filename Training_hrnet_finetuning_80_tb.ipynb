{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_hrnet_finetuning_80_tb.ipynb",
      "provenance": [],
      "mount_file_id": "1CMleg4ZTa1UwjJlfBmrSJT3ggY7JO-_b",
      "authorship_tag": "ABX9TyPpCUqEP8cm+XmYICaZ+ZwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KokiNiimura/study/blob/master/Training_hrnet_finetuning_80_tb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Dtu9s5Lkxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56d19fc-85c1-4fa0-c85d-e5fff26ce8ad"
      },
      "source": [
        "%cd /content/drive/My Drive/study/PyTorch_Advanced/03"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/study/PyTorch_Advanced/03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXKa0gG4MioW"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "import functools\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch._utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFCgKVvvdssn"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_nZJeoZd0rq"
      },
      "source": [
        "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
        "\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath=rootpath)\n",
        "\n",
        "color_mean = (0.485, 0.456, 0.406)\n",
        "color_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", \n",
        "                           transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", \n",
        "                           transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqhUuSBXzWUu"
      },
      "source": [
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "\n",
        "BN_MOMENTUM = 0.01\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out + residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn3 = BatchNorm2d(planes * self.expansion,\n",
        "                               momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out + residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class HighResolutionModule(nn.Module):\n",
        "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
        "                 num_channels, fuse_method, multi_scale_output=True):\n",
        "        super(HighResolutionModule, self).__init__()\n",
        "        self._check_branches(\n",
        "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
        "\n",
        "        self.num_inchannels = num_inchannels\n",
        "        self.fuse_method = fuse_method\n",
        "        self.num_branches = num_branches\n",
        "\n",
        "        self.multi_scale_output = multi_scale_output\n",
        "\n",
        "        self.branches = self._make_branches(\n",
        "            num_branches, blocks, num_blocks, num_channels)\n",
        "        self.fuse_layers = self._make_fuse_layers()\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def _check_branches(self, num_branches, blocks, num_blocks,\n",
        "                        num_inchannels, num_channels):\n",
        "        if num_branches != len(num_blocks):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n",
        "                num_branches, len(num_blocks))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        if num_branches != len(num_channels):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n",
        "                num_branches, len(num_channels))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        if num_branches != len(num_inchannels):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n",
        "                num_branches, len(num_inchannels))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
        "                         stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or \\\n",
        "           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.num_inchannels[branch_index],\n",
        "                          num_channels[branch_index] * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(num_channels[branch_index] * block.expansion,\n",
        "                            momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.num_inchannels[branch_index],\n",
        "                            num_channels[branch_index], stride, downsample))\n",
        "        self.num_inchannels[branch_index] = \\\n",
        "            num_channels[branch_index] * block.expansion\n",
        "        for i in range(1, num_blocks[branch_index]):\n",
        "            layers.append(block(self.num_inchannels[branch_index],\n",
        "                                num_channels[branch_index]))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
        "        branches = []\n",
        "\n",
        "        for i in range(num_branches):\n",
        "            branches.append(\n",
        "                self._make_one_branch(i, block, num_blocks, num_channels))\n",
        "\n",
        "        return nn.ModuleList(branches)\n",
        "\n",
        "    def _make_fuse_layers(self):\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "\n",
        "        num_branches = self.num_branches\n",
        "        num_inchannels = self.num_inchannels\n",
        "        fuse_layers = []\n",
        "        for i in range(num_branches if self.multi_scale_output else 1):\n",
        "            fuse_layer = []\n",
        "            for j in range(num_branches):\n",
        "                if j > i:\n",
        "                    fuse_layer.append(nn.Sequential(\n",
        "                        nn.Conv2d(num_inchannels[j],\n",
        "                                  num_inchannels[i],\n",
        "                                  1,\n",
        "                                  1,\n",
        "                                  0,\n",
        "                                  bias=False),\n",
        "                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n",
        "                elif j == i:\n",
        "                    fuse_layer.append(None)\n",
        "                else:\n",
        "                    conv3x3s = []\n",
        "                    for k in range(i-j):\n",
        "                        if k == i - j - 1:\n",
        "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
        "                            conv3x3s.append(nn.Sequential(\n",
        "                                nn.Conv2d(num_inchannels[j],\n",
        "                                          num_outchannels_conv3x3,\n",
        "                                          3, 2, 1, bias=False),\n",
        "                                BatchNorm2d(num_outchannels_conv3x3, \n",
        "                                            momentum=BN_MOMENTUM)))\n",
        "                        else:\n",
        "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
        "                            conv3x3s.append(nn.Sequential(\n",
        "                                nn.Conv2d(num_inchannels[j],\n",
        "                                          num_outchannels_conv3x3,\n",
        "                                          3, 2, 1, bias=False),\n",
        "                                BatchNorm2d(num_outchannels_conv3x3,\n",
        "                                            momentum=BN_MOMENTUM),\n",
        "                                nn.ReLU(inplace=False)))\n",
        "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
        "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
        "\n",
        "        return nn.ModuleList(fuse_layers)\n",
        "\n",
        "    def get_num_inchannels(self):\n",
        "        return self.num_inchannels\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.num_branches == 1:\n",
        "            return [self.branches[0](x[0])]\n",
        "\n",
        "        for i in range(self.num_branches):\n",
        "            x[i] = self.branches[i](x[i])\n",
        "\n",
        "        x_fuse = []\n",
        "        for i in range(len(self.fuse_layers)):\n",
        "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
        "            for j in range(1, self.num_branches):\n",
        "                if i == j:\n",
        "                    y = y + x[j]\n",
        "                elif j > i:\n",
        "                    width_output = x[i].shape[-1]\n",
        "                    height_output = x[i].shape[-2]\n",
        "                    y = y + F.interpolate(\n",
        "                        self.fuse_layers[i][j](x[j]),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear')\n",
        "                else:\n",
        "                    y = y + self.fuse_layers[i][j](x[j])\n",
        "            x_fuse.append(self.relu(y))\n",
        "\n",
        "        return x_fuse\n",
        "\n",
        "\n",
        "blocks_dict = {\n",
        "    'BASIC': BasicBlock,\n",
        "    'BOTTLENECK': Bottleneck\n",
        "}\n",
        "\n",
        "\n",
        "class HighResolutionNet(nn.Module):\n",
        "\n",
        "    def __init__(self, config, **kwargs):\n",
        "        #extra = config.MODEL.EXTRA\n",
        "        extra = config['MODEL']['EXTRA']\n",
        "        super(HighResolutionNet, self).__init__()\n",
        "\n",
        "        # stem net\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "        self.stage1_cfg = extra['STAGE1']\n",
        "        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n",
        "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
        "        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n",
        "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
        "        stage1_out_channel = block.expansion*num_channels\n",
        "\n",
        "        self.stage2_cfg = extra['STAGE2']\n",
        "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition1 = self._make_transition_layer(\n",
        "            [stage1_out_channel], num_channels)\n",
        "        self.stage2, pre_stage_channels = self._make_stage(\n",
        "            self.stage2_cfg, num_channels)\n",
        "\n",
        "        self.stage3_cfg = extra['STAGE3']\n",
        "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition2 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage3, pre_stage_channels = self._make_stage(\n",
        "            self.stage3_cfg, num_channels)\n",
        "\n",
        "        self.stage4_cfg = extra['STAGE4']\n",
        "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition3 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage4, pre_stage_channels = self._make_stage(\n",
        "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
        "        \n",
        "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
        "\n",
        "        self.last_layer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=last_inp_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0),\n",
        "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=config['NUM_CLASSES'],\n",
        "                kernel_size=extra['FINAL_CONV_KERNEL'],\n",
        "                stride=1,\n",
        "                padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0)\n",
        "        )\n",
        "\n",
        "    def _make_transition_layer(\n",
        "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
        "        num_branches_cur = len(num_channels_cur_layer)\n",
        "        num_branches_pre = len(num_channels_pre_layer)\n",
        "\n",
        "        transition_layers = []\n",
        "        for i in range(num_branches_cur):\n",
        "            if i < num_branches_pre:\n",
        "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
        "                    transition_layers.append(nn.Sequential(\n",
        "                        nn.Conv2d(num_channels_pre_layer[i],\n",
        "                                  num_channels_cur_layer[i],\n",
        "                                  3,\n",
        "                                  1,\n",
        "                                  1,\n",
        "                                  bias=False),\n",
        "                        BatchNorm2d(\n",
        "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
        "                        nn.ReLU(inplace=False)))\n",
        "                else:\n",
        "                    transition_layers.append(None)\n",
        "            else:\n",
        "                conv3x3s = []\n",
        "                for j in range(i+1-num_branches_pre):\n",
        "                    inchannels = num_channels_pre_layer[-1]\n",
        "                    outchannels = num_channels_cur_layer[i] \\\n",
        "                        if j == i-num_branches_pre else inchannels\n",
        "                    conv3x3s.append(nn.Sequential(\n",
        "                        nn.Conv2d(\n",
        "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
        "                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
        "                        nn.ReLU(inplace=False)))\n",
        "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
        "\n",
        "        return nn.ModuleList(transition_layers)\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_stage(self, layer_config, num_inchannels,\n",
        "                    multi_scale_output=True):\n",
        "        num_modules = layer_config['NUM_MODULES']\n",
        "        num_branches = layer_config['NUM_BRANCHES']\n",
        "        num_blocks = layer_config['NUM_BLOCKS']\n",
        "        num_channels = layer_config['NUM_CHANNELS']\n",
        "        block = blocks_dict[layer_config['BLOCK']]\n",
        "        fuse_method = layer_config['FUSE_METHOD']\n",
        "\n",
        "        modules = []\n",
        "        for i in range(num_modules):\n",
        "            # multi_scale_output is only used last module\n",
        "            if not multi_scale_output and i == num_modules - 1:\n",
        "                reset_multi_scale_output = False\n",
        "            else:\n",
        "                reset_multi_scale_output = True\n",
        "            modules.append(\n",
        "                HighResolutionModule(num_branches,\n",
        "                                      block,\n",
        "                                      num_blocks,\n",
        "                                      num_inchannels,\n",
        "                                      num_channels,\n",
        "                                      fuse_method,\n",
        "                                      reset_multi_scale_output)\n",
        "            )\n",
        "            num_inchannels = modules[-1].get_num_inchannels()\n",
        "\n",
        "        return nn.Sequential(*modules), num_inchannels\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        #print(x.shape)\n",
        "        x = self.layer1(x)\n",
        "        #print(x.shape)\n",
        "        x_list = []\n",
        "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
        "            if self.transition1[i] is not None:\n",
        "                x_list.append(self.transition1[i](x))\n",
        "            else:\n",
        "                x_list.append(x)\n",
        "        y_list = self.stage2(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
        "            if self.transition2[i] is not None:\n",
        "                if i < self.stage2_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition2[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition2[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        y_list = self.stage3(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
        "            if self.transition3[i] is not None:\n",
        "                if i < self.stage3_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition3[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition3[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        x = self.stage4(x_list)\n",
        "        #print(x.shape)\n",
        "        # Upsampling\n",
        "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
        "        #x0_h, x0_w = x0_h* 4, x0_w * 2 \n",
        "        #print(x0_h, x0_w)\n",
        "        x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')\n",
        "        x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')\n",
        "        x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')\n",
        "\n",
        "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
        "\n",
        "        x = self.last_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def init_weights(self, pretrained='',):\n",
        "        logger.info('=> init weights from normal distribution')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.001)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        if pretrained:\n",
        "            pretrained_dict = torch.load(pretrained)\n",
        "            logger.info('=> loading pretrained model {}'.format(pretrained))\n",
        "            model_dict = self.state_dict()\n",
        "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
        "                               if k in model_dict.keys()}\n",
        "            for k, _ in pretrained_dict.items():\n",
        "                logger.info(\n",
        "                    '=> loading {} pretrained model {}'.format(k, pretrained))\n",
        "            model_dict.update(pretrained_dict)\n",
        "            self.load_state_dict(model_dict)\n",
        "\n",
        "def get_seg_model(cfg, **kwargs):\n",
        "    model = HighResolutionNet(cfg, **kwargs)\n",
        "    model.init_weights(cfg['PRETRAINED'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1EGoRi10oMx"
      },
      "source": [
        "# dict for model configuration\n",
        "config = {}\n",
        "\n",
        "config['NUM_CLASSES'] = 21\n",
        "config['PRETRAINED'] = \"./hrnet_w18_small_model_v1.pth\"\n",
        "\n",
        "config['MODEL'] = {'EXTRA': {'FINAL_CONV_KERNEL': 1,\n",
        "      'STAGE1': {'BLOCK': 'BOTTLENECK', \n",
        "                 'FUSE_METHOD': 'SUM',\n",
        "                 'NUM_BLOCKS': [1],\n",
        "                 'NUM_CHANNELS': [32],\n",
        "                 'NUM_MODULES': 1,\n",
        "                 'NUM_RANCHES': 1\n",
        "                },\n",
        "      'STAGE2': {'BLOCK': 'BASIC',\n",
        "                 'FUSE_METHOD': 'SUM',\n",
        "                 'NUM_BLOCKS': [2, 2],\n",
        "                 'NUM_BRANCHES': 2,\n",
        "                 'NUM_CHANNELS': [16, 32],\n",
        "                 'NUM_MODULES': 1\n",
        "                },\n",
        "      'STAGE3':{'BLOCK': 'BASIC',\n",
        "                'FUSE_METHOD': 'SUM',\n",
        "                'NUM_BLOCKS': [2, 2, 2],\n",
        "                'NUM_BRANCHES': 3,\n",
        "                'NUM_CHANNELS': [16, 32, 64],\n",
        "                'NUM_MODULES': 1\n",
        "               },\n",
        "       'STAGE4': {'BLOCK': 'BASIC',\n",
        "                 'FUSE_METHOD': 'SUM',\n",
        "                 'NUM_BLOCKS': [2, 2, 2, 2],\n",
        "                 'NUM_BRANCHES': 4,\n",
        "                 'NUM_CHANNELS': [16, 32, 64, 128],\n",
        "                 'NUM_MODULES': 1\n",
        "                 }}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlMNdp2A012h"
      },
      "source": [
        "net = get_seg_model(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgDZepO_p2sW"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EXrJsEYt1ND"
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "def lambda_epoch(epoch):\n",
        "    max_epoch = 80\n",
        "    return math.pow((1-epoch/max_epoch), 0.9)\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6bGcczaA47x",
        "outputId": "477b7dab-da81-4071-b81e-60d3e85f79cf"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33dHkXVsA0oM"
      },
      "source": [
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GguQkl7qFT6"
      },
      "source": [
        "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device: {}\".format(device))\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
        "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    iteration = 1\n",
        "    logs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_val_loss = 0.0\n",
        "\n",
        "        print('--------------')\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('--------------')\n",
        "        \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                print('(train)')\n",
        "\n",
        "            else:\n",
        "                if ((epoch+1) % 5 == 0):\n",
        "                    net.eval()\n",
        "                    print('--------------')\n",
        "                    print('(val)')\n",
        "                else:\n",
        "                    continue\n",
        "        \n",
        "            count = 0\n",
        "            for images, anno_class_images in dataloaders_dict[phase]:\n",
        "                if images.size()[0] == 1:\n",
        "                    continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                anno_class_images = anno_class_images.to(device)\n",
        "\n",
        "                if (phase == 'train'):\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(images)\n",
        "                    loss = criterion(outputs, anno_class_images.long())\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "\n",
        "                        if (iteration % 20 == 0):\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('iteration {} || Loss: {:.4f} || 20iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item()/batch_size, duration))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                        epoch_train_loss += loss.item()\n",
        "                        iteration += 1\n",
        "\n",
        "                    else:\n",
        "                        epoch_val_loss += loss.item()\n",
        "\n",
        "        t_epoch_finish = time.time()\n",
        "        print('--------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss: {:.4f} || Epoch_VAL_Loss: {:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
        "        print('timer: {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        writer.add_scalars('data/scalars', {'train': epoch_train_loss/num_train_imgs}, epoch+1)\n",
        "        if ((epoch+1) % 5 == 0):\n",
        "            writer.add_scalars('data/scalars', {'val': epoch_val_loss/num_val_imgs}, epoch+1)\n",
        "\n",
        "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss/num_train_imgs, \n",
        "                        'val_loss': epoch_val_loss/num_val_imgs}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(\"log_output_hr_80.csv\")\n",
        "\n",
        "        torch.save(net.state_dict(), 'weights/hrnet_80_' + str(epoch+1) + '.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRaChXgJrefI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73aff4d9-898c-49e9-8dd6-c6c296ed43ad"
      },
      "source": [
        "num_epochs = 80\n",
        "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda:0\n",
            "--------------\n",
            "Epoch 1/80\n",
            "--------------\n",
            "(train)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 20 || Loss: 0.2555 || 20iter: 155.3476 sec.\n",
            "iteration 40 || Loss: 0.6177 || 20iter: 153.4184 sec.\n",
            "iteration 60 || Loss: 0.4534 || 20iter: 149.0691 sec.\n",
            "iteration 80 || Loss: 0.1569 || 20iter: 146.4030 sec.\n",
            "iteration 100 || Loss: 0.2145 || 20iter: 147.3079 sec.\n",
            "iteration 120 || Loss: 0.4646 || 20iter: 146.7228 sec.\n",
            "iteration 140 || Loss: 0.1929 || 20iter: 147.3936 sec.\n",
            "iteration 160 || Loss: 0.2496 || 20iter: 146.2830 sec.\n",
            "iteration 180 || Loss: 0.2644 || 20iter: 150.9505 sec.\n",
            "iteration 200 || Loss: 0.4125 || 20iter: 142.8459 sec.\n",
            "iteration 220 || Loss: 0.3321 || 20iter: 142.0786 sec.\n",
            "iteration 240 || Loss: 0.1860 || 20iter: 144.5097 sec.\n",
            "iteration 260 || Loss: 0.2572 || 20iter: 145.2233 sec.\n",
            "iteration 280 || Loss: 0.3356 || 20iter: 148.0152 sec.\n",
            "iteration 300 || Loss: 0.3668 || 20iter: 148.0265 sec.\n",
            "iteration 320 || Loss: 0.3315 || 20iter: 148.9296 sec.\n",
            "iteration 340 || Loss: 0.3172 || 20iter: 149.4191 sec.\n",
            "iteration 360 || Loss: 0.2093 || 20iter: 147.0783 sec.\n",
            "--------------\n",
            "epoch 1 || Epoch_TRAIN_Loss: 0.2799 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 2710.6863 sec.\n",
            "--------------\n",
            "Epoch 2/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 380 || Loss: 0.2376 || 20iter: 10.3915 sec.\n",
            "iteration 400 || Loss: 0.2798 || 20iter: 15.0710 sec.\n",
            "iteration 420 || Loss: 0.3963 || 20iter: 15.0111 sec.\n",
            "iteration 440 || Loss: 0.1937 || 20iter: 15.0348 sec.\n",
            "iteration 460 || Loss: 0.1345 || 20iter: 14.9809 sec.\n",
            "iteration 480 || Loss: 0.0841 || 20iter: 15.0170 sec.\n",
            "iteration 500 || Loss: 0.2405 || 20iter: 14.9996 sec.\n",
            "iteration 520 || Loss: 0.2782 || 20iter: 15.0302 sec.\n",
            "iteration 540 || Loss: 0.2605 || 20iter: 15.0076 sec.\n",
            "iteration 560 || Loss: 0.2936 || 20iter: 15.0637 sec.\n",
            "iteration 580 || Loss: 0.3278 || 20iter: 14.9954 sec.\n",
            "iteration 600 || Loss: 0.1231 || 20iter: 15.0117 sec.\n",
            "iteration 620 || Loss: 0.2391 || 20iter: 15.0265 sec.\n",
            "iteration 640 || Loss: 0.2954 || 20iter: 15.0389 sec.\n",
            "iteration 660 || Loss: 0.1737 || 20iter: 15.0730 sec.\n",
            "iteration 680 || Loss: 0.2359 || 20iter: 15.0542 sec.\n",
            "iteration 700 || Loss: 0.1015 || 20iter: 15.0626 sec.\n",
            "iteration 720 || Loss: 0.3452 || 20iter: 14.9904 sec.\n",
            "--------------\n",
            "epoch 2 || Epoch_TRAIN_Loss: 0.2415 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.5769 sec.\n",
            "--------------\n",
            "Epoch 3/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 740 || Loss: 0.2077 || 20iter: 5.7211 sec.\n",
            "iteration 760 || Loss: 0.1456 || 20iter: 15.0656 sec.\n",
            "iteration 780 || Loss: 0.1356 || 20iter: 15.1180 sec.\n",
            "iteration 800 || Loss: 0.2851 || 20iter: 14.9812 sec.\n",
            "iteration 820 || Loss: 0.3066 || 20iter: 14.9813 sec.\n",
            "iteration 840 || Loss: 0.3242 || 20iter: 15.0073 sec.\n",
            "iteration 860 || Loss: 0.2439 || 20iter: 15.0596 sec.\n",
            "iteration 880 || Loss: 0.2568 || 20iter: 14.9952 sec.\n",
            "iteration 900 || Loss: 0.3948 || 20iter: 15.0275 sec.\n",
            "iteration 920 || Loss: 0.1345 || 20iter: 14.9819 sec.\n",
            "iteration 940 || Loss: 0.2985 || 20iter: 14.9812 sec.\n",
            "iteration 960 || Loss: 0.0960 || 20iter: 15.0353 sec.\n",
            "iteration 980 || Loss: 0.1312 || 20iter: 15.0317 sec.\n",
            "iteration 1000 || Loss: 0.2520 || 20iter: 15.0258 sec.\n",
            "iteration 1020 || Loss: 0.1515 || 20iter: 15.0247 sec.\n",
            "iteration 1040 || Loss: 0.4095 || 20iter: 15.0531 sec.\n",
            "iteration 1060 || Loss: 0.2214 || 20iter: 15.0566 sec.\n",
            "iteration 1080 || Loss: 0.1823 || 20iter: 15.1031 sec.\n",
            "--------------\n",
            "epoch 3 || Epoch_TRAIN_Loss: 0.2258 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.6013 sec.\n",
            "--------------\n",
            "Epoch 4/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 1100 || Loss: 0.1664 || 20iter: 1.0646 sec.\n",
            "iteration 1120 || Loss: 0.3204 || 20iter: 15.0610 sec.\n",
            "iteration 1140 || Loss: 0.2222 || 20iter: 15.0464 sec.\n",
            "iteration 1160 || Loss: 0.1903 || 20iter: 15.0595 sec.\n",
            "iteration 1180 || Loss: 0.2439 || 20iter: 15.0454 sec.\n",
            "iteration 1200 || Loss: 0.1655 || 20iter: 15.0426 sec.\n",
            "iteration 1220 || Loss: 0.3537 || 20iter: 14.9952 sec.\n",
            "iteration 1240 || Loss: 0.1772 || 20iter: 15.0340 sec.\n",
            "iteration 1260 || Loss: 0.2231 || 20iter: 15.0798 sec.\n",
            "iteration 1280 || Loss: 0.3113 || 20iter: 15.0508 sec.\n",
            "iteration 1300 || Loss: 0.1748 || 20iter: 15.0009 sec.\n",
            "iteration 1320 || Loss: 0.1486 || 20iter: 14.9944 sec.\n",
            "iteration 1340 || Loss: 0.1565 || 20iter: 14.9849 sec.\n",
            "iteration 1360 || Loss: 0.3864 || 20iter: 14.9781 sec.\n",
            "iteration 1380 || Loss: 0.2356 || 20iter: 15.0101 sec.\n",
            "iteration 1400 || Loss: 0.1843 || 20iter: 14.9760 sec.\n",
            "iteration 1420 || Loss: 0.1660 || 20iter: 15.0660 sec.\n",
            "iteration 1440 || Loss: 0.1511 || 20iter: 14.9756 sec.\n",
            "iteration 1460 || Loss: 0.2596 || 20iter: 15.0499 sec.\n",
            "--------------\n",
            "epoch 4 || Epoch_TRAIN_Loss: 0.2189 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.5153 sec.\n",
            "--------------\n",
            "Epoch 5/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 1480 || Loss: 0.3631 || 20iter: 11.8855 sec.\n",
            "iteration 1500 || Loss: 0.1668 || 20iter: 15.0161 sec.\n",
            "iteration 1520 || Loss: 0.2185 || 20iter: 14.9353 sec.\n",
            "iteration 1540 || Loss: 0.1374 || 20iter: 15.0357 sec.\n",
            "iteration 1560 || Loss: 0.1032 || 20iter: 14.9875 sec.\n",
            "iteration 1580 || Loss: 0.1793 || 20iter: 15.0293 sec.\n",
            "iteration 1600 || Loss: 0.2920 || 20iter: 15.0226 sec.\n",
            "iteration 1620 || Loss: 0.1556 || 20iter: 15.0139 sec.\n",
            "iteration 1640 || Loss: 0.1631 || 20iter: 14.9915 sec.\n",
            "iteration 1660 || Loss: 0.1268 || 20iter: 15.0309 sec.\n",
            "iteration 1680 || Loss: 0.2159 || 20iter: 15.0222 sec.\n",
            "iteration 1700 || Loss: 0.2755 || 20iter: 14.9967 sec.\n",
            "iteration 1720 || Loss: 0.3263 || 20iter: 14.9412 sec.\n",
            "iteration 1740 || Loss: 0.2425 || 20iter: 15.0181 sec.\n",
            "iteration 1760 || Loss: 0.1602 || 20iter: 14.9922 sec.\n",
            "iteration 1780 || Loss: 0.1390 || 20iter: 15.0155 sec.\n",
            "iteration 1800 || Loss: 0.0676 || 20iter: 15.0445 sec.\n",
            "iteration 1820 || Loss: 0.1388 || 20iter: 15.0664 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 5 || Epoch_TRAIN_Loss: 0.2107 || Epoch_VAL_Loss: 0.2529\n",
            "timer: 2734.4377 sec.\n",
            "--------------\n",
            "Epoch 6/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 1840 || Loss: 0.2974 || 20iter: 7.2721 sec.\n",
            "iteration 1860 || Loss: 0.1581 || 20iter: 15.0588 sec.\n",
            "iteration 1880 || Loss: 0.2352 || 20iter: 14.9971 sec.\n",
            "iteration 1900 || Loss: 0.1629 || 20iter: 15.0890 sec.\n",
            "iteration 1920 || Loss: 0.1870 || 20iter: 15.0052 sec.\n",
            "iteration 1940 || Loss: 0.1312 || 20iter: 15.0054 sec.\n",
            "iteration 1960 || Loss: 0.1947 || 20iter: 15.0614 sec.\n",
            "iteration 1980 || Loss: 0.1185 || 20iter: 15.0463 sec.\n",
            "iteration 2000 || Loss: 0.1453 || 20iter: 14.9900 sec.\n",
            "iteration 2020 || Loss: 0.1998 || 20iter: 15.1296 sec.\n",
            "iteration 2040 || Loss: 0.1555 || 20iter: 15.0100 sec.\n",
            "iteration 2060 || Loss: 0.1245 || 20iter: 15.1283 sec.\n",
            "iteration 2080 || Loss: 0.0941 || 20iter: 15.0485 sec.\n",
            "iteration 2100 || Loss: 0.0670 || 20iter: 15.0546 sec.\n",
            "iteration 2120 || Loss: 0.3673 || 20iter: 15.0051 sec.\n",
            "iteration 2140 || Loss: 0.3779 || 20iter: 15.0175 sec.\n",
            "iteration 2160 || Loss: 0.1941 || 20iter: 15.0473 sec.\n",
            "iteration 2180 || Loss: 0.5085 || 20iter: 15.0375 sec.\n",
            "--------------\n",
            "epoch 6 || Epoch_TRAIN_Loss: 0.2081 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.8058 sec.\n",
            "--------------\n",
            "Epoch 7/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 2200 || Loss: 0.1287 || 20iter: 2.6005 sec.\n",
            "iteration 2220 || Loss: 0.1526 || 20iter: 15.0269 sec.\n",
            "iteration 2240 || Loss: 0.3142 || 20iter: 15.0340 sec.\n",
            "iteration 2260 || Loss: 0.3896 || 20iter: 15.0171 sec.\n",
            "iteration 2280 || Loss: 0.2639 || 20iter: 15.0429 sec.\n",
            "iteration 2300 || Loss: 0.1129 || 20iter: 15.0138 sec.\n",
            "iteration 2320 || Loss: 0.1611 || 20iter: 15.0106 sec.\n",
            "iteration 2340 || Loss: 0.2643 || 20iter: 15.0417 sec.\n",
            "iteration 2360 || Loss: 0.1442 || 20iter: 15.0861 sec.\n",
            "iteration 2380 || Loss: 0.1298 || 20iter: 15.0204 sec.\n",
            "iteration 2400 || Loss: 0.3935 || 20iter: 15.0325 sec.\n",
            "iteration 2420 || Loss: 0.2929 || 20iter: 15.1374 sec.\n",
            "iteration 2440 || Loss: 0.1578 || 20iter: 15.0813 sec.\n",
            "iteration 2460 || Loss: 0.1939 || 20iter: 15.0492 sec.\n",
            "iteration 2480 || Loss: 0.2164 || 20iter: 15.0577 sec.\n",
            "iteration 2500 || Loss: 0.1369 || 20iter: 15.0712 sec.\n",
            "iteration 2520 || Loss: 0.5087 || 20iter: 15.0909 sec.\n",
            "iteration 2540 || Loss: 0.2041 || 20iter: 15.0504 sec.\n",
            "iteration 2560 || Loss: 0.2447 || 20iter: 15.0433 sec.\n",
            "--------------\n",
            "epoch 7 || Epoch_TRAIN_Loss: 0.2074 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.9668 sec.\n",
            "--------------\n",
            "Epoch 8/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 2580 || Loss: 0.0894 || 20iter: 13.4996 sec.\n",
            "iteration 2600 || Loss: 0.2684 || 20iter: 15.0717 sec.\n",
            "iteration 2620 || Loss: 0.1086 || 20iter: 14.9884 sec.\n",
            "iteration 2640 || Loss: 0.1695 || 20iter: 15.0213 sec.\n",
            "iteration 2660 || Loss: 0.1393 || 20iter: 15.0839 sec.\n",
            "iteration 2680 || Loss: 0.1245 || 20iter: 15.1037 sec.\n",
            "iteration 2700 || Loss: 0.1357 || 20iter: 14.9745 sec.\n",
            "iteration 2720 || Loss: 0.2836 || 20iter: 15.0144 sec.\n",
            "iteration 2740 || Loss: 0.2546 || 20iter: 14.9911 sec.\n",
            "iteration 2760 || Loss: 0.2980 || 20iter: 15.0519 sec.\n",
            "iteration 2780 || Loss: 0.1720 || 20iter: 15.0408 sec.\n",
            "iteration 2800 || Loss: 0.3648 || 20iter: 15.0281 sec.\n",
            "iteration 2820 || Loss: 0.1205 || 20iter: 15.1311 sec.\n",
            "iteration 2840 || Loss: 0.1553 || 20iter: 15.0231 sec.\n",
            "iteration 2860 || Loss: 0.2590 || 20iter: 15.0516 sec.\n",
            "iteration 2880 || Loss: 0.2373 || 20iter: 15.0496 sec.\n",
            "iteration 2900 || Loss: 0.1467 || 20iter: 15.1216 sec.\n",
            "iteration 2920 || Loss: 0.3010 || 20iter: 15.0529 sec.\n",
            "--------------\n",
            "epoch 8 || Epoch_TRAIN_Loss: 0.2044 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.9277 sec.\n",
            "--------------\n",
            "Epoch 9/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 2940 || Loss: 0.1493 || 20iter: 8.8452 sec.\n",
            "iteration 2960 || Loss: 0.2273 || 20iter: 14.9659 sec.\n",
            "iteration 2980 || Loss: 0.0590 || 20iter: 15.1005 sec.\n",
            "iteration 3000 || Loss: 0.2734 || 20iter: 15.0826 sec.\n",
            "iteration 3020 || Loss: 0.2181 || 20iter: 15.0049 sec.\n",
            "iteration 3040 || Loss: 0.1605 || 20iter: 15.0333 sec.\n",
            "iteration 3060 || Loss: 0.3117 || 20iter: 15.0103 sec.\n",
            "iteration 3080 || Loss: 0.1878 || 20iter: 15.0111 sec.\n",
            "iteration 3100 || Loss: 0.1241 || 20iter: 15.0446 sec.\n",
            "iteration 3120 || Loss: 0.1836 || 20iter: 15.0659 sec.\n",
            "iteration 3140 || Loss: 0.2694 || 20iter: 15.0179 sec.\n",
            "iteration 3160 || Loss: 0.3610 || 20iter: 14.9989 sec.\n",
            "iteration 3180 || Loss: 0.1980 || 20iter: 15.0687 sec.\n",
            "iteration 3200 || Loss: 0.2112 || 20iter: 15.0504 sec.\n",
            "iteration 3220 || Loss: 0.1899 || 20iter: 15.1031 sec.\n",
            "iteration 3240 || Loss: 0.2391 || 20iter: 15.0274 sec.\n",
            "iteration 3260 || Loss: 0.1603 || 20iter: 15.0251 sec.\n",
            "iteration 3280 || Loss: 0.1287 || 20iter: 14.9951 sec.\n",
            "--------------\n",
            "epoch 9 || Epoch_TRAIN_Loss: 0.1961 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.7055 sec.\n",
            "--------------\n",
            "Epoch 10/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 3300 || Loss: 0.3462 || 20iter: 4.1828 sec.\n",
            "iteration 3320 || Loss: 0.0795 || 20iter: 15.0606 sec.\n",
            "iteration 3340 || Loss: 0.3871 || 20iter: 14.9693 sec.\n",
            "iteration 3360 || Loss: 0.2862 || 20iter: 15.0659 sec.\n",
            "iteration 3380 || Loss: 0.2044 || 20iter: 15.0535 sec.\n",
            "iteration 3400 || Loss: 0.1386 || 20iter: 15.0518 sec.\n",
            "iteration 3420 || Loss: 0.3351 || 20iter: 15.1030 sec.\n",
            "iteration 3440 || Loss: 0.1398 || 20iter: 15.0393 sec.\n",
            "iteration 3460 || Loss: 0.4158 || 20iter: 15.0607 sec.\n",
            "iteration 3480 || Loss: 0.1320 || 20iter: 15.0038 sec.\n",
            "iteration 3500 || Loss: 0.2629 || 20iter: 15.0405 sec.\n",
            "iteration 3520 || Loss: 0.1185 || 20iter: 15.0184 sec.\n",
            "iteration 3540 || Loss: 0.1640 || 20iter: 15.0210 sec.\n",
            "iteration 3560 || Loss: 0.1608 || 20iter: 15.0266 sec.\n",
            "iteration 3580 || Loss: 0.1932 || 20iter: 15.0099 sec.\n",
            "iteration 3600 || Loss: 0.0953 || 20iter: 15.0648 sec.\n",
            "iteration 3620 || Loss: 0.2554 || 20iter: 15.1107 sec.\n",
            "iteration 3640 || Loss: 0.1196 || 20iter: 14.9924 sec.\n",
            "iteration 3660 || Loss: 0.2056 || 20iter: 15.0083 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 10 || Epoch_TRAIN_Loss: 0.1906 || Epoch_VAL_Loss: 0.2195\n",
            "timer: 360.2615 sec.\n",
            "--------------\n",
            "Epoch 11/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 3680 || Loss: 0.1954 || 20iter: 15.0398 sec.\n",
            "iteration 3700 || Loss: 0.2699 || 20iter: 15.0511 sec.\n",
            "iteration 3720 || Loss: 0.1421 || 20iter: 15.0780 sec.\n",
            "iteration 3740 || Loss: 0.1506 || 20iter: 14.9709 sec.\n",
            "iteration 3760 || Loss: 0.1898 || 20iter: 14.9436 sec.\n",
            "iteration 3780 || Loss: 0.2826 || 20iter: 14.9895 sec.\n",
            "iteration 3800 || Loss: 0.2374 || 20iter: 15.0364 sec.\n",
            "iteration 3820 || Loss: 0.1482 || 20iter: 15.0056 sec.\n",
            "iteration 3840 || Loss: 0.1165 || 20iter: 15.0109 sec.\n",
            "iteration 3860 || Loss: 0.1603 || 20iter: 15.0555 sec.\n",
            "iteration 3880 || Loss: 0.2844 || 20iter: 15.0551 sec.\n",
            "iteration 3900 || Loss: 0.1202 || 20iter: 15.0536 sec.\n",
            "iteration 3920 || Loss: 0.2061 || 20iter: 15.1418 sec.\n",
            "iteration 3940 || Loss: 0.2829 || 20iter: 15.0299 sec.\n",
            "iteration 3960 || Loss: 0.1501 || 20iter: 15.0162 sec.\n",
            "iteration 3980 || Loss: 0.1090 || 20iter: 14.9798 sec.\n",
            "iteration 4000 || Loss: 0.0855 || 20iter: 15.0550 sec.\n",
            "iteration 4020 || Loss: 0.2410 || 20iter: 15.0366 sec.\n",
            "--------------\n",
            "epoch 11 || Epoch_TRAIN_Loss: 0.1929 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.6225 sec.\n",
            "--------------\n",
            "Epoch 12/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 4040 || Loss: 0.0891 || 20iter: 10.4163 sec.\n",
            "iteration 4060 || Loss: 0.0932 || 20iter: 14.9650 sec.\n",
            "iteration 4080 || Loss: 0.1495 || 20iter: 14.9817 sec.\n",
            "iteration 4100 || Loss: 0.0917 || 20iter: 15.0172 sec.\n",
            "iteration 4120 || Loss: 0.0393 || 20iter: 14.9519 sec.\n",
            "iteration 4140 || Loss: 0.1762 || 20iter: 15.2003 sec.\n",
            "iteration 4160 || Loss: 0.1218 || 20iter: 15.2585 sec.\n",
            "iteration 4180 || Loss: 0.1646 || 20iter: 15.0595 sec.\n",
            "iteration 4200 || Loss: 0.1283 || 20iter: 14.9988 sec.\n",
            "iteration 4220 || Loss: 0.2264 || 20iter: 15.0307 sec.\n",
            "iteration 4240 || Loss: 0.3019 || 20iter: 15.0229 sec.\n",
            "iteration 4260 || Loss: 0.1830 || 20iter: 14.9866 sec.\n",
            "iteration 4280 || Loss: 0.2302 || 20iter: 14.9972 sec.\n",
            "iteration 4300 || Loss: 0.3935 || 20iter: 15.0844 sec.\n",
            "iteration 4320 || Loss: 0.1558 || 20iter: 15.1253 sec.\n",
            "iteration 4340 || Loss: 0.3121 || 20iter: 15.0561 sec.\n",
            "iteration 4360 || Loss: 0.2588 || 20iter: 15.0657 sec.\n",
            "iteration 4380 || Loss: 0.1940 || 20iter: 14.9751 sec.\n",
            "--------------\n",
            "epoch 12 || Epoch_TRAIN_Loss: 0.1855 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.9087 sec.\n",
            "--------------\n",
            "Epoch 13/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 4400 || Loss: 0.0902 || 20iter: 5.7095 sec.\n",
            "iteration 4420 || Loss: 0.2241 || 20iter: 15.0746 sec.\n",
            "iteration 4440 || Loss: 0.2662 || 20iter: 15.0379 sec.\n",
            "iteration 4460 || Loss: 0.1300 || 20iter: 15.0454 sec.\n",
            "iteration 4480 || Loss: 0.1850 || 20iter: 14.9917 sec.\n",
            "iteration 4500 || Loss: 0.0640 || 20iter: 15.0140 sec.\n",
            "iteration 4520 || Loss: 0.1772 || 20iter: 14.9481 sec.\n",
            "iteration 4540 || Loss: 0.1378 || 20iter: 15.0351 sec.\n",
            "iteration 4560 || Loss: 0.1099 || 20iter: 15.0104 sec.\n",
            "iteration 4580 || Loss: 0.2042 || 20iter: 15.0441 sec.\n",
            "iteration 4600 || Loss: 0.2084 || 20iter: 14.9837 sec.\n",
            "iteration 4620 || Loss: 0.4003 || 20iter: 15.0084 sec.\n",
            "iteration 4640 || Loss: 0.1519 || 20iter: 15.0414 sec.\n",
            "iteration 4660 || Loss: 0.3200 || 20iter: 15.0659 sec.\n",
            "iteration 4680 || Loss: 0.1782 || 20iter: 15.0099 sec.\n",
            "iteration 4700 || Loss: 0.1455 || 20iter: 14.9948 sec.\n",
            "iteration 4720 || Loss: 0.4346 || 20iter: 15.1209 sec.\n",
            "iteration 4740 || Loss: 0.1185 || 20iter: 15.0168 sec.\n",
            "--------------\n",
            "epoch 13 || Epoch_TRAIN_Loss: 0.1831 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.5371 sec.\n",
            "--------------\n",
            "Epoch 14/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 4760 || Loss: 0.1129 || 20iter: 1.0397 sec.\n",
            "iteration 4780 || Loss: 0.3268 || 20iter: 15.0202 sec.\n",
            "iteration 4800 || Loss: 0.1276 || 20iter: 15.0185 sec.\n",
            "iteration 4820 || Loss: 0.0664 || 20iter: 15.0788 sec.\n",
            "iteration 4840 || Loss: 0.2872 || 20iter: 15.0128 sec.\n",
            "iteration 4860 || Loss: 0.1887 || 20iter: 15.0762 sec.\n",
            "iteration 4880 || Loss: 0.0786 || 20iter: 14.9843 sec.\n",
            "iteration 4900 || Loss: 0.2059 || 20iter: 15.0119 sec.\n",
            "iteration 4920 || Loss: 0.4483 || 20iter: 14.9887 sec.\n",
            "iteration 4940 || Loss: 0.1576 || 20iter: 14.9854 sec.\n",
            "iteration 4960 || Loss: 0.1309 || 20iter: 15.0148 sec.\n",
            "iteration 4980 || Loss: 0.0788 || 20iter: 15.0307 sec.\n",
            "iteration 5000 || Loss: 0.1484 || 20iter: 14.9857 sec.\n",
            "iteration 5020 || Loss: 0.1542 || 20iter: 15.0255 sec.\n",
            "iteration 5040 || Loss: 0.1293 || 20iter: 14.9896 sec.\n",
            "iteration 5060 || Loss: 0.1147 || 20iter: 15.0359 sec.\n",
            "iteration 5080 || Loss: 0.2172 || 20iter: 15.0211 sec.\n",
            "iteration 5100 || Loss: 0.2487 || 20iter: 15.0266 sec.\n",
            "iteration 5120 || Loss: 0.1362 || 20iter: 15.1177 sec.\n",
            "--------------\n",
            "epoch 14 || Epoch_TRAIN_Loss: 0.1836 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.5096 sec.\n",
            "--------------\n",
            "Epoch 15/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 5140 || Loss: 0.1649 || 20iter: 11.9193 sec.\n",
            "iteration 5160 || Loss: 0.1693 || 20iter: 14.9550 sec.\n",
            "iteration 5180 || Loss: 0.0570 || 20iter: 15.0194 sec.\n",
            "iteration 5200 || Loss: 0.1050 || 20iter: 15.0502 sec.\n",
            "iteration 5220 || Loss: 0.1026 || 20iter: 14.9873 sec.\n",
            "iteration 5240 || Loss: 0.2410 || 20iter: 15.0264 sec.\n",
            "iteration 5260 || Loss: 0.3357 || 20iter: 15.0525 sec.\n",
            "iteration 5280 || Loss: 0.1021 || 20iter: 15.0495 sec.\n",
            "iteration 5300 || Loss: 0.1642 || 20iter: 14.9652 sec.\n",
            "iteration 5320 || Loss: 0.1595 || 20iter: 15.0238 sec.\n",
            "iteration 5340 || Loss: 0.0562 || 20iter: 14.9834 sec.\n",
            "iteration 5360 || Loss: 0.2049 || 20iter: 15.0595 sec.\n",
            "iteration 5380 || Loss: 0.2189 || 20iter: 15.0723 sec.\n",
            "iteration 5400 || Loss: 0.3208 || 20iter: 15.0542 sec.\n",
            "iteration 5420 || Loss: 0.1532 || 20iter: 15.0593 sec.\n",
            "iteration 5440 || Loss: 0.1992 || 20iter: 14.9727 sec.\n",
            "iteration 5460 || Loss: 0.1326 || 20iter: 15.0317 sec.\n",
            "iteration 5480 || Loss: 0.1161 || 20iter: 15.0449 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 15 || Epoch_TRAIN_Loss: 0.1798 || Epoch_VAL_Loss: 0.2041\n",
            "timer: 360.0912 sec.\n",
            "--------------\n",
            "Epoch 16/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 5500 || Loss: 0.2097 || 20iter: 7.2834 sec.\n",
            "iteration 5520 || Loss: 0.1161 || 20iter: 15.0350 sec.\n",
            "iteration 5540 || Loss: 0.3108 || 20iter: 15.0135 sec.\n",
            "iteration 5560 || Loss: 0.1907 || 20iter: 15.0268 sec.\n",
            "iteration 5580 || Loss: 0.1526 || 20iter: 15.0081 sec.\n",
            "iteration 5600 || Loss: 0.1839 || 20iter: 15.0410 sec.\n",
            "iteration 5620 || Loss: 0.1306 || 20iter: 15.0680 sec.\n",
            "iteration 5640 || Loss: 0.1566 || 20iter: 15.0059 sec.\n",
            "iteration 5660 || Loss: 0.3645 || 20iter: 15.0472 sec.\n",
            "iteration 5680 || Loss: 0.2196 || 20iter: 15.0553 sec.\n",
            "iteration 5700 || Loss: 0.2581 || 20iter: 15.0344 sec.\n",
            "iteration 5720 || Loss: 0.0731 || 20iter: 14.9731 sec.\n",
            "iteration 5740 || Loss: 0.1772 || 20iter: 15.0361 sec.\n",
            "iteration 5760 || Loss: 0.0967 || 20iter: 14.9809 sec.\n",
            "iteration 5780 || Loss: 0.2831 || 20iter: 15.0337 sec.\n",
            "iteration 5800 || Loss: 0.1828 || 20iter: 15.0621 sec.\n",
            "iteration 5820 || Loss: 0.0662 || 20iter: 15.1226 sec.\n",
            "iteration 5840 || Loss: 0.2487 || 20iter: 14.9985 sec.\n",
            "--------------\n",
            "epoch 16 || Epoch_TRAIN_Loss: 0.1774 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.6199 sec.\n",
            "--------------\n",
            "Epoch 17/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 5860 || Loss: 0.2325 || 20iter: 2.5947 sec.\n",
            "iteration 5880 || Loss: 0.0992 || 20iter: 15.0128 sec.\n",
            "iteration 5900 || Loss: 0.1404 || 20iter: 15.1131 sec.\n",
            "iteration 5920 || Loss: 0.2417 || 20iter: 15.0156 sec.\n",
            "iteration 5940 || Loss: 0.1685 || 20iter: 15.0034 sec.\n",
            "iteration 5960 || Loss: 0.1647 || 20iter: 15.0128 sec.\n",
            "iteration 5980 || Loss: 0.2894 || 20iter: 15.0293 sec.\n",
            "iteration 6000 || Loss: 0.1624 || 20iter: 15.0735 sec.\n",
            "iteration 6020 || Loss: 0.1382 || 20iter: 15.0191 sec.\n",
            "iteration 6040 || Loss: 0.1293 || 20iter: 15.0325 sec.\n",
            "iteration 6060 || Loss: 0.0850 || 20iter: 15.0081 sec.\n",
            "iteration 6080 || Loss: 0.2067 || 20iter: 15.0274 sec.\n",
            "iteration 6100 || Loss: 0.1476 || 20iter: 15.0536 sec.\n",
            "iteration 6120 || Loss: 0.0784 || 20iter: 14.9960 sec.\n",
            "iteration 6140 || Loss: 0.0793 || 20iter: 14.9809 sec.\n",
            "iteration 6160 || Loss: 0.1382 || 20iter: 15.0496 sec.\n",
            "iteration 6180 || Loss: 0.2096 || 20iter: 15.0468 sec.\n",
            "iteration 6200 || Loss: 0.0861 || 20iter: 15.0098 sec.\n",
            "iteration 6220 || Loss: 0.1406 || 20iter: 15.1623 sec.\n",
            "--------------\n",
            "epoch 17 || Epoch_TRAIN_Loss: 0.1764 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.6958 sec.\n",
            "--------------\n",
            "Epoch 18/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 6240 || Loss: 0.0505 || 20iter: 13.5061 sec.\n",
            "iteration 6260 || Loss: 0.0668 || 20iter: 15.0535 sec.\n",
            "iteration 6280 || Loss: 0.1272 || 20iter: 14.9923 sec.\n",
            "iteration 6300 || Loss: 0.2038 || 20iter: 15.0721 sec.\n",
            "iteration 6320 || Loss: 0.0980 || 20iter: 15.0186 sec.\n",
            "iteration 6340 || Loss: 0.3679 || 20iter: 15.0645 sec.\n",
            "iteration 6360 || Loss: 0.0988 || 20iter: 14.9766 sec.\n",
            "iteration 6380 || Loss: 0.1405 || 20iter: 15.0441 sec.\n",
            "iteration 6400 || Loss: 0.2500 || 20iter: 15.0098 sec.\n",
            "iteration 6420 || Loss: 0.2618 || 20iter: 15.0898 sec.\n",
            "iteration 6440 || Loss: 0.1533 || 20iter: 15.0282 sec.\n",
            "iteration 6460 || Loss: 0.2613 || 20iter: 15.0291 sec.\n",
            "iteration 6480 || Loss: 0.1461 || 20iter: 15.0397 sec.\n",
            "iteration 6500 || Loss: 0.0914 || 20iter: 15.0348 sec.\n",
            "iteration 6520 || Loss: 0.2686 || 20iter: 15.0601 sec.\n",
            "iteration 6540 || Loss: 0.1500 || 20iter: 15.0259 sec.\n",
            "iteration 6560 || Loss: 0.1069 || 20iter: 15.0597 sec.\n",
            "iteration 6580 || Loss: 0.2324 || 20iter: 15.0455 sec.\n",
            "--------------\n",
            "epoch 18 || Epoch_TRAIN_Loss: 0.1735 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.7812 sec.\n",
            "--------------\n",
            "Epoch 19/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 6600 || Loss: 0.1101 || 20iter: 8.8591 sec.\n",
            "iteration 6620 || Loss: 0.2445 || 20iter: 15.1388 sec.\n",
            "iteration 6640 || Loss: 0.0973 || 20iter: 15.0493 sec.\n",
            "iteration 6660 || Loss: 0.1490 || 20iter: 14.9804 sec.\n",
            "iteration 6680 || Loss: 0.1571 || 20iter: 15.0566 sec.\n",
            "iteration 6700 || Loss: 0.2068 || 20iter: 15.0352 sec.\n",
            "iteration 6720 || Loss: 0.0915 || 20iter: 15.0802 sec.\n",
            "iteration 6740 || Loss: 0.2569 || 20iter: 15.0626 sec.\n",
            "iteration 6760 || Loss: 0.0979 || 20iter: 15.1071 sec.\n",
            "iteration 6780 || Loss: 0.2597 || 20iter: 15.0071 sec.\n",
            "iteration 6800 || Loss: 0.1859 || 20iter: 14.9900 sec.\n",
            "iteration 6820 || Loss: 0.0948 || 20iter: 15.0028 sec.\n",
            "iteration 6840 || Loss: 0.1627 || 20iter: 14.9782 sec.\n",
            "iteration 6860 || Loss: 0.1213 || 20iter: 14.9368 sec.\n",
            "iteration 6880 || Loss: 0.1430 || 20iter: 14.9963 sec.\n",
            "iteration 6900 || Loss: 0.3368 || 20iter: 14.9462 sec.\n",
            "iteration 6920 || Loss: 0.1254 || 20iter: 15.0007 sec.\n",
            "iteration 6940 || Loss: 0.0531 || 20iter: 15.0245 sec.\n",
            "--------------\n",
            "epoch 19 || Epoch_TRAIN_Loss: 0.1756 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 284.4782 sec.\n",
            "--------------\n",
            "Epoch 20/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 6960 || Loss: 0.1233 || 20iter: 4.1411 sec.\n",
            "iteration 6980 || Loss: 0.2636 || 20iter: 14.9848 sec.\n",
            "iteration 7000 || Loss: 0.0873 || 20iter: 14.9653 sec.\n",
            "iteration 7020 || Loss: 0.1244 || 20iter: 15.0551 sec.\n",
            "iteration 7040 || Loss: 0.0732 || 20iter: 15.0557 sec.\n",
            "iteration 7060 || Loss: 0.2365 || 20iter: 14.9873 sec.\n",
            "iteration 7080 || Loss: 0.3063 || 20iter: 15.0039 sec.\n",
            "iteration 7100 || Loss: 0.1642 || 20iter: 14.9736 sec.\n",
            "iteration 7120 || Loss: 0.0948 || 20iter: 15.0200 sec.\n",
            "iteration 7140 || Loss: 0.1727 || 20iter: 15.0299 sec.\n",
            "iteration 7160 || Loss: 0.1137 || 20iter: 15.0188 sec.\n",
            "iteration 7180 || Loss: 0.1810 || 20iter: 15.0150 sec.\n",
            "iteration 7200 || Loss: 0.1565 || 20iter: 15.0364 sec.\n",
            "iteration 7220 || Loss: 0.1126 || 20iter: 14.9407 sec.\n",
            "iteration 7240 || Loss: 0.2798 || 20iter: 15.0390 sec.\n",
            "iteration 7260 || Loss: 0.1446 || 20iter: 14.9516 sec.\n",
            "iteration 7280 || Loss: 0.1459 || 20iter: 14.9809 sec.\n",
            "iteration 7300 || Loss: 0.1488 || 20iter: 15.0081 sec.\n",
            "iteration 7320 || Loss: 0.1461 || 20iter: 15.0044 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 20 || Epoch_TRAIN_Loss: 0.1719 || Epoch_VAL_Loss: 0.2121\n",
            "timer: 359.5327 sec.\n",
            "--------------\n",
            "Epoch 21/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 7340 || Loss: 0.2016 || 20iter: 14.9806 sec.\n",
            "iteration 7360 || Loss: 0.0800 || 20iter: 14.9896 sec.\n",
            "iteration 7380 || Loss: 0.0879 || 20iter: 14.9807 sec.\n",
            "iteration 7400 || Loss: 0.0451 || 20iter: 14.9148 sec.\n",
            "iteration 7420 || Loss: 0.1398 || 20iter: 14.9958 sec.\n",
            "iteration 7440 || Loss: 0.1700 || 20iter: 14.9420 sec.\n",
            "iteration 7460 || Loss: 0.1249 || 20iter: 15.0442 sec.\n",
            "iteration 7480 || Loss: 0.2249 || 20iter: 14.9682 sec.\n",
            "iteration 7500 || Loss: 0.2188 || 20iter: 14.9988 sec.\n",
            "iteration 7520 || Loss: 0.1243 || 20iter: 14.9573 sec.\n",
            "iteration 7540 || Loss: 0.2826 || 20iter: 14.9729 sec.\n",
            "iteration 7560 || Loss: 0.0885 || 20iter: 15.0322 sec.\n",
            "iteration 7580 || Loss: 0.1372 || 20iter: 15.0265 sec.\n",
            "iteration 7600 || Loss: 0.2992 || 20iter: 14.9912 sec.\n",
            "iteration 7620 || Loss: 0.1014 || 20iter: 15.0238 sec.\n",
            "iteration 7640 || Loss: 0.2039 || 20iter: 14.9886 sec.\n",
            "iteration 7660 || Loss: 0.1599 || 20iter: 14.9856 sec.\n",
            "iteration 7680 || Loss: 0.1372 || 20iter: 14.9758 sec.\n",
            "--------------\n",
            "epoch 21 || Epoch_TRAIN_Loss: 0.1712 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.7921 sec.\n",
            "--------------\n",
            "Epoch 22/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 7700 || Loss: 0.2000 || 20iter: 10.3606 sec.\n",
            "iteration 7720 || Loss: 0.0806 || 20iter: 15.0696 sec.\n",
            "iteration 7740 || Loss: 0.0904 || 20iter: 15.0440 sec.\n",
            "iteration 7760 || Loss: 0.1028 || 20iter: 15.0068 sec.\n",
            "iteration 7780 || Loss: 0.1948 || 20iter: 14.9535 sec.\n",
            "iteration 7800 || Loss: 0.2008 || 20iter: 15.0193 sec.\n",
            "iteration 7820 || Loss: 0.1444 || 20iter: 14.9859 sec.\n",
            "iteration 7840 || Loss: 0.1091 || 20iter: 14.9945 sec.\n",
            "iteration 7860 || Loss: 0.0968 || 20iter: 14.9521 sec.\n",
            "iteration 7880 || Loss: 0.1739 || 20iter: 15.0354 sec.\n",
            "iteration 7900 || Loss: 0.2163 || 20iter: 14.9807 sec.\n",
            "iteration 7920 || Loss: 0.1072 || 20iter: 14.9692 sec.\n",
            "iteration 7940 || Loss: 0.4413 || 20iter: 14.9538 sec.\n",
            "iteration 7960 || Loss: 0.1095 || 20iter: 14.9765 sec.\n",
            "iteration 7980 || Loss: 0.2423 || 20iter: 14.9964 sec.\n",
            "iteration 8000 || Loss: 0.1426 || 20iter: 14.9788 sec.\n",
            "iteration 8020 || Loss: 0.3355 || 20iter: 15.0052 sec.\n",
            "iteration 8040 || Loss: 0.1527 || 20iter: 14.9923 sec.\n",
            "--------------\n",
            "epoch 22 || Epoch_TRAIN_Loss: 0.1698 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.9502 sec.\n",
            "--------------\n",
            "Epoch 23/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 8060 || Loss: 0.2488 || 20iter: 5.7108 sec.\n",
            "iteration 8080 || Loss: 0.3021 || 20iter: 15.0300 sec.\n",
            "iteration 8100 || Loss: 0.2145 || 20iter: 14.9462 sec.\n",
            "iteration 8120 || Loss: 0.1519 || 20iter: 15.0331 sec.\n",
            "iteration 8140 || Loss: 0.1041 || 20iter: 14.9444 sec.\n",
            "iteration 8160 || Loss: 0.0790 || 20iter: 15.0166 sec.\n",
            "iteration 8180 || Loss: 0.1632 || 20iter: 15.0309 sec.\n",
            "iteration 8200 || Loss: 0.1490 || 20iter: 15.0093 sec.\n",
            "iteration 8220 || Loss: 0.1529 || 20iter: 14.9683 sec.\n",
            "iteration 8240 || Loss: 0.2869 || 20iter: 14.9749 sec.\n",
            "iteration 8260 || Loss: 0.1714 || 20iter: 14.9751 sec.\n",
            "iteration 8280 || Loss: 0.2835 || 20iter: 15.0396 sec.\n",
            "iteration 8300 || Loss: 0.0850 || 20iter: 14.9836 sec.\n",
            "iteration 8320 || Loss: 0.1052 || 20iter: 14.9855 sec.\n",
            "iteration 8340 || Loss: 0.1081 || 20iter: 14.9634 sec.\n",
            "iteration 8360 || Loss: 0.1280 || 20iter: 14.9572 sec.\n",
            "iteration 8380 || Loss: 0.0707 || 20iter: 15.0054 sec.\n",
            "iteration 8400 || Loss: 0.3009 || 20iter: 15.0568 sec.\n",
            "--------------\n",
            "epoch 23 || Epoch_TRAIN_Loss: 0.1705 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.9582 sec.\n",
            "--------------\n",
            "Epoch 24/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 8420 || Loss: 0.1091 || 20iter: 1.0501 sec.\n",
            "iteration 8440 || Loss: 0.1740 || 20iter: 15.0256 sec.\n",
            "iteration 8460 || Loss: 0.3123 || 20iter: 15.0131 sec.\n",
            "iteration 8480 || Loss: 0.1422 || 20iter: 14.9937 sec.\n",
            "iteration 8500 || Loss: 0.1365 || 20iter: 15.0573 sec.\n",
            "iteration 8520 || Loss: 0.2239 || 20iter: 15.0783 sec.\n",
            "iteration 8540 || Loss: 0.0900 || 20iter: 15.0049 sec.\n",
            "iteration 8560 || Loss: 0.2051 || 20iter: 14.9649 sec.\n",
            "iteration 8580 || Loss: 0.1952 || 20iter: 14.9657 sec.\n",
            "iteration 8600 || Loss: 0.1281 || 20iter: 15.0562 sec.\n",
            "iteration 8620 || Loss: 0.0773 || 20iter: 14.9579 sec.\n",
            "iteration 8640 || Loss: 0.0974 || 20iter: 14.9057 sec.\n",
            "iteration 8660 || Loss: 0.1104 || 20iter: 14.9587 sec.\n",
            "iteration 8680 || Loss: 0.0521 || 20iter: 14.9274 sec.\n",
            "iteration 8700 || Loss: 0.1296 || 20iter: 15.0175 sec.\n",
            "iteration 8720 || Loss: 0.1421 || 20iter: 14.9797 sec.\n",
            "iteration 8740 || Loss: 0.1545 || 20iter: 14.9555 sec.\n",
            "iteration 8760 || Loss: 0.1141 || 20iter: 14.9680 sec.\n",
            "iteration 8780 || Loss: 0.2480 || 20iter: 14.9242 sec.\n",
            "--------------\n",
            "epoch 24 || Epoch_TRAIN_Loss: 0.1695 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.7943 sec.\n",
            "--------------\n",
            "Epoch 25/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 8800 || Loss: 0.3235 || 20iter: 11.8826 sec.\n",
            "iteration 8820 || Loss: 0.2305 || 20iter: 14.9488 sec.\n",
            "iteration 8840 || Loss: 0.0848 || 20iter: 14.9356 sec.\n",
            "iteration 8860 || Loss: 0.0656 || 20iter: 15.0356 sec.\n",
            "iteration 8880 || Loss: 0.2044 || 20iter: 14.9850 sec.\n",
            "iteration 8900 || Loss: 0.0749 || 20iter: 14.9234 sec.\n",
            "iteration 8920 || Loss: 0.1659 || 20iter: 15.0952 sec.\n",
            "iteration 8940 || Loss: 0.1203 || 20iter: 15.0081 sec.\n",
            "iteration 8960 || Loss: 0.1046 || 20iter: 15.0126 sec.\n",
            "iteration 8980 || Loss: 0.1642 || 20iter: 14.9926 sec.\n",
            "iteration 9000 || Loss: 0.0667 || 20iter: 14.9949 sec.\n",
            "iteration 9020 || Loss: 0.1368 || 20iter: 15.0058 sec.\n",
            "iteration 9040 || Loss: 0.3442 || 20iter: 14.9768 sec.\n",
            "iteration 9060 || Loss: 0.1587 || 20iter: 14.9597 sec.\n",
            "iteration 9080 || Loss: 0.1592 || 20iter: 15.0430 sec.\n",
            "iteration 9100 || Loss: 0.0805 || 20iter: 14.9613 sec.\n",
            "iteration 9120 || Loss: 0.1336 || 20iter: 14.9887 sec.\n",
            "iteration 9140 || Loss: 0.2046 || 20iter: 14.9989 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 25 || Epoch_TRAIN_Loss: 0.1652 || Epoch_VAL_Loss: 0.1976\n",
            "timer: 358.9205 sec.\n",
            "--------------\n",
            "Epoch 26/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 9160 || Loss: 0.0671 || 20iter: 7.2681 sec.\n",
            "iteration 9180 || Loss: 0.1018 || 20iter: 14.9754 sec.\n",
            "iteration 9200 || Loss: 0.3171 || 20iter: 14.9533 sec.\n",
            "iteration 9220 || Loss: 0.1599 || 20iter: 15.0196 sec.\n",
            "iteration 9240 || Loss: 0.1348 || 20iter: 15.0308 sec.\n",
            "iteration 9260 || Loss: 0.1970 || 20iter: 14.9986 sec.\n",
            "iteration 9280 || Loss: 0.1356 || 20iter: 14.9960 sec.\n",
            "iteration 9300 || Loss: 0.1707 || 20iter: 14.9664 sec.\n",
            "iteration 9320 || Loss: 0.2587 || 20iter: 15.0160 sec.\n",
            "iteration 9340 || Loss: 0.1032 || 20iter: 14.9530 sec.\n",
            "iteration 9360 || Loss: 0.1168 || 20iter: 14.9766 sec.\n",
            "iteration 9380 || Loss: 0.1541 || 20iter: 14.9723 sec.\n",
            "iteration 9400 || Loss: 0.3413 || 20iter: 15.0167 sec.\n",
            "iteration 9420 || Loss: 0.1292 || 20iter: 14.9722 sec.\n",
            "iteration 9440 || Loss: 0.2005 || 20iter: 14.9673 sec.\n",
            "iteration 9460 || Loss: 0.1777 || 20iter: 14.9348 sec.\n",
            "iteration 9480 || Loss: 0.0763 || 20iter: 14.9432 sec.\n",
            "iteration 9500 || Loss: 0.1964 || 20iter: 14.9911 sec.\n",
            "--------------\n",
            "epoch 26 || Epoch_TRAIN_Loss: 0.1613 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.7562 sec.\n",
            "--------------\n",
            "Epoch 27/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 9520 || Loss: 0.3431 || 20iter: 2.6092 sec.\n",
            "iteration 9540 || Loss: 0.1365 || 20iter: 15.0241 sec.\n",
            "iteration 9560 || Loss: 0.1277 || 20iter: 14.9700 sec.\n",
            "iteration 9580 || Loss: 0.0521 || 20iter: 14.9764 sec.\n",
            "iteration 9600 || Loss: 0.0963 || 20iter: 14.9955 sec.\n",
            "iteration 9620 || Loss: 0.2735 || 20iter: 15.0328 sec.\n",
            "iteration 9640 || Loss: 0.1544 || 20iter: 15.0377 sec.\n",
            "iteration 9660 || Loss: 0.1878 || 20iter: 14.9446 sec.\n",
            "iteration 9680 || Loss: 0.1775 || 20iter: 15.0384 sec.\n",
            "iteration 9700 || Loss: 0.2686 || 20iter: 14.9846 sec.\n",
            "iteration 9720 || Loss: 0.0813 || 20iter: 14.9768 sec.\n",
            "iteration 9740 || Loss: 0.1622 || 20iter: 14.9951 sec.\n",
            "iteration 9760 || Loss: 0.0940 || 20iter: 14.9516 sec.\n",
            "iteration 9780 || Loss: 0.1046 || 20iter: 14.9665 sec.\n",
            "iteration 9800 || Loss: 0.1964 || 20iter: 14.9691 sec.\n",
            "iteration 9820 || Loss: 0.0965 || 20iter: 14.9434 sec.\n",
            "iteration 9840 || Loss: 0.1229 || 20iter: 15.0153 sec.\n",
            "iteration 9860 || Loss: 0.1537 || 20iter: 14.9259 sec.\n",
            "iteration 9880 || Loss: 0.0897 || 20iter: 15.0187 sec.\n",
            "--------------\n",
            "epoch 27 || Epoch_TRAIN_Loss: 0.1633 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.8295 sec.\n",
            "--------------\n",
            "Epoch 28/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 9900 || Loss: 0.1596 || 20iter: 13.4996 sec.\n",
            "iteration 9920 || Loss: 0.0719 || 20iter: 15.0155 sec.\n",
            "iteration 9940 || Loss: 0.1785 || 20iter: 14.9626 sec.\n",
            "iteration 9960 || Loss: 0.1184 || 20iter: 15.0097 sec.\n",
            "iteration 9980 || Loss: 0.1325 || 20iter: 14.9713 sec.\n",
            "iteration 10000 || Loss: 0.3434 || 20iter: 14.9640 sec.\n",
            "iteration 10020 || Loss: 0.0663 || 20iter: 15.0046 sec.\n",
            "iteration 10040 || Loss: 0.2936 || 20iter: 14.9985 sec.\n",
            "iteration 10060 || Loss: 0.1459 || 20iter: 15.0011 sec.\n",
            "iteration 10080 || Loss: 0.1334 || 20iter: 14.9741 sec.\n",
            "iteration 10100 || Loss: 0.1476 || 20iter: 14.9778 sec.\n",
            "iteration 10120 || Loss: 0.1714 || 20iter: 14.9468 sec.\n",
            "iteration 10140 || Loss: 0.3682 || 20iter: 14.9441 sec.\n",
            "iteration 10160 || Loss: 0.1005 || 20iter: 15.0480 sec.\n",
            "iteration 10180 || Loss: 0.2294 || 20iter: 15.0351 sec.\n",
            "iteration 10200 || Loss: 0.1139 || 20iter: 14.9330 sec.\n",
            "iteration 10220 || Loss: 0.1238 || 20iter: 14.9399 sec.\n",
            "iteration 10240 || Loss: 0.2146 || 20iter: 14.9678 sec.\n",
            "--------------\n",
            "epoch 28 || Epoch_TRAIN_Loss: 0.1648 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.7908 sec.\n",
            "--------------\n",
            "Epoch 29/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 10260 || Loss: 0.1654 || 20iter: 8.7981 sec.\n",
            "iteration 10280 || Loss: 0.2012 || 20iter: 14.9345 sec.\n",
            "iteration 10300 || Loss: 0.0983 || 20iter: 14.9695 sec.\n",
            "iteration 10320 || Loss: 0.0635 || 20iter: 14.9734 sec.\n",
            "iteration 10340 || Loss: 0.1686 || 20iter: 14.9854 sec.\n",
            "iteration 10360 || Loss: 0.1427 || 20iter: 15.0019 sec.\n",
            "iteration 10380 || Loss: 0.2247 || 20iter: 15.0142 sec.\n",
            "iteration 10400 || Loss: 0.1530 || 20iter: 14.9424 sec.\n",
            "iteration 10420 || Loss: 0.1835 || 20iter: 15.0195 sec.\n",
            "iteration 10440 || Loss: 0.1146 || 20iter: 14.9921 sec.\n",
            "iteration 10460 || Loss: 0.0852 || 20iter: 14.9962 sec.\n",
            "iteration 10480 || Loss: 0.1288 || 20iter: 14.9842 sec.\n",
            "iteration 10500 || Loss: 0.2362 || 20iter: 14.9555 sec.\n",
            "iteration 10520 || Loss: 0.0955 || 20iter: 14.9480 sec.\n",
            "iteration 10540 || Loss: 0.1953 || 20iter: 14.9449 sec.\n",
            "iteration 10560 || Loss: 0.1424 || 20iter: 14.9381 sec.\n",
            "iteration 10580 || Loss: 0.2166 || 20iter: 14.9411 sec.\n",
            "iteration 10600 || Loss: 0.1496 || 20iter: 14.9563 sec.\n",
            "--------------\n",
            "epoch 29 || Epoch_TRAIN_Loss: 0.1586 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.4728 sec.\n",
            "--------------\n",
            "Epoch 30/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 10620 || Loss: 0.0802 || 20iter: 4.1166 sec.\n",
            "iteration 10640 || Loss: 0.1185 || 20iter: 14.9783 sec.\n",
            "iteration 10660 || Loss: 0.0842 || 20iter: 14.9445 sec.\n",
            "iteration 10680 || Loss: 0.1827 || 20iter: 14.9983 sec.\n",
            "iteration 10700 || Loss: 0.2355 || 20iter: 14.9675 sec.\n",
            "iteration 10720 || Loss: 0.1052 || 20iter: 14.9372 sec.\n",
            "iteration 10740 || Loss: 0.4019 || 20iter: 14.9653 sec.\n",
            "iteration 10760 || Loss: 0.0678 || 20iter: 14.9527 sec.\n",
            "iteration 10780 || Loss: 0.2069 || 20iter: 14.9717 sec.\n",
            "iteration 10800 || Loss: 0.0984 || 20iter: 14.9696 sec.\n",
            "iteration 10820 || Loss: 0.1665 || 20iter: 14.9764 sec.\n",
            "iteration 10840 || Loss: 0.0858 || 20iter: 14.9712 sec.\n",
            "iteration 10860 || Loss: 0.1299 || 20iter: 15.0252 sec.\n",
            "iteration 10880 || Loss: 0.1723 || 20iter: 14.9995 sec.\n",
            "iteration 10900 || Loss: 0.2809 || 20iter: 14.9910 sec.\n",
            "iteration 10920 || Loss: 0.2638 || 20iter: 14.9334 sec.\n",
            "iteration 10940 || Loss: 0.1693 || 20iter: 14.9437 sec.\n",
            "iteration 10960 || Loss: 0.1151 || 20iter: 15.0286 sec.\n",
            "iteration 10980 || Loss: 0.2192 || 20iter: 15.0164 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 30 || Epoch_TRAIN_Loss: 0.1571 || Epoch_VAL_Loss: 0.1890\n",
            "timer: 358.4690 sec.\n",
            "--------------\n",
            "Epoch 31/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 11000 || Loss: 0.1941 || 20iter: 15.0326 sec.\n",
            "iteration 11020 || Loss: 0.2057 || 20iter: 14.9964 sec.\n",
            "iteration 11040 || Loss: 0.0983 || 20iter: 14.9474 sec.\n",
            "iteration 11060 || Loss: 0.0869 || 20iter: 14.9209 sec.\n",
            "iteration 11080 || Loss: 0.2903 || 20iter: 14.9163 sec.\n",
            "iteration 11100 || Loss: 0.2522 || 20iter: 15.0106 sec.\n",
            "iteration 11120 || Loss: 0.1284 || 20iter: 14.9658 sec.\n",
            "iteration 11140 || Loss: 0.2891 || 20iter: 14.9348 sec.\n",
            "iteration 11160 || Loss: 0.1164 || 20iter: 14.9495 sec.\n",
            "iteration 11180 || Loss: 0.0949 || 20iter: 15.0124 sec.\n",
            "iteration 11200 || Loss: 0.1668 || 20iter: 15.0098 sec.\n",
            "iteration 11220 || Loss: 0.1446 || 20iter: 14.9776 sec.\n",
            "iteration 11240 || Loss: 0.0891 || 20iter: 14.9193 sec.\n",
            "iteration 11260 || Loss: 0.0529 || 20iter: 14.9513 sec.\n",
            "iteration 11280 || Loss: 0.1076 || 20iter: 14.9677 sec.\n",
            "iteration 11300 || Loss: 0.1405 || 20iter: 14.9788 sec.\n",
            "iteration 11320 || Loss: 0.1235 || 20iter: 14.9272 sec.\n",
            "iteration 11340 || Loss: 0.1929 || 20iter: 14.9405 sec.\n",
            "--------------\n",
            "epoch 31 || Epoch_TRAIN_Loss: 0.1557 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.3850 sec.\n",
            "--------------\n",
            "Epoch 32/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 11360 || Loss: 0.2630 || 20iter: 10.3344 sec.\n",
            "iteration 11380 || Loss: 0.0410 || 20iter: 14.9863 sec.\n",
            "iteration 11400 || Loss: 0.2246 || 20iter: 14.9282 sec.\n",
            "iteration 11420 || Loss: 0.3284 || 20iter: 14.9973 sec.\n",
            "iteration 11440 || Loss: 0.0781 || 20iter: 14.9612 sec.\n",
            "iteration 11460 || Loss: 0.1643 || 20iter: 14.9731 sec.\n",
            "iteration 11480 || Loss: 0.1673 || 20iter: 14.9387 sec.\n",
            "iteration 11500 || Loss: 0.0959 || 20iter: 14.9824 sec.\n",
            "iteration 11520 || Loss: 0.2689 || 20iter: 15.0299 sec.\n",
            "iteration 11540 || Loss: 0.3442 || 20iter: 15.0604 sec.\n",
            "iteration 11560 || Loss: 0.2246 || 20iter: 14.9531 sec.\n",
            "iteration 11580 || Loss: 0.1316 || 20iter: 14.9563 sec.\n",
            "iteration 11600 || Loss: 0.0853 || 20iter: 14.8969 sec.\n",
            "iteration 11620 || Loss: 0.1810 || 20iter: 14.9825 sec.\n",
            "iteration 11640 || Loss: 0.0700 || 20iter: 14.9452 sec.\n",
            "iteration 11660 || Loss: 0.1531 || 20iter: 14.9652 sec.\n",
            "iteration 11680 || Loss: 0.0987 || 20iter: 14.9479 sec.\n",
            "iteration 11700 || Loss: 0.1217 || 20iter: 14.9715 sec.\n",
            "--------------\n",
            "epoch 32 || Epoch_TRAIN_Loss: 0.1603 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.4758 sec.\n",
            "--------------\n",
            "Epoch 33/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 11720 || Loss: 0.1444 || 20iter: 5.6729 sec.\n",
            "iteration 11740 || Loss: 0.1861 || 20iter: 14.9185 sec.\n",
            "iteration 11760 || Loss: 0.1053 || 20iter: 14.9468 sec.\n",
            "iteration 11780 || Loss: 0.0891 || 20iter: 14.9489 sec.\n",
            "iteration 11800 || Loss: 0.0902 || 20iter: 14.9836 sec.\n",
            "iteration 11820 || Loss: 0.1594 || 20iter: 14.9484 sec.\n",
            "iteration 11840 || Loss: 0.2435 || 20iter: 14.9365 sec.\n",
            "iteration 11860 || Loss: 0.1123 || 20iter: 14.9556 sec.\n",
            "iteration 11880 || Loss: 0.1562 || 20iter: 14.9584 sec.\n",
            "iteration 11900 || Loss: 0.1062 || 20iter: 14.8981 sec.\n",
            "iteration 11920 || Loss: 0.0839 || 20iter: 14.9429 sec.\n",
            "iteration 11940 || Loss: 0.1070 || 20iter: 15.1113 sec.\n",
            "iteration 11960 || Loss: 0.0604 || 20iter: 14.9788 sec.\n",
            "iteration 11980 || Loss: 0.2281 || 20iter: 14.9426 sec.\n",
            "iteration 12000 || Loss: 0.1026 || 20iter: 14.8849 sec.\n",
            "iteration 12020 || Loss: 0.1384 || 20iter: 14.9279 sec.\n",
            "iteration 12040 || Loss: 0.0662 || 20iter: 14.9150 sec.\n",
            "iteration 12060 || Loss: 0.1748 || 20iter: 14.9489 sec.\n",
            "--------------\n",
            "epoch 33 || Epoch_TRAIN_Loss: 0.1532 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.1152 sec.\n",
            "--------------\n",
            "Epoch 34/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 12080 || Loss: 0.0357 || 20iter: 1.0331 sec.\n",
            "iteration 12100 || Loss: 0.1002 || 20iter: 14.9090 sec.\n",
            "iteration 12120 || Loss: 0.1280 || 20iter: 14.9454 sec.\n",
            "iteration 12140 || Loss: 0.2943 || 20iter: 14.9854 sec.\n",
            "iteration 12160 || Loss: 0.0868 || 20iter: 14.9183 sec.\n",
            "iteration 12180 || Loss: 0.2980 || 20iter: 14.9214 sec.\n",
            "iteration 12200 || Loss: 0.1175 || 20iter: 14.9801 sec.\n",
            "iteration 12220 || Loss: 0.3051 || 20iter: 14.9288 sec.\n",
            "iteration 12240 || Loss: 0.1351 || 20iter: 14.9887 sec.\n",
            "iteration 12260 || Loss: 0.1059 || 20iter: 14.9581 sec.\n",
            "iteration 12280 || Loss: 0.1167 || 20iter: 14.9776 sec.\n",
            "iteration 12300 || Loss: 0.1152 || 20iter: 14.9212 sec.\n",
            "iteration 12320 || Loss: 0.0860 || 20iter: 15.0170 sec.\n",
            "iteration 12340 || Loss: 0.1196 || 20iter: 15.1084 sec.\n",
            "iteration 12360 || Loss: 0.1343 || 20iter: 14.9840 sec.\n",
            "iteration 12380 || Loss: 0.2718 || 20iter: 14.9546 sec.\n",
            "iteration 12400 || Loss: 0.1753 || 20iter: 14.8780 sec.\n",
            "iteration 12420 || Loss: 0.0468 || 20iter: 14.9728 sec.\n",
            "iteration 12440 || Loss: 0.3433 || 20iter: 14.9915 sec.\n",
            "--------------\n",
            "epoch 34 || Epoch_TRAIN_Loss: 0.1563 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.3806 sec.\n",
            "--------------\n",
            "Epoch 35/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 12460 || Loss: 0.1519 || 20iter: 11.8405 sec.\n",
            "iteration 12480 || Loss: 0.1362 || 20iter: 14.9213 sec.\n",
            "iteration 12500 || Loss: 0.1650 || 20iter: 14.9618 sec.\n",
            "iteration 12520 || Loss: 0.0736 || 20iter: 14.9598 sec.\n",
            "iteration 12540 || Loss: 0.2111 || 20iter: 14.9636 sec.\n",
            "iteration 12560 || Loss: 0.1562 || 20iter: 14.9919 sec.\n",
            "iteration 12580 || Loss: 0.0512 || 20iter: 14.9226 sec.\n",
            "iteration 12600 || Loss: 0.1425 || 20iter: 14.9263 sec.\n",
            "iteration 12620 || Loss: 0.1512 || 20iter: 14.9203 sec.\n",
            "iteration 12640 || Loss: 0.1522 || 20iter: 14.9981 sec.\n",
            "iteration 12660 || Loss: 0.0918 || 20iter: 14.9580 sec.\n",
            "iteration 12680 || Loss: 0.2238 || 20iter: 14.9442 sec.\n",
            "iteration 12700 || Loss: 0.2326 || 20iter: 14.9392 sec.\n",
            "iteration 12720 || Loss: 0.0299 || 20iter: 14.9337 sec.\n",
            "iteration 12740 || Loss: 0.1404 || 20iter: 15.0903 sec.\n",
            "iteration 12760 || Loss: 0.0920 || 20iter: 15.0007 sec.\n",
            "iteration 12780 || Loss: 0.1917 || 20iter: 14.9749 sec.\n",
            "iteration 12800 || Loss: 0.0705 || 20iter: 14.9936 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 35 || Epoch_TRAIN_Loss: 0.1586 || Epoch_VAL_Loss: 0.1944\n",
            "timer: 358.1647 sec.\n",
            "--------------\n",
            "Epoch 36/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 12820 || Loss: 0.1539 || 20iter: 7.2530 sec.\n",
            "iteration 12840 || Loss: 0.1433 || 20iter: 14.9799 sec.\n",
            "iteration 12860 || Loss: 0.1911 || 20iter: 14.9494 sec.\n",
            "iteration 12880 || Loss: 0.1120 || 20iter: 15.0001 sec.\n",
            "iteration 12900 || Loss: 0.0485 || 20iter: 14.9085 sec.\n",
            "iteration 12920 || Loss: 0.0893 || 20iter: 14.9510 sec.\n",
            "iteration 12940 || Loss: 0.3149 || 20iter: 14.9267 sec.\n",
            "iteration 12960 || Loss: 0.0534 || 20iter: 14.9383 sec.\n",
            "iteration 12980 || Loss: 0.3091 || 20iter: 15.0042 sec.\n",
            "iteration 13000 || Loss: 0.2219 || 20iter: 14.9720 sec.\n",
            "iteration 13020 || Loss: 0.1613 || 20iter: 14.9487 sec.\n",
            "iteration 13040 || Loss: 0.1250 || 20iter: 15.0511 sec.\n",
            "iteration 13060 || Loss: 0.1108 || 20iter: 14.9396 sec.\n",
            "iteration 13080 || Loss: 0.1831 || 20iter: 14.9330 sec.\n",
            "iteration 13100 || Loss: 0.0658 || 20iter: 14.9271 sec.\n",
            "iteration 13120 || Loss: 0.3560 || 20iter: 15.0069 sec.\n",
            "iteration 13140 || Loss: 0.0884 || 20iter: 14.9456 sec.\n",
            "iteration 13160 || Loss: 0.1976 || 20iter: 15.0236 sec.\n",
            "--------------\n",
            "epoch 36 || Epoch_TRAIN_Loss: 0.1558 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.3766 sec.\n",
            "--------------\n",
            "Epoch 37/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 13180 || Loss: 0.2167 || 20iter: 2.6013 sec.\n",
            "iteration 13200 || Loss: 0.2086 || 20iter: 14.9931 sec.\n",
            "iteration 13220 || Loss: 0.2259 || 20iter: 14.9451 sec.\n",
            "iteration 13240 || Loss: 0.0660 || 20iter: 14.9346 sec.\n",
            "iteration 13260 || Loss: 0.0377 || 20iter: 14.9271 sec.\n",
            "iteration 13280 || Loss: 0.0772 || 20iter: 15.0089 sec.\n",
            "iteration 13300 || Loss: 0.0990 || 20iter: 14.9832 sec.\n",
            "iteration 13320 || Loss: 0.0869 || 20iter: 14.9180 sec.\n",
            "iteration 13340 || Loss: 0.3438 || 20iter: 14.9498 sec.\n",
            "iteration 13360 || Loss: 0.1165 || 20iter: 14.9630 sec.\n",
            "iteration 13380 || Loss: 0.0997 || 20iter: 14.9476 sec.\n",
            "iteration 13400 || Loss: 0.1165 || 20iter: 14.9851 sec.\n",
            "iteration 13420 || Loss: 0.1290 || 20iter: 14.9932 sec.\n",
            "iteration 13440 || Loss: 0.0866 || 20iter: 15.0313 sec.\n",
            "iteration 13460 || Loss: 0.1711 || 20iter: 14.9422 sec.\n",
            "iteration 13480 || Loss: 0.2955 || 20iter: 14.9604 sec.\n",
            "iteration 13500 || Loss: 0.1175 || 20iter: 14.9831 sec.\n",
            "iteration 13520 || Loss: 0.1036 || 20iter: 14.9505 sec.\n",
            "iteration 13540 || Loss: 0.2168 || 20iter: 14.9641 sec.\n",
            "--------------\n",
            "epoch 37 || Epoch_TRAIN_Loss: 0.1530 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.4240 sec.\n",
            "--------------\n",
            "Epoch 38/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 13560 || Loss: 0.1462 || 20iter: 13.4128 sec.\n",
            "iteration 13580 || Loss: 0.0411 || 20iter: 14.9271 sec.\n",
            "iteration 13600 || Loss: 0.0885 || 20iter: 14.9992 sec.\n",
            "iteration 13620 || Loss: 0.1628 || 20iter: 14.9355 sec.\n",
            "iteration 13640 || Loss: 0.1291 || 20iter: 14.9659 sec.\n",
            "iteration 13660 || Loss: 0.2219 || 20iter: 14.9630 sec.\n",
            "iteration 13680 || Loss: 0.1674 || 20iter: 14.9522 sec.\n",
            "iteration 13700 || Loss: 0.0947 || 20iter: 14.8719 sec.\n",
            "iteration 13720 || Loss: 0.2108 || 20iter: 14.9726 sec.\n",
            "iteration 13740 || Loss: 0.0706 || 20iter: 14.9428 sec.\n",
            "iteration 13760 || Loss: 0.0754 || 20iter: 14.9772 sec.\n",
            "iteration 13780 || Loss: 0.1106 || 20iter: 14.9308 sec.\n",
            "iteration 13800 || Loss: 0.1901 || 20iter: 14.9819 sec.\n",
            "iteration 13820 || Loss: 0.0971 || 20iter: 15.0094 sec.\n",
            "iteration 13840 || Loss: 0.2689 || 20iter: 15.0015 sec.\n",
            "iteration 13860 || Loss: 0.1770 || 20iter: 14.9686 sec.\n",
            "iteration 13880 || Loss: 0.1616 || 20iter: 14.9881 sec.\n",
            "iteration 13900 || Loss: 0.0969 || 20iter: 14.9501 sec.\n",
            "--------------\n",
            "epoch 38 || Epoch_TRAIN_Loss: 0.1515 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.3442 sec.\n",
            "--------------\n",
            "Epoch 39/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 13920 || Loss: 0.1575 || 20iter: 8.8311 sec.\n",
            "iteration 13940 || Loss: 0.2312 || 20iter: 14.9207 sec.\n",
            "iteration 13960 || Loss: 0.1100 || 20iter: 14.9146 sec.\n",
            "iteration 13980 || Loss: 0.1004 || 20iter: 14.9785 sec.\n",
            "iteration 14000 || Loss: 0.0353 || 20iter: 15.0169 sec.\n",
            "iteration 14020 || Loss: 0.0972 || 20iter: 15.0075 sec.\n",
            "iteration 14040 || Loss: 0.1753 || 20iter: 14.8553 sec.\n",
            "iteration 14060 || Loss: 0.2362 || 20iter: 14.9575 sec.\n",
            "iteration 14080 || Loss: 0.1254 || 20iter: 14.9590 sec.\n",
            "iteration 14100 || Loss: 0.0844 || 20iter: 14.9868 sec.\n",
            "iteration 14120 || Loss: 0.0528 || 20iter: 14.9851 sec.\n",
            "iteration 14140 || Loss: 0.0540 || 20iter: 14.9882 sec.\n",
            "iteration 14160 || Loss: 0.2940 || 20iter: 14.9621 sec.\n",
            "iteration 14180 || Loss: 0.1450 || 20iter: 15.0321 sec.\n",
            "iteration 14200 || Loss: 0.1939 || 20iter: 14.9110 sec.\n",
            "iteration 14220 || Loss: 0.1934 || 20iter: 14.9944 sec.\n",
            "iteration 14240 || Loss: 0.1366 || 20iter: 15.0147 sec.\n",
            "iteration 14260 || Loss: 0.2059 || 20iter: 14.9984 sec.\n",
            "--------------\n",
            "epoch 39 || Epoch_TRAIN_Loss: 0.1487 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.5147 sec.\n",
            "--------------\n",
            "Epoch 40/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 14280 || Loss: 0.0459 || 20iter: 4.1202 sec.\n",
            "iteration 14300 || Loss: 0.1876 || 20iter: 14.9864 sec.\n",
            "iteration 14320 || Loss: 0.0995 || 20iter: 14.9495 sec.\n",
            "iteration 14340 || Loss: 0.2070 || 20iter: 15.0457 sec.\n",
            "iteration 14360 || Loss: 0.1507 || 20iter: 14.9131 sec.\n",
            "iteration 14380 || Loss: 0.1359 || 20iter: 14.9490 sec.\n",
            "iteration 14400 || Loss: 0.2686 || 20iter: 14.9061 sec.\n",
            "iteration 14420 || Loss: 0.0551 || 20iter: 14.9961 sec.\n",
            "iteration 14440 || Loss: 0.0981 || 20iter: 14.9958 sec.\n",
            "iteration 14460 || Loss: 0.1533 || 20iter: 14.9578 sec.\n",
            "iteration 14480 || Loss: 0.1931 || 20iter: 14.9344 sec.\n",
            "iteration 14500 || Loss: 0.1021 || 20iter: 14.9921 sec.\n",
            "iteration 14520 || Loss: 0.2660 || 20iter: 14.9832 sec.\n",
            "iteration 14540 || Loss: 0.1436 || 20iter: 15.0459 sec.\n",
            "iteration 14560 || Loss: 0.1444 || 20iter: 14.9617 sec.\n",
            "iteration 14580 || Loss: 0.2228 || 20iter: 14.9677 sec.\n",
            "iteration 14600 || Loss: 0.1076 || 20iter: 14.9324 sec.\n",
            "iteration 14620 || Loss: 0.2389 || 20iter: 15.0211 sec.\n",
            "iteration 14640 || Loss: 0.0865 || 20iter: 14.9745 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 40 || Epoch_TRAIN_Loss: 0.1475 || Epoch_VAL_Loss: 0.2095\n",
            "timer: 358.7336 sec.\n",
            "--------------\n",
            "Epoch 41/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 14660 || Loss: 0.1235 || 20iter: 15.0432 sec.\n",
            "iteration 14680 || Loss: 0.1124 || 20iter: 14.9551 sec.\n",
            "iteration 14700 || Loss: 0.1714 || 20iter: 14.9232 sec.\n",
            "iteration 14720 || Loss: 0.1132 || 20iter: 14.9250 sec.\n",
            "iteration 14740 || Loss: 0.1434 || 20iter: 14.9228 sec.\n",
            "iteration 14760 || Loss: 0.1118 || 20iter: 15.0125 sec.\n",
            "iteration 14780 || Loss: 0.1307 || 20iter: 14.9540 sec.\n",
            "iteration 14800 || Loss: 0.0602 || 20iter: 14.9586 sec.\n",
            "iteration 14820 || Loss: 0.1288 || 20iter: 14.9421 sec.\n",
            "iteration 14840 || Loss: 0.2422 || 20iter: 14.9374 sec.\n",
            "iteration 14860 || Loss: 0.0746 || 20iter: 14.9741 sec.\n",
            "iteration 14880 || Loss: 0.1986 || 20iter: 14.9184 sec.\n",
            "iteration 14900 || Loss: 0.0521 || 20iter: 15.0177 sec.\n",
            "iteration 14920 || Loss: 0.1217 || 20iter: 14.9686 sec.\n",
            "iteration 14940 || Loss: 0.2397 || 20iter: 14.9986 sec.\n",
            "iteration 14960 || Loss: 0.3019 || 20iter: 15.0661 sec.\n",
            "iteration 14980 || Loss: 0.1471 || 20iter: 14.9491 sec.\n",
            "iteration 15000 || Loss: 0.0990 || 20iter: 14.9290 sec.\n",
            "--------------\n",
            "epoch 41 || Epoch_TRAIN_Loss: 0.1482 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.4358 sec.\n",
            "--------------\n",
            "Epoch 42/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 15020 || Loss: 0.0479 || 20iter: 10.3426 sec.\n",
            "iteration 15040 || Loss: 0.0899 || 20iter: 14.9175 sec.\n",
            "iteration 15060 || Loss: 0.2785 || 20iter: 14.9899 sec.\n",
            "iteration 15080 || Loss: 0.1799 || 20iter: 14.9338 sec.\n",
            "iteration 15100 || Loss: 0.1516 || 20iter: 14.9503 sec.\n",
            "iteration 15120 || Loss: 0.0549 || 20iter: 14.9488 sec.\n",
            "iteration 15140 || Loss: 0.0790 || 20iter: 14.9335 sec.\n",
            "iteration 15160 || Loss: 0.1426 || 20iter: 14.9447 sec.\n",
            "iteration 15180 || Loss: 0.2105 || 20iter: 14.8748 sec.\n",
            "iteration 15200 || Loss: 0.1613 || 20iter: 14.9180 sec.\n",
            "iteration 15220 || Loss: 0.1681 || 20iter: 14.9252 sec.\n",
            "iteration 15240 || Loss: 0.0604 || 20iter: 14.9305 sec.\n",
            "iteration 15260 || Loss: 0.1610 || 20iter: 14.9434 sec.\n",
            "iteration 15280 || Loss: 0.1497 || 20iter: 14.9693 sec.\n",
            "iteration 15300 || Loss: 0.1698 || 20iter: 14.9408 sec.\n",
            "iteration 15320 || Loss: 0.1821 || 20iter: 14.9563 sec.\n",
            "iteration 15340 || Loss: 0.0473 || 20iter: 14.9176 sec.\n",
            "iteration 15360 || Loss: 0.1657 || 20iter: 15.0751 sec.\n",
            "--------------\n",
            "epoch 42 || Epoch_TRAIN_Loss: 0.1445 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.0657 sec.\n",
            "--------------\n",
            "Epoch 43/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 15380 || Loss: 0.2329 || 20iter: 5.6747 sec.\n",
            "iteration 15400 || Loss: 0.1307 || 20iter: 14.9951 sec.\n",
            "iteration 15420 || Loss: 0.0958 || 20iter: 14.9125 sec.\n",
            "iteration 15440 || Loss: 0.0928 || 20iter: 14.9171 sec.\n",
            "iteration 15460 || Loss: 0.2665 || 20iter: 14.9283 sec.\n",
            "iteration 15480 || Loss: 0.1820 || 20iter: 14.9698 sec.\n",
            "iteration 15500 || Loss: 0.1189 || 20iter: 14.9491 sec.\n",
            "iteration 15520 || Loss: 0.0974 || 20iter: 14.8830 sec.\n",
            "iteration 15540 || Loss: 0.2684 || 20iter: 14.9391 sec.\n",
            "iteration 15560 || Loss: 0.2073 || 20iter: 14.9912 sec.\n",
            "iteration 15580 || Loss: 0.1229 || 20iter: 14.9274 sec.\n",
            "iteration 15600 || Loss: 0.0725 || 20iter: 14.9689 sec.\n",
            "iteration 15620 || Loss: 0.0962 || 20iter: 14.9299 sec.\n",
            "iteration 15640 || Loss: 0.1923 || 20iter: 14.8679 sec.\n",
            "iteration 15660 || Loss: 0.1234 || 20iter: 14.9797 sec.\n",
            "iteration 15680 || Loss: 0.0651 || 20iter: 14.9496 sec.\n",
            "iteration 15700 || Loss: 0.1194 || 20iter: 15.0131 sec.\n",
            "iteration 15720 || Loss: 0.1639 || 20iter: 14.9213 sec.\n",
            "--------------\n",
            "epoch 43 || Epoch_TRAIN_Loss: 0.1488 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.0595 sec.\n",
            "--------------\n",
            "Epoch 44/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 15740 || Loss: 0.3555 || 20iter: 1.0253 sec.\n",
            "iteration 15760 || Loss: 0.1767 || 20iter: 15.0308 sec.\n",
            "iteration 15780 || Loss: 0.2435 || 20iter: 14.9506 sec.\n",
            "iteration 15800 || Loss: 0.2670 || 20iter: 14.9668 sec.\n",
            "iteration 15820 || Loss: 0.1910 || 20iter: 14.8916 sec.\n",
            "iteration 15840 || Loss: 0.2186 || 20iter: 14.9363 sec.\n",
            "iteration 15860 || Loss: 0.0898 || 20iter: 14.9290 sec.\n",
            "iteration 15880 || Loss: 0.0683 || 20iter: 14.9309 sec.\n",
            "iteration 15900 || Loss: 0.1055 || 20iter: 14.9707 sec.\n",
            "iteration 15920 || Loss: 0.1527 || 20iter: 14.9585 sec.\n",
            "iteration 15940 || Loss: 0.2946 || 20iter: 14.9412 sec.\n",
            "iteration 15960 || Loss: 0.2803 || 20iter: 14.9096 sec.\n",
            "iteration 15980 || Loss: 0.0948 || 20iter: 14.9422 sec.\n",
            "iteration 16000 || Loss: 0.0709 || 20iter: 14.9730 sec.\n",
            "iteration 16020 || Loss: 0.2253 || 20iter: 14.9044 sec.\n",
            "iteration 16040 || Loss: 0.0535 || 20iter: 14.9445 sec.\n",
            "iteration 16060 || Loss: 0.2543 || 20iter: 14.9299 sec.\n",
            "iteration 16080 || Loss: 0.1956 || 20iter: 14.9180 sec.\n",
            "iteration 16100 || Loss: 0.0738 || 20iter: 14.9872 sec.\n",
            "--------------\n",
            "epoch 44 || Epoch_TRAIN_Loss: 0.1498 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.0630 sec.\n",
            "--------------\n",
            "Epoch 45/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 16120 || Loss: 0.0727 || 20iter: 11.8027 sec.\n",
            "iteration 16140 || Loss: 0.0696 || 20iter: 14.9487 sec.\n",
            "iteration 16160 || Loss: 0.0820 || 20iter: 15.0775 sec.\n",
            "iteration 16180 || Loss: 0.1286 || 20iter: 14.9485 sec.\n",
            "iteration 16200 || Loss: 0.1139 || 20iter: 14.9511 sec.\n",
            "iteration 16220 || Loss: 0.3460 || 20iter: 14.9629 sec.\n",
            "iteration 16240 || Loss: 0.1061 || 20iter: 14.9383 sec.\n",
            "iteration 16260 || Loss: 0.0764 || 20iter: 14.8913 sec.\n",
            "iteration 16280 || Loss: 0.0850 || 20iter: 14.9756 sec.\n",
            "iteration 16300 || Loss: 0.1107 || 20iter: 14.9096 sec.\n",
            "iteration 16320 || Loss: 0.0994 || 20iter: 14.9722 sec.\n",
            "iteration 16340 || Loss: 0.2133 || 20iter: 14.9517 sec.\n",
            "iteration 16360 || Loss: 0.0799 || 20iter: 14.9486 sec.\n",
            "iteration 16380 || Loss: 0.2307 || 20iter: 14.9221 sec.\n",
            "iteration 16400 || Loss: 0.2030 || 20iter: 14.9489 sec.\n",
            "iteration 16420 || Loss: 0.1140 || 20iter: 14.9699 sec.\n",
            "iteration 16440 || Loss: 0.0789 || 20iter: 14.8785 sec.\n",
            "iteration 16460 || Loss: 0.0972 || 20iter: 14.9070 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 45 || Epoch_TRAIN_Loss: 0.1454 || Epoch_VAL_Loss: 0.1905\n",
            "timer: 358.0043 sec.\n",
            "--------------\n",
            "Epoch 46/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 16480 || Loss: 0.1475 || 20iter: 7.2401 sec.\n",
            "iteration 16500 || Loss: 0.0694 || 20iter: 14.8947 sec.\n",
            "iteration 16520 || Loss: 0.0889 || 20iter: 14.9721 sec.\n",
            "iteration 16540 || Loss: 0.1427 || 20iter: 14.9383 sec.\n",
            "iteration 16560 || Loss: 0.1227 || 20iter: 14.9348 sec.\n",
            "iteration 16580 || Loss: 0.0574 || 20iter: 14.9015 sec.\n",
            "iteration 16600 || Loss: 0.3240 || 20iter: 14.9672 sec.\n",
            "iteration 16620 || Loss: 0.1691 || 20iter: 14.9407 sec.\n",
            "iteration 16640 || Loss: 0.2157 || 20iter: 14.9520 sec.\n",
            "iteration 16660 || Loss: 0.0630 || 20iter: 14.9550 sec.\n",
            "iteration 16680 || Loss: 0.1607 || 20iter: 14.9515 sec.\n",
            "iteration 16700 || Loss: 0.2079 || 20iter: 14.9164 sec.\n",
            "iteration 16720 || Loss: 0.0614 || 20iter: 14.9715 sec.\n",
            "iteration 16740 || Loss: 0.1723 || 20iter: 14.9957 sec.\n",
            "iteration 16760 || Loss: 0.2393 || 20iter: 14.9489 sec.\n",
            "iteration 16780 || Loss: 0.0711 || 20iter: 14.9255 sec.\n",
            "iteration 16800 || Loss: 0.1819 || 20iter: 14.9541 sec.\n",
            "iteration 16820 || Loss: 0.0398 || 20iter: 14.8937 sec.\n",
            "--------------\n",
            "epoch 46 || Epoch_TRAIN_Loss: 0.1450 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.9960 sec.\n",
            "--------------\n",
            "Epoch 47/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 16840 || Loss: 0.1254 || 20iter: 2.5837 sec.\n",
            "iteration 16860 || Loss: 0.2107 || 20iter: 15.0383 sec.\n",
            "iteration 16880 || Loss: 0.0913 || 20iter: 14.9595 sec.\n",
            "iteration 16900 || Loss: 0.1093 || 20iter: 14.9366 sec.\n",
            "iteration 16920 || Loss: 0.2140 || 20iter: 14.9245 sec.\n",
            "iteration 16940 || Loss: 0.0684 || 20iter: 14.9613 sec.\n",
            "iteration 16960 || Loss: 0.2096 || 20iter: 14.9480 sec.\n",
            "iteration 16980 || Loss: 0.2019 || 20iter: 14.9110 sec.\n",
            "iteration 17000 || Loss: 0.1996 || 20iter: 14.9276 sec.\n",
            "iteration 17020 || Loss: 0.1086 || 20iter: 14.9457 sec.\n",
            "iteration 17040 || Loss: 0.0658 || 20iter: 14.9747 sec.\n",
            "iteration 17060 || Loss: 0.2566 || 20iter: 14.9514 sec.\n",
            "iteration 17080 || Loss: 0.2045 || 20iter: 14.9250 sec.\n",
            "iteration 17100 || Loss: 0.1655 || 20iter: 14.9570 sec.\n",
            "iteration 17120 || Loss: 0.4182 || 20iter: 14.9272 sec.\n",
            "iteration 17140 || Loss: 0.1193 || 20iter: 14.9160 sec.\n",
            "iteration 17160 || Loss: 0.1691 || 20iter: 14.9677 sec.\n",
            "iteration 17180 || Loss: 0.1519 || 20iter: 14.9836 sec.\n",
            "iteration 17200 || Loss: 0.1802 || 20iter: 14.9373 sec.\n",
            "--------------\n",
            "epoch 47 || Epoch_TRAIN_Loss: 0.1436 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.1093 sec.\n",
            "--------------\n",
            "Epoch 48/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 17220 || Loss: 0.0662 || 20iter: 13.3579 sec.\n",
            "iteration 17240 || Loss: 0.1006 || 20iter: 14.8989 sec.\n",
            "iteration 17260 || Loss: 0.1351 || 20iter: 15.0608 sec.\n",
            "iteration 17280 || Loss: 0.0950 || 20iter: 15.0087 sec.\n",
            "iteration 17300 || Loss: 0.1532 || 20iter: 14.9405 sec.\n",
            "iteration 17320 || Loss: 0.0771 || 20iter: 14.9862 sec.\n",
            "iteration 17340 || Loss: 0.2523 || 20iter: 14.8833 sec.\n",
            "iteration 17360 || Loss: 0.0352 || 20iter: 14.9468 sec.\n",
            "iteration 17380 || Loss: 0.1444 || 20iter: 14.9143 sec.\n",
            "iteration 17400 || Loss: 0.0695 || 20iter: 14.8990 sec.\n",
            "iteration 17420 || Loss: 0.0666 || 20iter: 14.9992 sec.\n",
            "iteration 17440 || Loss: 0.2812 || 20iter: 14.9467 sec.\n",
            "iteration 17460 || Loss: 0.1846 || 20iter: 14.9727 sec.\n",
            "iteration 17480 || Loss: 0.0753 || 20iter: 14.9214 sec.\n",
            "iteration 17500 || Loss: 0.1089 || 20iter: 14.9337 sec.\n",
            "iteration 17520 || Loss: 0.1267 || 20iter: 14.9208 sec.\n",
            "iteration 17540 || Loss: 0.0880 || 20iter: 14.9407 sec.\n",
            "iteration 17560 || Loss: 0.0824 || 20iter: 14.9155 sec.\n",
            "--------------\n",
            "epoch 48 || Epoch_TRAIN_Loss: 0.1434 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.0604 sec.\n",
            "--------------\n",
            "Epoch 49/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 17580 || Loss: 0.0516 || 20iter: 8.7583 sec.\n",
            "iteration 17600 || Loss: 0.0834 || 20iter: 14.9128 sec.\n",
            "iteration 17620 || Loss: 0.1151 || 20iter: 14.9739 sec.\n",
            "iteration 17640 || Loss: 0.0695 || 20iter: 14.9094 sec.\n",
            "iteration 17660 || Loss: 0.3068 || 20iter: 14.9857 sec.\n",
            "iteration 17680 || Loss: 0.2592 || 20iter: 15.0207 sec.\n",
            "iteration 17700 || Loss: 0.1142 || 20iter: 14.9353 sec.\n",
            "iteration 17720 || Loss: 0.0975 || 20iter: 14.9390 sec.\n",
            "iteration 17740 || Loss: 0.1370 || 20iter: 14.9203 sec.\n",
            "iteration 17760 || Loss: 0.3432 || 20iter: 14.9540 sec.\n",
            "iteration 17780 || Loss: 0.0828 || 20iter: 14.9333 sec.\n",
            "iteration 17800 || Loss: 0.1119 || 20iter: 14.9538 sec.\n",
            "iteration 17820 || Loss: 0.3157 || 20iter: 14.8938 sec.\n",
            "iteration 17840 || Loss: 0.1296 || 20iter: 14.9208 sec.\n",
            "iteration 17860 || Loss: 0.0552 || 20iter: 14.9276 sec.\n",
            "iteration 17880 || Loss: 0.0764 || 20iter: 14.9799 sec.\n",
            "iteration 17900 || Loss: 0.1413 || 20iter: 14.9518 sec.\n",
            "iteration 17920 || Loss: 0.0382 || 20iter: 14.9604 sec.\n",
            "--------------\n",
            "epoch 49 || Epoch_TRAIN_Loss: 0.1413 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.0400 sec.\n",
            "--------------\n",
            "Epoch 50/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 17940 || Loss: 0.0874 || 20iter: 4.1458 sec.\n",
            "iteration 17960 || Loss: 0.0887 || 20iter: 14.8993 sec.\n",
            "iteration 17980 || Loss: 0.3373 || 20iter: 14.9816 sec.\n",
            "iteration 18000 || Loss: 0.1508 || 20iter: 14.9639 sec.\n",
            "iteration 18020 || Loss: 0.1402 || 20iter: 14.9553 sec.\n",
            "iteration 18040 || Loss: 0.2182 || 20iter: 14.9452 sec.\n",
            "iteration 18060 || Loss: 0.1734 || 20iter: 14.9680 sec.\n",
            "iteration 18080 || Loss: 0.1033 || 20iter: 15.0186 sec.\n",
            "iteration 18100 || Loss: 0.1712 || 20iter: 14.9944 sec.\n",
            "iteration 18120 || Loss: 0.1077 || 20iter: 14.8831 sec.\n",
            "iteration 18140 || Loss: 0.0560 || 20iter: 14.9809 sec.\n",
            "iteration 18160 || Loss: 0.0348 || 20iter: 14.9135 sec.\n",
            "iteration 18180 || Loss: 0.2120 || 20iter: 14.9894 sec.\n",
            "iteration 18200 || Loss: 0.0734 || 20iter: 14.9118 sec.\n",
            "iteration 18220 || Loss: 0.1645 || 20iter: 14.9883 sec.\n",
            "iteration 18240 || Loss: 0.1131 || 20iter: 14.9363 sec.\n",
            "iteration 18260 || Loss: 0.1475 || 20iter: 14.9695 sec.\n",
            "iteration 18280 || Loss: 0.0826 || 20iter: 14.9024 sec.\n",
            "iteration 18300 || Loss: 0.1128 || 20iter: 14.9613 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 50 || Epoch_TRAIN_Loss: 0.1434 || Epoch_VAL_Loss: 0.1793\n",
            "timer: 357.9391 sec.\n",
            "--------------\n",
            "Epoch 51/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 18320 || Loss: 0.1300 || 20iter: 14.9481 sec.\n",
            "iteration 18340 || Loss: 0.1439 || 20iter: 14.9388 sec.\n",
            "iteration 18360 || Loss: 0.1000 || 20iter: 14.9467 sec.\n",
            "iteration 18380 || Loss: 0.1819 || 20iter: 15.0041 sec.\n",
            "iteration 18400 || Loss: 0.2818 || 20iter: 14.9613 sec.\n",
            "iteration 18420 || Loss: 0.2278 || 20iter: 14.9125 sec.\n",
            "iteration 18440 || Loss: 0.1491 || 20iter: 14.9366 sec.\n",
            "iteration 18460 || Loss: 0.1343 || 20iter: 14.8874 sec.\n",
            "iteration 18480 || Loss: 0.1458 || 20iter: 14.9358 sec.\n",
            "iteration 18500 || Loss: 0.1414 || 20iter: 14.9722 sec.\n",
            "iteration 18520 || Loss: 0.0920 || 20iter: 14.9567 sec.\n",
            "iteration 18540 || Loss: 0.1103 || 20iter: 14.9107 sec.\n",
            "iteration 18560 || Loss: 0.0278 || 20iter: 14.9587 sec.\n",
            "iteration 18580 || Loss: 0.1500 || 20iter: 14.9196 sec.\n",
            "iteration 18600 || Loss: 0.1706 || 20iter: 14.8796 sec.\n",
            "iteration 18620 || Loss: 0.1216 || 20iter: 15.0000 sec.\n",
            "iteration 18640 || Loss: 0.1362 || 20iter: 14.9932 sec.\n",
            "iteration 18660 || Loss: 0.0880 || 20iter: 14.9765 sec.\n",
            "--------------\n",
            "epoch 51 || Epoch_TRAIN_Loss: 0.1377 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.0790 sec.\n",
            "--------------\n",
            "Epoch 52/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 18680 || Loss: 0.1298 || 20iter: 10.3088 sec.\n",
            "iteration 18700 || Loss: 0.0375 || 20iter: 14.9838 sec.\n",
            "iteration 18720 || Loss: 0.0601 || 20iter: 14.9871 sec.\n",
            "iteration 18740 || Loss: 0.4059 || 20iter: 14.9496 sec.\n",
            "iteration 18760 || Loss: 0.1482 || 20iter: 14.9694 sec.\n",
            "iteration 18780 || Loss: 0.1087 || 20iter: 15.1102 sec.\n",
            "iteration 18800 || Loss: 0.1140 || 20iter: 14.9049 sec.\n",
            "iteration 18820 || Loss: 0.2169 || 20iter: 14.9384 sec.\n",
            "iteration 18840 || Loss: 0.1054 || 20iter: 14.9553 sec.\n",
            "iteration 18860 || Loss: 0.0489 || 20iter: 14.9326 sec.\n",
            "iteration 18880 || Loss: 0.0487 || 20iter: 14.9250 sec.\n",
            "iteration 18900 || Loss: 0.0703 || 20iter: 14.9292 sec.\n",
            "iteration 18920 || Loss: 0.2306 || 20iter: 14.9215 sec.\n",
            "iteration 18940 || Loss: 0.1426 || 20iter: 14.9650 sec.\n",
            "iteration 18960 || Loss: 0.1181 || 20iter: 14.8733 sec.\n",
            "iteration 18980 || Loss: 0.0657 || 20iter: 14.9702 sec.\n",
            "iteration 19000 || Loss: 0.0998 || 20iter: 14.9783 sec.\n",
            "iteration 19020 || Loss: 0.1017 || 20iter: 14.9381 sec.\n",
            "--------------\n",
            "epoch 52 || Epoch_TRAIN_Loss: 0.1406 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.1635 sec.\n",
            "--------------\n",
            "Epoch 53/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 19040 || Loss: 0.0866 || 20iter: 5.6604 sec.\n",
            "iteration 19060 || Loss: 0.1405 || 20iter: 15.0185 sec.\n",
            "iteration 19080 || Loss: 0.0508 || 20iter: 14.8971 sec.\n",
            "iteration 19100 || Loss: 0.1464 || 20iter: 14.9630 sec.\n",
            "iteration 19120 || Loss: 0.0522 || 20iter: 14.9308 sec.\n",
            "iteration 19140 || Loss: 0.3275 || 20iter: 14.9973 sec.\n",
            "iteration 19160 || Loss: 0.1569 || 20iter: 14.9497 sec.\n",
            "iteration 19180 || Loss: 0.1074 || 20iter: 15.0035 sec.\n",
            "iteration 19200 || Loss: 0.0780 || 20iter: 14.8774 sec.\n",
            "iteration 19220 || Loss: 0.0881 || 20iter: 14.9542 sec.\n",
            "iteration 19240 || Loss: 0.0613 || 20iter: 14.9270 sec.\n",
            "iteration 19260 || Loss: 0.0654 || 20iter: 14.9648 sec.\n",
            "iteration 19280 || Loss: 0.1910 || 20iter: 14.9471 sec.\n",
            "iteration 19300 || Loss: 0.1395 || 20iter: 14.9707 sec.\n",
            "iteration 19320 || Loss: 0.1269 || 20iter: 14.9126 sec.\n",
            "iteration 19340 || Loss: 0.1477 || 20iter: 14.9635 sec.\n",
            "iteration 19360 || Loss: 0.2457 || 20iter: 14.9162 sec.\n",
            "iteration 19380 || Loss: 0.1439 || 20iter: 14.9525 sec.\n",
            "--------------\n",
            "epoch 53 || Epoch_TRAIN_Loss: 0.1369 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.1129 sec.\n",
            "--------------\n",
            "Epoch 54/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 19400 || Loss: 0.1833 || 20iter: 1.0385 sec.\n",
            "iteration 19420 || Loss: 0.1540 || 20iter: 14.9192 sec.\n",
            "iteration 19440 || Loss: 0.1415 || 20iter: 14.9244 sec.\n",
            "iteration 19460 || Loss: 0.1194 || 20iter: 14.9832 sec.\n",
            "iteration 19480 || Loss: 0.1695 || 20iter: 14.9194 sec.\n",
            "iteration 19500 || Loss: 0.1490 || 20iter: 14.9487 sec.\n",
            "iteration 19520 || Loss: 0.1645 || 20iter: 14.9127 sec.\n",
            "iteration 19540 || Loss: 0.2968 || 20iter: 14.9569 sec.\n",
            "iteration 19560 || Loss: 0.2502 || 20iter: 14.9828 sec.\n",
            "iteration 19580 || Loss: 0.1224 || 20iter: 15.1105 sec.\n",
            "iteration 19600 || Loss: 0.0399 || 20iter: 14.9057 sec.\n",
            "iteration 19620 || Loss: 0.0828 || 20iter: 14.9620 sec.\n",
            "iteration 19640 || Loss: 0.2440 || 20iter: 14.9899 sec.\n",
            "iteration 19660 || Loss: 0.2388 || 20iter: 14.9663 sec.\n",
            "iteration 19680 || Loss: 0.1997 || 20iter: 14.9461 sec.\n",
            "iteration 19700 || Loss: 0.1662 || 20iter: 14.9478 sec.\n",
            "iteration 19720 || Loss: 0.0838 || 20iter: 14.9388 sec.\n",
            "iteration 19740 || Loss: 0.2446 || 20iter: 14.9375 sec.\n",
            "iteration 19760 || Loss: 0.1569 || 20iter: 14.9955 sec.\n",
            "--------------\n",
            "epoch 54 || Epoch_TRAIN_Loss: 0.1363 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.2820 sec.\n",
            "--------------\n",
            "Epoch 55/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 19780 || Loss: 0.1300 || 20iter: 11.8861 sec.\n",
            "iteration 19800 || Loss: 0.1144 || 20iter: 14.9283 sec.\n",
            "iteration 19820 || Loss: 0.1113 || 20iter: 14.9773 sec.\n",
            "iteration 19840 || Loss: 0.0688 || 20iter: 14.9529 sec.\n",
            "iteration 19860 || Loss: 0.1632 || 20iter: 14.9672 sec.\n",
            "iteration 19880 || Loss: 0.1259 || 20iter: 14.9180 sec.\n",
            "iteration 19900 || Loss: 0.1001 || 20iter: 14.9618 sec.\n",
            "iteration 19920 || Loss: 0.0855 || 20iter: 14.9119 sec.\n",
            "iteration 19940 || Loss: 0.3337 || 20iter: 14.9092 sec.\n",
            "iteration 19960 || Loss: 0.1430 || 20iter: 14.9288 sec.\n",
            "iteration 19980 || Loss: 0.0969 || 20iter: 15.0230 sec.\n",
            "iteration 20000 || Loss: 0.1199 || 20iter: 14.9723 sec.\n",
            "iteration 20020 || Loss: 0.0992 || 20iter: 14.9419 sec.\n",
            "iteration 20040 || Loss: 0.1793 || 20iter: 14.9167 sec.\n",
            "iteration 20060 || Loss: 0.1299 || 20iter: 14.9432 sec.\n",
            "iteration 20080 || Loss: 0.1412 || 20iter: 14.9478 sec.\n",
            "iteration 20100 || Loss: 0.2350 || 20iter: 14.9796 sec.\n",
            "iteration 20120 || Loss: 0.1356 || 20iter: 14.9633 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 55 || Epoch_TRAIN_Loss: 0.1367 || Epoch_VAL_Loss: 0.1865\n",
            "timer: 357.7441 sec.\n",
            "--------------\n",
            "Epoch 56/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 20140 || Loss: 0.1241 || 20iter: 7.2129 sec.\n",
            "iteration 20160 || Loss: 0.1222 || 20iter: 14.9379 sec.\n",
            "iteration 20180 || Loss: 0.1429 || 20iter: 14.9745 sec.\n",
            "iteration 20200 || Loss: 0.1964 || 20iter: 14.9400 sec.\n",
            "iteration 20220 || Loss: 0.1294 || 20iter: 14.8977 sec.\n",
            "iteration 20240 || Loss: 0.1660 || 20iter: 14.9265 sec.\n",
            "iteration 20260 || Loss: 0.1964 || 20iter: 14.9367 sec.\n",
            "iteration 20280 || Loss: 0.0970 || 20iter: 14.9286 sec.\n",
            "iteration 20300 || Loss: 0.1347 || 20iter: 14.9431 sec.\n",
            "iteration 20320 || Loss: 0.1236 || 20iter: 14.9287 sec.\n",
            "iteration 20340 || Loss: 0.0569 || 20iter: 14.9128 sec.\n",
            "iteration 20360 || Loss: 0.0589 || 20iter: 14.9775 sec.\n",
            "iteration 20380 || Loss: 0.3046 || 20iter: 14.9140 sec.\n",
            "iteration 20400 || Loss: 0.1457 || 20iter: 14.9577 sec.\n",
            "iteration 20420 || Loss: 0.3536 || 20iter: 14.9030 sec.\n",
            "iteration 20440 || Loss: 0.1180 || 20iter: 14.9634 sec.\n",
            "iteration 20460 || Loss: 0.0593 || 20iter: 14.9177 sec.\n",
            "iteration 20480 || Loss: 0.1412 || 20iter: 14.9697 sec.\n",
            "--------------\n",
            "epoch 56 || Epoch_TRAIN_Loss: 0.1375 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.9194 sec.\n",
            "--------------\n",
            "Epoch 57/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 20500 || Loss: 0.0805 || 20iter: 2.5669 sec.\n",
            "iteration 20520 || Loss: 0.0711 || 20iter: 14.9675 sec.\n",
            "iteration 20540 || Loss: 0.1756 || 20iter: 14.9210 sec.\n",
            "iteration 20560 || Loss: 0.2657 || 20iter: 14.9508 sec.\n",
            "iteration 20580 || Loss: 0.1535 || 20iter: 14.9622 sec.\n",
            "iteration 20600 || Loss: 0.1368 || 20iter: 14.8961 sec.\n",
            "iteration 20620 || Loss: 0.0939 || 20iter: 14.9011 sec.\n",
            "iteration 20640 || Loss: 0.2392 || 20iter: 14.8963 sec.\n",
            "iteration 20660 || Loss: 0.1987 || 20iter: 14.8849 sec.\n",
            "iteration 20680 || Loss: 0.1688 || 20iter: 14.9825 sec.\n",
            "iteration 20700 || Loss: 0.1270 || 20iter: 15.0075 sec.\n",
            "iteration 20720 || Loss: 0.0442 || 20iter: 14.9027 sec.\n",
            "iteration 20740 || Loss: 0.0759 || 20iter: 14.8897 sec.\n",
            "iteration 20760 || Loss: 0.1457 || 20iter: 14.8766 sec.\n",
            "iteration 20780 || Loss: 0.1339 || 20iter: 14.9074 sec.\n",
            "iteration 20800 || Loss: 0.2776 || 20iter: 14.9726 sec.\n",
            "iteration 20820 || Loss: 0.0904 || 20iter: 14.9287 sec.\n",
            "iteration 20840 || Loss: 0.1425 || 20iter: 14.9381 sec.\n",
            "iteration 20860 || Loss: 0.2219 || 20iter: 14.9304 sec.\n",
            "--------------\n",
            "epoch 57 || Epoch_TRAIN_Loss: 0.1355 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.7233 sec.\n",
            "--------------\n",
            "Epoch 58/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 20880 || Loss: 0.2008 || 20iter: 13.3993 sec.\n",
            "iteration 20900 || Loss: 0.0867 || 20iter: 14.9607 sec.\n",
            "iteration 20920 || Loss: 0.0682 || 20iter: 14.9149 sec.\n",
            "iteration 20940 || Loss: 0.1932 || 20iter: 14.9561 sec.\n",
            "iteration 20960 || Loss: 0.1006 || 20iter: 14.8722 sec.\n",
            "iteration 20980 || Loss: 0.1175 || 20iter: 14.8888 sec.\n",
            "iteration 21000 || Loss: 0.1266 || 20iter: 14.9397 sec.\n",
            "iteration 21020 || Loss: 0.2118 || 20iter: 14.9622 sec.\n",
            "iteration 21040 || Loss: 0.2223 || 20iter: 14.8845 sec.\n",
            "iteration 21060 || Loss: 0.1728 || 20iter: 14.8864 sec.\n",
            "iteration 21080 || Loss: 0.1254 || 20iter: 14.9761 sec.\n",
            "iteration 21100 || Loss: 0.1453 || 20iter: 14.9968 sec.\n",
            "iteration 21120 || Loss: 0.1658 || 20iter: 14.9834 sec.\n",
            "iteration 21140 || Loss: 0.1196 || 20iter: 14.8873 sec.\n",
            "iteration 21160 || Loss: 0.0634 || 20iter: 14.9825 sec.\n",
            "iteration 21180 || Loss: 0.0609 || 20iter: 14.9169 sec.\n",
            "iteration 21200 || Loss: 0.2086 || 20iter: 14.9260 sec.\n",
            "iteration 21220 || Loss: 0.1054 || 20iter: 14.9568 sec.\n",
            "--------------\n",
            "epoch 58 || Epoch_TRAIN_Loss: 0.1319 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.8008 sec.\n",
            "--------------\n",
            "Epoch 59/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 21240 || Loss: 0.3901 || 20iter: 8.7769 sec.\n",
            "iteration 21260 || Loss: 0.1708 || 20iter: 14.8920 sec.\n",
            "iteration 21280 || Loss: 0.1851 || 20iter: 14.9394 sec.\n",
            "iteration 21300 || Loss: 0.0858 || 20iter: 14.9257 sec.\n",
            "iteration 21320 || Loss: 0.1771 || 20iter: 14.9530 sec.\n",
            "iteration 21340 || Loss: 0.1807 || 20iter: 14.9524 sec.\n",
            "iteration 21360 || Loss: 0.1101 || 20iter: 14.9238 sec.\n",
            "iteration 21380 || Loss: 0.0500 || 20iter: 14.8954 sec.\n",
            "iteration 21400 || Loss: 0.1773 || 20iter: 14.8829 sec.\n",
            "iteration 21420 || Loss: 0.1254 || 20iter: 14.9112 sec.\n",
            "iteration 21440 || Loss: 0.1269 || 20iter: 14.9612 sec.\n",
            "iteration 21460 || Loss: 0.1718 || 20iter: 14.8700 sec.\n",
            "iteration 21480 || Loss: 0.1319 || 20iter: 15.0013 sec.\n",
            "iteration 21500 || Loss: 0.1529 || 20iter: 15.0411 sec.\n",
            "iteration 21520 || Loss: 0.1228 || 20iter: 14.9088 sec.\n",
            "iteration 21540 || Loss: 0.2436 || 20iter: 14.9389 sec.\n",
            "iteration 21560 || Loss: 0.0346 || 20iter: 14.8857 sec.\n",
            "iteration 21580 || Loss: 0.2062 || 20iter: 14.8698 sec.\n",
            "--------------\n",
            "epoch 59 || Epoch_TRAIN_Loss: 0.1335 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.6921 sec.\n",
            "--------------\n",
            "Epoch 60/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 21600 || Loss: 0.1191 || 20iter: 4.1172 sec.\n",
            "iteration 21620 || Loss: 0.2041 || 20iter: 14.9174 sec.\n",
            "iteration 21640 || Loss: 0.3139 || 20iter: 14.9679 sec.\n",
            "iteration 21660 || Loss: 0.1924 || 20iter: 14.9642 sec.\n",
            "iteration 21680 || Loss: 0.1461 || 20iter: 14.9093 sec.\n",
            "iteration 21700 || Loss: 0.0756 || 20iter: 14.9091 sec.\n",
            "iteration 21720 || Loss: 0.0450 || 20iter: 14.9361 sec.\n",
            "iteration 21740 || Loss: 0.1574 || 20iter: 14.9488 sec.\n",
            "iteration 21760 || Loss: 0.1440 || 20iter: 14.9706 sec.\n",
            "iteration 21780 || Loss: 0.0474 || 20iter: 14.9117 sec.\n",
            "iteration 21800 || Loss: 0.0738 || 20iter: 14.8850 sec.\n",
            "iteration 21820 || Loss: 0.0670 || 20iter: 14.9314 sec.\n",
            "iteration 21840 || Loss: 0.1646 || 20iter: 14.9076 sec.\n",
            "iteration 21860 || Loss: 0.1920 || 20iter: 14.9278 sec.\n",
            "iteration 21880 || Loss: 0.3172 || 20iter: 14.9103 sec.\n",
            "iteration 21900 || Loss: 0.2494 || 20iter: 15.2903 sec.\n",
            "iteration 21920 || Loss: 0.0602 || 20iter: 14.9857 sec.\n",
            "iteration 21940 || Loss: 0.1262 || 20iter: 14.9540 sec.\n",
            "iteration 21960 || Loss: 0.0773 || 20iter: 14.9371 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 60 || Epoch_TRAIN_Loss: 0.1326 || Epoch_VAL_Loss: 0.1744\n",
            "timer: 357.6989 sec.\n",
            "--------------\n",
            "Epoch 61/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 21980 || Loss: 0.1590 || 20iter: 14.9211 sec.\n",
            "iteration 22000 || Loss: 0.1240 || 20iter: 14.9534 sec.\n",
            "iteration 22020 || Loss: 0.0930 || 20iter: 14.9210 sec.\n",
            "iteration 22040 || Loss: 0.1247 || 20iter: 14.9197 sec.\n",
            "iteration 22060 || Loss: 0.0795 || 20iter: 14.9293 sec.\n",
            "iteration 22080 || Loss: 0.1364 || 20iter: 14.9621 sec.\n",
            "iteration 22100 || Loss: 0.0643 || 20iter: 14.9264 sec.\n",
            "iteration 22120 || Loss: 0.1109 || 20iter: 14.9191 sec.\n",
            "iteration 22140 || Loss: 0.1025 || 20iter: 14.9463 sec.\n",
            "iteration 22160 || Loss: 0.1023 || 20iter: 14.9239 sec.\n",
            "iteration 22180 || Loss: 0.0677 || 20iter: 14.9136 sec.\n",
            "iteration 22200 || Loss: 0.1447 || 20iter: 14.9366 sec.\n",
            "iteration 22220 || Loss: 0.1076 || 20iter: 14.8942 sec.\n",
            "iteration 22240 || Loss: 0.1169 || 20iter: 14.9449 sec.\n",
            "iteration 22260 || Loss: 0.0867 || 20iter: 14.9224 sec.\n",
            "iteration 22280 || Loss: 0.1276 || 20iter: 14.9419 sec.\n",
            "iteration 22300 || Loss: 0.1039 || 20iter: 14.9228 sec.\n",
            "iteration 22320 || Loss: 0.4080 || 20iter: 14.9105 sec.\n",
            "--------------\n",
            "epoch 61 || Epoch_TRAIN_Loss: 0.1310 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.7192 sec.\n",
            "--------------\n",
            "Epoch 62/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 22340 || Loss: 0.0662 || 20iter: 10.3258 sec.\n",
            "iteration 22360 || Loss: 0.2591 || 20iter: 14.9636 sec.\n",
            "iteration 22380 || Loss: 0.1262 || 20iter: 14.9188 sec.\n",
            "iteration 22400 || Loss: 0.1601 || 20iter: 14.9468 sec.\n",
            "iteration 22420 || Loss: 0.0902 || 20iter: 14.9539 sec.\n",
            "iteration 22440 || Loss: 0.0847 || 20iter: 14.9248 sec.\n",
            "iteration 22460 || Loss: 0.0913 || 20iter: 14.9425 sec.\n",
            "iteration 22480 || Loss: 0.1889 || 20iter: 14.9521 sec.\n",
            "iteration 22500 || Loss: 0.1120 || 20iter: 14.9120 sec.\n",
            "iteration 22520 || Loss: 0.1180 || 20iter: 14.8503 sec.\n",
            "iteration 22540 || Loss: 0.0495 || 20iter: 14.8959 sec.\n",
            "iteration 22560 || Loss: 0.2413 || 20iter: 14.9024 sec.\n",
            "iteration 22580 || Loss: 0.0953 || 20iter: 14.9270 sec.\n",
            "iteration 22600 || Loss: 0.0857 || 20iter: 15.0306 sec.\n",
            "iteration 22620 || Loss: 0.0638 || 20iter: 14.9551 sec.\n",
            "iteration 22640 || Loss: 0.1015 || 20iter: 14.9393 sec.\n",
            "iteration 22660 || Loss: 0.3713 || 20iter: 14.9187 sec.\n",
            "iteration 22680 || Loss: 0.0831 || 20iter: 14.9334 sec.\n",
            "--------------\n",
            "epoch 62 || Epoch_TRAIN_Loss: 0.1283 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.8625 sec.\n",
            "--------------\n",
            "Epoch 63/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 22700 || Loss: 0.1120 || 20iter: 5.6694 sec.\n",
            "iteration 22720 || Loss: 0.2029 || 20iter: 14.9266 sec.\n",
            "iteration 22740 || Loss: 0.1179 || 20iter: 14.9278 sec.\n",
            "iteration 22760 || Loss: 0.1011 || 20iter: 14.8804 sec.\n",
            "iteration 22780 || Loss: 0.1114 || 20iter: 14.9352 sec.\n",
            "iteration 22800 || Loss: 0.1183 || 20iter: 14.9768 sec.\n",
            "iteration 22820 || Loss: 0.0525 || 20iter: 14.9793 sec.\n",
            "iteration 22840 || Loss: 0.0709 || 20iter: 14.9152 sec.\n",
            "iteration 22860 || Loss: 0.0687 || 20iter: 14.9054 sec.\n",
            "iteration 22880 || Loss: 0.1052 || 20iter: 14.9137 sec.\n",
            "iteration 22900 || Loss: 0.2377 || 20iter: 14.9450 sec.\n",
            "iteration 22920 || Loss: 0.0987 || 20iter: 14.8844 sec.\n",
            "iteration 22940 || Loss: 0.1426 || 20iter: 14.9105 sec.\n",
            "iteration 22960 || Loss: 0.0848 || 20iter: 14.9252 sec.\n",
            "iteration 22980 || Loss: 0.0612 || 20iter: 14.9824 sec.\n",
            "iteration 23000 || Loss: 0.2985 || 20iter: 15.0260 sec.\n",
            "iteration 23020 || Loss: 0.1633 || 20iter: 14.9277 sec.\n",
            "iteration 23040 || Loss: 0.0862 || 20iter: 14.9305 sec.\n",
            "--------------\n",
            "epoch 63 || Epoch_TRAIN_Loss: 0.1259 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.8273 sec.\n",
            "--------------\n",
            "Epoch 64/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 23060 || Loss: 0.1236 || 20iter: 1.0294 sec.\n",
            "iteration 23080 || Loss: 0.1983 || 20iter: 14.9840 sec.\n",
            "iteration 23100 || Loss: 0.2648 || 20iter: 14.9628 sec.\n",
            "iteration 23120 || Loss: 0.2979 || 20iter: 14.9849 sec.\n",
            "iteration 23140 || Loss: 0.1213 || 20iter: 14.9785 sec.\n",
            "iteration 23160 || Loss: 0.0246 || 20iter: 14.9226 sec.\n",
            "iteration 23180 || Loss: 0.1405 || 20iter: 14.9065 sec.\n",
            "iteration 23200 || Loss: 0.1016 || 20iter: 14.9219 sec.\n",
            "iteration 23220 || Loss: 0.0446 || 20iter: 15.0008 sec.\n",
            "iteration 23240 || Loss: 0.0348 || 20iter: 14.9598 sec.\n",
            "iteration 23260 || Loss: 0.1618 || 20iter: 14.9561 sec.\n",
            "iteration 23280 || Loss: 0.0907 || 20iter: 14.9413 sec.\n",
            "iteration 23300 || Loss: 0.0578 || 20iter: 14.9638 sec.\n",
            "iteration 23320 || Loss: 0.1436 || 20iter: 14.9643 sec.\n",
            "iteration 23340 || Loss: 0.1069 || 20iter: 14.8735 sec.\n",
            "iteration 23360 || Loss: 0.0726 || 20iter: 14.9067 sec.\n",
            "iteration 23380 || Loss: 0.1245 || 20iter: 14.9220 sec.\n",
            "iteration 23400 || Loss: 0.1195 || 20iter: 14.9222 sec.\n",
            "iteration 23420 || Loss: 0.1379 || 20iter: 14.9622 sec.\n",
            "--------------\n",
            "epoch 64 || Epoch_TRAIN_Loss: 0.1239 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 283.0695 sec.\n",
            "--------------\n",
            "Epoch 65/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 23440 || Loss: 0.0993 || 20iter: 11.8421 sec.\n",
            "iteration 23460 || Loss: 0.1686 || 20iter: 14.9071 sec.\n",
            "iteration 23480 || Loss: 0.0928 || 20iter: 14.9322 sec.\n",
            "iteration 23500 || Loss: 0.1371 || 20iter: 14.9377 sec.\n",
            "iteration 23520 || Loss: 0.1223 || 20iter: 14.9640 sec.\n",
            "iteration 23540 || Loss: 0.1189 || 20iter: 14.9022 sec.\n",
            "iteration 23560 || Loss: 0.2712 || 20iter: 14.9094 sec.\n",
            "iteration 23580 || Loss: 0.0818 || 20iter: 14.9215 sec.\n",
            "iteration 23600 || Loss: 0.1377 || 20iter: 14.9510 sec.\n",
            "iteration 23620 || Loss: 0.1544 || 20iter: 14.8842 sec.\n",
            "iteration 23640 || Loss: 0.2493 || 20iter: 14.9149 sec.\n",
            "iteration 23660 || Loss: 0.0704 || 20iter: 14.8988 sec.\n",
            "iteration 23680 || Loss: 0.0492 || 20iter: 14.9219 sec.\n",
            "iteration 23700 || Loss: 0.0837 || 20iter: 14.9161 sec.\n",
            "iteration 23720 || Loss: 0.1708 || 20iter: 14.9758 sec.\n",
            "iteration 23740 || Loss: 0.2948 || 20iter: 15.0228 sec.\n",
            "iteration 23760 || Loss: 0.3379 || 20iter: 14.8818 sec.\n",
            "iteration 23780 || Loss: 0.0746 || 20iter: 14.9594 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 65 || Epoch_TRAIN_Loss: 0.1306 || Epoch_VAL_Loss: 0.1783\n",
            "timer: 357.4659 sec.\n",
            "--------------\n",
            "Epoch 66/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 23800 || Loss: 0.0893 || 20iter: 7.1891 sec.\n",
            "iteration 23820 || Loss: 0.1156 || 20iter: 14.9045 sec.\n",
            "iteration 23840 || Loss: 0.0681 || 20iter: 14.9203 sec.\n",
            "iteration 23860 || Loss: 0.1025 || 20iter: 14.9340 sec.\n",
            "iteration 23880 || Loss: 0.1957 || 20iter: 14.9329 sec.\n",
            "iteration 23900 || Loss: 0.0624 || 20iter: 14.9639 sec.\n",
            "iteration 23920 || Loss: 0.1207 || 20iter: 14.8832 sec.\n",
            "iteration 23940 || Loss: 0.0765 || 20iter: 14.9994 sec.\n",
            "iteration 23960 || Loss: 0.2526 || 20iter: 14.9664 sec.\n",
            "iteration 23980 || Loss: 0.1861 || 20iter: 14.9553 sec.\n",
            "iteration 24000 || Loss: 0.1206 || 20iter: 14.9297 sec.\n",
            "iteration 24020 || Loss: 0.1248 || 20iter: 14.8854 sec.\n",
            "iteration 24040 || Loss: 0.2225 || 20iter: 14.9055 sec.\n",
            "iteration 24060 || Loss: 0.0657 || 20iter: 14.9693 sec.\n",
            "iteration 24080 || Loss: 0.0442 || 20iter: 14.9270 sec.\n",
            "iteration 24100 || Loss: 0.0805 || 20iter: 14.9082 sec.\n",
            "iteration 24120 || Loss: 0.2457 || 20iter: 14.9833 sec.\n",
            "iteration 24140 || Loss: 0.0687 || 20iter: 14.9351 sec.\n",
            "--------------\n",
            "epoch 66 || Epoch_TRAIN_Loss: 0.1273 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.8470 sec.\n",
            "--------------\n",
            "Epoch 67/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 24160 || Loss: 0.1304 || 20iter: 2.5574 sec.\n",
            "iteration 24180 || Loss: 0.0712 || 20iter: 14.8853 sec.\n",
            "iteration 24200 || Loss: 0.0400 || 20iter: 14.8959 sec.\n",
            "iteration 24220 || Loss: 0.1666 || 20iter: 14.9123 sec.\n",
            "iteration 24240 || Loss: 0.1404 || 20iter: 14.9176 sec.\n",
            "iteration 24260 || Loss: 0.1571 || 20iter: 14.9389 sec.\n",
            "iteration 24280 || Loss: 0.0766 || 20iter: 14.9435 sec.\n",
            "iteration 24300 || Loss: 0.1855 || 20iter: 15.0131 sec.\n",
            "iteration 24320 || Loss: 0.1233 || 20iter: 14.9439 sec.\n",
            "iteration 24340 || Loss: 0.0906 || 20iter: 14.9561 sec.\n",
            "iteration 24360 || Loss: 0.1939 || 20iter: 14.9629 sec.\n",
            "iteration 24380 || Loss: 0.0995 || 20iter: 14.8934 sec.\n",
            "iteration 24400 || Loss: 0.0810 || 20iter: 14.9087 sec.\n",
            "iteration 24420 || Loss: 0.0682 || 20iter: 14.8866 sec.\n",
            "iteration 24440 || Loss: 0.1765 || 20iter: 14.9259 sec.\n",
            "iteration 24460 || Loss: 0.1030 || 20iter: 14.9096 sec.\n",
            "iteration 24480 || Loss: 0.0997 || 20iter: 14.9028 sec.\n",
            "iteration 24500 || Loss: 0.1386 || 20iter: 14.9290 sec.\n",
            "iteration 24520 || Loss: 0.0379 || 20iter: 14.9613 sec.\n",
            "--------------\n",
            "epoch 67 || Epoch_TRAIN_Loss: 0.1207 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.6953 sec.\n",
            "--------------\n",
            "Epoch 68/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 24540 || Loss: 0.0507 || 20iter: 13.3752 sec.\n",
            "iteration 24560 || Loss: 0.0885 || 20iter: 14.9248 sec.\n",
            "iteration 24580 || Loss: 0.0936 || 20iter: 14.9146 sec.\n",
            "iteration 24600 || Loss: 0.0634 || 20iter: 14.9101 sec.\n",
            "iteration 24620 || Loss: 0.1759 || 20iter: 14.8750 sec.\n",
            "iteration 24640 || Loss: 0.2377 || 20iter: 14.9799 sec.\n",
            "iteration 24660 || Loss: 0.1595 || 20iter: 14.9424 sec.\n",
            "iteration 24680 || Loss: 0.2201 || 20iter: 14.9620 sec.\n",
            "iteration 24700 || Loss: 0.0884 || 20iter: 14.9523 sec.\n",
            "iteration 24720 || Loss: 0.1554 || 20iter: 14.9171 sec.\n",
            "iteration 24740 || Loss: 0.0468 || 20iter: 14.9041 sec.\n",
            "iteration 24760 || Loss: 0.0906 || 20iter: 14.9663 sec.\n",
            "iteration 24780 || Loss: 0.1269 || 20iter: 14.9678 sec.\n",
            "iteration 24800 || Loss: 0.0836 || 20iter: 14.8782 sec.\n",
            "iteration 24820 || Loss: 0.1678 || 20iter: 14.9214 sec.\n",
            "iteration 24840 || Loss: 0.1986 || 20iter: 14.9202 sec.\n",
            "iteration 24860 || Loss: 0.0754 || 20iter: 14.9370 sec.\n",
            "iteration 24880 || Loss: 0.0779 || 20iter: 14.9231 sec.\n",
            "--------------\n",
            "epoch 68 || Epoch_TRAIN_Loss: 0.1215 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.7245 sec.\n",
            "--------------\n",
            "Epoch 69/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 24900 || Loss: 0.2221 || 20iter: 8.7647 sec.\n",
            "iteration 24920 || Loss: 0.0829 || 20iter: 15.0274 sec.\n",
            "iteration 24940 || Loss: 0.1491 || 20iter: 14.9565 sec.\n",
            "iteration 24960 || Loss: 0.0481 || 20iter: 14.9224 sec.\n",
            "iteration 24980 || Loss: 0.0706 || 20iter: 14.9204 sec.\n",
            "iteration 25000 || Loss: 0.1099 || 20iter: 14.8906 sec.\n",
            "iteration 25020 || Loss: 0.1981 || 20iter: 14.9268 sec.\n",
            "iteration 25040 || Loss: 0.0634 || 20iter: 14.8747 sec.\n",
            "iteration 25060 || Loss: 0.1628 || 20iter: 14.9361 sec.\n",
            "iteration 25080 || Loss: 0.3503 || 20iter: 14.9496 sec.\n",
            "iteration 25100 || Loss: 0.0870 || 20iter: 14.9719 sec.\n",
            "iteration 25120 || Loss: 0.1591 || 20iter: 14.9494 sec.\n",
            "iteration 25140 || Loss: 0.1066 || 20iter: 14.8931 sec.\n",
            "iteration 25160 || Loss: 0.0754 || 20iter: 14.9156 sec.\n",
            "iteration 25180 || Loss: 0.1151 || 20iter: 14.9206 sec.\n",
            "iteration 25200 || Loss: 0.1963 || 20iter: 14.9286 sec.\n",
            "iteration 25220 || Loss: 0.0943 || 20iter: 14.9264 sec.\n",
            "iteration 25240 || Loss: 0.1299 || 20iter: 14.9694 sec.\n",
            "--------------\n",
            "epoch 69 || Epoch_TRAIN_Loss: 0.1252 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.8871 sec.\n",
            "--------------\n",
            "Epoch 70/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 25260 || Loss: 0.1514 || 20iter: 4.1266 sec.\n",
            "iteration 25280 || Loss: 0.0884 || 20iter: 14.9305 sec.\n",
            "iteration 25300 || Loss: 0.0707 || 20iter: 14.9815 sec.\n",
            "iteration 25320 || Loss: 0.1103 || 20iter: 15.0102 sec.\n",
            "iteration 25340 || Loss: 0.0646 || 20iter: 14.9412 sec.\n",
            "iteration 25360 || Loss: 0.1306 || 20iter: 14.9423 sec.\n",
            "iteration 25380 || Loss: 0.1392 || 20iter: 14.8947 sec.\n",
            "iteration 25400 || Loss: 0.1095 || 20iter: 14.9166 sec.\n",
            "iteration 25420 || Loss: 0.1081 || 20iter: 14.9089 sec.\n",
            "iteration 25440 || Loss: 0.0831 || 20iter: 14.9206 sec.\n",
            "iteration 25460 || Loss: 0.0764 || 20iter: 14.9215 sec.\n",
            "iteration 25480 || Loss: 0.1373 || 20iter: 14.9429 sec.\n",
            "iteration 25500 || Loss: 0.1005 || 20iter: 14.9583 sec.\n",
            "iteration 25520 || Loss: 0.3096 || 20iter: 14.9335 sec.\n",
            "iteration 25540 || Loss: 0.0596 || 20iter: 14.8801 sec.\n",
            "iteration 25560 || Loss: 0.1018 || 20iter: 14.8490 sec.\n",
            "iteration 25580 || Loss: 0.1052 || 20iter: 14.9058 sec.\n",
            "iteration 25600 || Loss: 0.2052 || 20iter: 14.9578 sec.\n",
            "iteration 25620 || Loss: 0.0534 || 20iter: 14.9588 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 70 || Epoch_TRAIN_Loss: 0.1210 || Epoch_VAL_Loss: 0.1747\n",
            "timer: 357.3618 sec.\n",
            "--------------\n",
            "Epoch 71/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 25640 || Loss: 0.3746 || 20iter: 14.9913 sec.\n",
            "iteration 25660 || Loss: 0.0575 || 20iter: 14.9142 sec.\n",
            "iteration 25680 || Loss: 0.0505 || 20iter: 14.9051 sec.\n",
            "iteration 25700 || Loss: 0.0812 || 20iter: 14.8916 sec.\n",
            "iteration 25720 || Loss: 0.0805 || 20iter: 14.9762 sec.\n",
            "iteration 25740 || Loss: 0.0937 || 20iter: 14.9281 sec.\n",
            "iteration 25760 || Loss: 0.0905 || 20iter: 14.9145 sec.\n",
            "iteration 25780 || Loss: 0.0618 || 20iter: 14.9801 sec.\n",
            "iteration 25800 || Loss: 0.0813 || 20iter: 14.9032 sec.\n",
            "iteration 25820 || Loss: 0.1744 || 20iter: 14.9711 sec.\n",
            "iteration 25840 || Loss: 0.0680 || 20iter: 14.8782 sec.\n",
            "iteration 25860 || Loss: 0.1112 || 20iter: 14.8628 sec.\n",
            "iteration 25880 || Loss: 0.1669 || 20iter: 14.9088 sec.\n",
            "iteration 25900 || Loss: 0.0664 || 20iter: 14.9313 sec.\n",
            "iteration 25920 || Loss: 0.0341 || 20iter: 14.8170 sec.\n",
            "iteration 25940 || Loss: 0.0645 || 20iter: 14.9184 sec.\n",
            "iteration 25960 || Loss: 0.0415 || 20iter: 14.8811 sec.\n",
            "iteration 25980 || Loss: 0.0751 || 20iter: 14.9063 sec.\n",
            "--------------\n",
            "epoch 71 || Epoch_TRAIN_Loss: 0.1208 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.4753 sec.\n",
            "--------------\n",
            "Epoch 72/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 26000 || Loss: 0.1114 || 20iter: 10.3032 sec.\n",
            "iteration 26020 || Loss: 0.1673 || 20iter: 14.9478 sec.\n",
            "iteration 26040 || Loss: 0.2112 || 20iter: 14.9944 sec.\n",
            "iteration 26060 || Loss: 0.0783 || 20iter: 14.9194 sec.\n",
            "iteration 26080 || Loss: 0.0936 || 20iter: 14.8665 sec.\n",
            "iteration 26100 || Loss: 0.0850 || 20iter: 14.9039 sec.\n",
            "iteration 26120 || Loss: 0.1276 || 20iter: 14.9086 sec.\n",
            "iteration 26140 || Loss: 0.1245 || 20iter: 14.9549 sec.\n",
            "iteration 26160 || Loss: 0.1240 || 20iter: 14.9080 sec.\n",
            "iteration 26180 || Loss: 0.0864 || 20iter: 14.9654 sec.\n",
            "iteration 26200 || Loss: 0.0635 || 20iter: 14.9139 sec.\n",
            "iteration 26220 || Loss: 0.1340 || 20iter: 14.8702 sec.\n",
            "iteration 26240 || Loss: 0.1777 || 20iter: 14.9363 sec.\n",
            "iteration 26260 || Loss: 0.1592 || 20iter: 14.9227 sec.\n",
            "iteration 26280 || Loss: 0.1309 || 20iter: 14.9129 sec.\n",
            "iteration 26300 || Loss: 0.1828 || 20iter: 14.9046 sec.\n",
            "iteration 26320 || Loss: 0.0935 || 20iter: 14.8915 sec.\n",
            "iteration 26340 || Loss: 0.0595 || 20iter: 14.8921 sec.\n",
            "--------------\n",
            "epoch 72 || Epoch_TRAIN_Loss: 0.1211 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.5469 sec.\n",
            "--------------\n",
            "Epoch 73/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 26360 || Loss: 0.1187 || 20iter: 5.6615 sec.\n",
            "iteration 26380 || Loss: 0.1334 || 20iter: 14.9434 sec.\n",
            "iteration 26400 || Loss: 0.1664 || 20iter: 14.9014 sec.\n",
            "iteration 26420 || Loss: 0.1003 || 20iter: 14.9206 sec.\n",
            "iteration 26440 || Loss: 0.1266 || 20iter: 15.0200 sec.\n",
            "iteration 26460 || Loss: 0.1234 || 20iter: 14.9582 sec.\n",
            "iteration 26480 || Loss: 0.0690 || 20iter: 14.8568 sec.\n",
            "iteration 26500 || Loss: 0.1019 || 20iter: 14.9038 sec.\n",
            "iteration 26520 || Loss: 0.1101 || 20iter: 14.8928 sec.\n",
            "iteration 26540 || Loss: 0.1078 || 20iter: 14.9260 sec.\n",
            "iteration 26560 || Loss: 0.1160 || 20iter: 14.9555 sec.\n",
            "iteration 26580 || Loss: 0.0883 || 20iter: 15.0096 sec.\n",
            "iteration 26600 || Loss: 0.1640 || 20iter: 14.8559 sec.\n",
            "iteration 26620 || Loss: 0.0657 || 20iter: 14.9168 sec.\n",
            "iteration 26640 || Loss: 0.0784 || 20iter: 14.8434 sec.\n",
            "iteration 26660 || Loss: 0.0778 || 20iter: 14.9454 sec.\n",
            "iteration 26680 || Loss: 0.3455 || 20iter: 14.9126 sec.\n",
            "iteration 26700 || Loss: 0.1297 || 20iter: 14.9291 sec.\n",
            "--------------\n",
            "epoch 73 || Epoch_TRAIN_Loss: 0.1192 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.6570 sec.\n",
            "--------------\n",
            "Epoch 74/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 26720 || Loss: 0.0706 || 20iter: 1.0258 sec.\n",
            "iteration 26740 || Loss: 0.0655 || 20iter: 14.9557 sec.\n",
            "iteration 26760 || Loss: 0.1260 || 20iter: 14.8812 sec.\n",
            "iteration 26780 || Loss: 0.1164 || 20iter: 14.8966 sec.\n",
            "iteration 26800 || Loss: 0.1343 || 20iter: 14.9194 sec.\n",
            "iteration 26820 || Loss: 0.1208 || 20iter: 14.8899 sec.\n",
            "iteration 26840 || Loss: 0.0684 || 20iter: 15.0054 sec.\n",
            "iteration 26860 || Loss: 0.1308 || 20iter: 14.8902 sec.\n",
            "iteration 26880 || Loss: 0.1522 || 20iter: 14.9746 sec.\n",
            "iteration 26900 || Loss: 0.1380 || 20iter: 14.9228 sec.\n",
            "iteration 26920 || Loss: 0.0545 || 20iter: 14.9009 sec.\n",
            "iteration 26940 || Loss: 0.0849 || 20iter: 14.9221 sec.\n",
            "iteration 26960 || Loss: 0.1945 || 20iter: 14.8994 sec.\n",
            "iteration 26980 || Loss: 0.0699 || 20iter: 14.9200 sec.\n",
            "iteration 27000 || Loss: 0.0434 || 20iter: 14.9041 sec.\n",
            "iteration 27020 || Loss: 0.0947 || 20iter: 14.9236 sec.\n",
            "iteration 27040 || Loss: 0.0761 || 20iter: 14.9312 sec.\n",
            "iteration 27060 || Loss: 0.0872 || 20iter: 14.8982 sec.\n",
            "iteration 27080 || Loss: 0.2251 || 20iter: 14.9739 sec.\n",
            "--------------\n",
            "epoch 74 || Epoch_TRAIN_Loss: 0.1209 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.6292 sec.\n",
            "--------------\n",
            "Epoch 75/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 27100 || Loss: 0.1260 || 20iter: 11.8251 sec.\n",
            "iteration 27120 || Loss: 0.1547 || 20iter: 14.9289 sec.\n",
            "iteration 27140 || Loss: 0.1210 || 20iter: 14.8679 sec.\n",
            "iteration 27160 || Loss: 0.0882 || 20iter: 14.9445 sec.\n",
            "iteration 27180 || Loss: 0.1842 || 20iter: 14.9942 sec.\n",
            "iteration 27200 || Loss: 0.1033 || 20iter: 14.9232 sec.\n",
            "iteration 27220 || Loss: 0.0536 || 20iter: 14.9047 sec.\n",
            "iteration 27240 || Loss: 0.1304 || 20iter: 14.9376 sec.\n",
            "iteration 27260 || Loss: 0.0894 || 20iter: 14.8879 sec.\n",
            "iteration 27280 || Loss: 0.1482 || 20iter: 14.9629 sec.\n",
            "iteration 27300 || Loss: 0.0849 || 20iter: 14.9893 sec.\n",
            "iteration 27320 || Loss: 0.0653 || 20iter: 14.8842 sec.\n",
            "iteration 27340 || Loss: 0.0532 || 20iter: 14.8914 sec.\n",
            "iteration 27360 || Loss: 0.0896 || 20iter: 14.9361 sec.\n",
            "iteration 27380 || Loss: 0.3442 || 20iter: 14.9005 sec.\n",
            "iteration 27400 || Loss: 0.1597 || 20iter: 14.9238 sec.\n",
            "iteration 27420 || Loss: 0.0185 || 20iter: 14.9499 sec.\n",
            "iteration 27440 || Loss: 0.0623 || 20iter: 14.9370 sec.\n",
            "--------------\n",
            "(val)\n",
            "--------------\n",
            "epoch 75 || Epoch_TRAIN_Loss: 0.1158 || Epoch_VAL_Loss: 0.1712\n",
            "timer: 357.3115 sec.\n",
            "--------------\n",
            "Epoch 76/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 27460 || Loss: 0.0404 || 20iter: 7.2166 sec.\n",
            "iteration 27480 || Loss: 0.1874 || 20iter: 14.9152 sec.\n",
            "iteration 27500 || Loss: 0.0544 || 20iter: 14.9616 sec.\n",
            "iteration 27520 || Loss: 0.1523 || 20iter: 14.9079 sec.\n",
            "iteration 27540 || Loss: 0.0719 || 20iter: 15.0447 sec.\n",
            "iteration 27560 || Loss: 0.1176 || 20iter: 14.9383 sec.\n",
            "iteration 27580 || Loss: 0.1846 || 20iter: 14.9355 sec.\n",
            "iteration 27600 || Loss: 0.0996 || 20iter: 14.9521 sec.\n",
            "iteration 27620 || Loss: 0.0497 || 20iter: 14.9409 sec.\n",
            "iteration 27640 || Loss: 0.1711 || 20iter: 14.9568 sec.\n",
            "iteration 27660 || Loss: 0.0885 || 20iter: 14.9832 sec.\n",
            "iteration 27680 || Loss: 0.1066 || 20iter: 14.9079 sec.\n",
            "iteration 27700 || Loss: 0.2587 || 20iter: 14.8988 sec.\n",
            "iteration 27720 || Loss: 0.0987 || 20iter: 14.9529 sec.\n",
            "iteration 27740 || Loss: 0.0882 || 20iter: 14.9405 sec.\n",
            "iteration 27760 || Loss: 0.1482 || 20iter: 14.9331 sec.\n",
            "iteration 27780 || Loss: 0.2027 || 20iter: 14.9218 sec.\n",
            "iteration 27800 || Loss: 0.0889 || 20iter: 14.9184 sec.\n",
            "--------------\n",
            "epoch 76 || Epoch_TRAIN_Loss: 0.1158 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.9611 sec.\n",
            "--------------\n",
            "Epoch 77/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 27820 || Loss: 0.1793 || 20iter: 2.5767 sec.\n",
            "iteration 27840 || Loss: 0.1722 || 20iter: 14.9077 sec.\n",
            "iteration 27860 || Loss: 0.1173 || 20iter: 14.9009 sec.\n",
            "iteration 27880 || Loss: 0.0557 || 20iter: 14.9188 sec.\n",
            "iteration 27900 || Loss: 0.1369 || 20iter: 14.9016 sec.\n",
            "iteration 27920 || Loss: 0.1663 || 20iter: 15.0103 sec.\n",
            "iteration 27940 || Loss: 0.1292 || 20iter: 14.9949 sec.\n",
            "iteration 27960 || Loss: 0.1788 || 20iter: 14.9385 sec.\n",
            "iteration 27980 || Loss: 0.2209 || 20iter: 14.9303 sec.\n",
            "iteration 28000 || Loss: 0.1013 || 20iter: 14.9509 sec.\n",
            "iteration 28020 || Loss: 0.0672 || 20iter: 14.8911 sec.\n",
            "iteration 28040 || Loss: 0.1002 || 20iter: 14.9403 sec.\n",
            "iteration 28060 || Loss: 0.1542 || 20iter: 14.9738 sec.\n",
            "iteration 28080 || Loss: 0.1182 || 20iter: 14.8426 sec.\n",
            "iteration 28100 || Loss: 0.1292 || 20iter: 14.8856 sec.\n",
            "iteration 28120 || Loss: 0.0666 || 20iter: 14.9386 sec.\n",
            "iteration 28140 || Loss: 0.0785 || 20iter: 14.9152 sec.\n",
            "iteration 28160 || Loss: 0.0594 || 20iter: 14.9037 sec.\n",
            "iteration 28180 || Loss: 0.1223 || 20iter: 14.8869 sec.\n",
            "--------------\n",
            "epoch 77 || Epoch_TRAIN_Loss: 0.1157 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.6526 sec.\n",
            "--------------\n",
            "Epoch 78/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 28200 || Loss: 0.0656 || 20iter: 13.4130 sec.\n",
            "iteration 28220 || Loss: 0.1725 || 20iter: 14.9347 sec.\n",
            "iteration 28240 || Loss: 0.0906 || 20iter: 14.9633 sec.\n",
            "iteration 28260 || Loss: 0.1431 || 20iter: 14.9167 sec.\n",
            "iteration 28280 || Loss: 0.1693 || 20iter: 14.9298 sec.\n",
            "iteration 28300 || Loss: 0.0779 || 20iter: 14.8694 sec.\n",
            "iteration 28320 || Loss: 0.0529 || 20iter: 14.9416 sec.\n",
            "iteration 28340 || Loss: 0.1512 || 20iter: 15.0039 sec.\n",
            "iteration 28360 || Loss: 0.1374 || 20iter: 14.9703 sec.\n",
            "iteration 28380 || Loss: 0.1184 || 20iter: 14.8760 sec.\n",
            "iteration 28400 || Loss: 0.1152 || 20iter: 14.9024 sec.\n",
            "iteration 28420 || Loss: 0.1092 || 20iter: 14.9475 sec.\n",
            "iteration 28440 || Loss: 0.1122 || 20iter: 14.9624 sec.\n",
            "iteration 28460 || Loss: 0.0897 || 20iter: 14.9646 sec.\n",
            "iteration 28480 || Loss: 0.0273 || 20iter: 14.9083 sec.\n",
            "iteration 28500 || Loss: 0.0506 || 20iter: 14.9120 sec.\n",
            "iteration 28520 || Loss: 0.0328 || 20iter: 14.9114 sec.\n",
            "iteration 28540 || Loss: 0.1063 || 20iter: 14.9183 sec.\n",
            "--------------\n",
            "epoch 78 || Epoch_TRAIN_Loss: 0.1132 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.7581 sec.\n",
            "--------------\n",
            "Epoch 79/80\n",
            "--------------\n",
            "(train)\n",
            "iteration 28560 || Loss: 0.1477 || 20iter: 8.7361 sec.\n",
            "iteration 28580 || Loss: 0.1268 || 20iter: 14.9039 sec.\n",
            "iteration 28600 || Loss: 0.0723 || 20iter: 14.8855 sec.\n",
            "iteration 28620 || Loss: 0.1094 || 20iter: 14.9148 sec.\n",
            "iteration 28640 || Loss: 0.0192 || 20iter: 14.9160 sec.\n",
            "iteration 28660 || Loss: 0.2355 || 20iter: 14.9339 sec.\n",
            "iteration 28680 || Loss: 0.1131 || 20iter: 14.9052 sec.\n",
            "iteration 28700 || Loss: 0.1557 || 20iter: 14.8593 sec.\n",
            "iteration 28720 || Loss: 0.0646 || 20iter: 14.9149 sec.\n",
            "iteration 28740 || Loss: 0.0459 || 20iter: 14.9820 sec.\n",
            "iteration 28760 || Loss: 0.1420 || 20iter: 15.0207 sec.\n",
            "iteration 28780 || Loss: 0.0968 || 20iter: 14.9190 sec.\n",
            "iteration 28800 || Loss: 0.1092 || 20iter: 14.9429 sec.\n",
            "iteration 28820 || Loss: 0.1628 || 20iter: 14.8978 sec.\n",
            "iteration 28840 || Loss: 0.0728 || 20iter: 14.9358 sec.\n",
            "iteration 28860 || Loss: 0.1996 || 20iter: 14.9604 sec.\n",
            "iteration 28880 || Loss: 0.1109 || 20iter: 14.8942 sec.\n",
            "iteration 28900 || Loss: 0.1382 || 20iter: 14.9078 sec.\n",
            "--------------\n",
            "epoch 79 || Epoch_TRAIN_Loss: 0.1119 || Epoch_VAL_Loss: 0.0000\n",
            "timer: 282.6179 sec.\n",
            "--------------\n",
            "Epoch 80/80\n",
            "--------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d84a4e47526b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-46635ada8432>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(train)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH_DEPRECATION_WARNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mget_lr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         return [base_lr * lmbda(self.last_epoch)\n\u001b[0;32m--> 251\u001b[0;31m                 for lmbda, base_lr in zip(self.lr_lambdas, self.base_lrs)]\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         return [base_lr * lmbda(self.last_epoch)\n\u001b[0;32m--> 251\u001b[0;31m                 for lmbda, base_lr in zip(self.lr_lambdas, self.base_lrs)]\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-41969fd4b451>\u001b[0m in \u001b[0;36mlambda_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlambda_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambdaLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: math domain error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZJWV8b_-i4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqPTK67w6J5J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}